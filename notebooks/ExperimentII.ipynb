{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45bc8af2-7866-4752-99d2-ca1cce9093bf",
   "metadata": {},
   "source": [
    "# EXPERIMENT II - INFLUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec9429-347c-4db8-a7c7-edbcca9b21d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f8d8b-f4a9-4b71-a902-811df189b85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "import ujson as json\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import requests \n",
    "from scipy.stats import entropy\n",
    "\n",
    "tqdm.pandas()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "# plt.style.use(\"dark_background\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates.default = 'presentation'\n",
    "\n",
    "import rich\n",
    "from itertools import combinations\n",
    "import sys \n",
    "from statistics import mean, stdev\n",
    "import struct, io, string\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from scipy.stats import chisquare,kstest\n",
    "from scipy import stats \n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45eb9af-ba27-431b-ae1d-4ef13b8901cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_parquet(name, **args):\n",
    "    path = basepath / f'{name}'\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    if 'publication_year' in df.columns:\n",
    "        df.loc[:, 'publication_year'] = pd.to_numeric(df.publication_year)\n",
    "        df = df[df.publication_year != 0]  # discard works with missing years\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r}')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792b599-a016-48f6-bc41-6fd368e1c590",
   "metadata": {},
   "source": [
    "## LOAD FIELDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7458ac6-0040-4a36-a963-5886582b7573",
   "metadata": {},
   "source": [
    "### Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7a32-1ba3-442e-ac05-b9a6562051f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'Physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d46212-99ea-47da-8705-7694a07c931d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/Physics/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6acbf8-1d20-4b0c-a7f0-83856a933308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af71260-1a9a-4cf5-83a1-a13e5f95dd8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Gravitational wave',\n",
    "    'Dark matter',\n",
    "    'Fluid dynamics',\n",
    "    'Soliton',\n",
    "    'Supersymmetry',\n",
    "    'Statistical physics',          \n",
    "    'Superconductivity' \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d6b4b-0c03-47de-8d12-343965f407c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e4f70-3d94-460c-bd47-b3411a2dbb67",
   "metadata": {},
   "source": [
    "### Computer Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c828ec-f331-41ab-8b52-e7e0d44b6ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'CS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521705c-9fa5-44b6-9e97-8c3eaf9c1a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/CS/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6525010-0f1a-4833-8abe-277b3b123ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2362d7-9929-42e3-8da4-a2cfdb5713ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Compiler',\n",
    "    'Mobile computing',\n",
    "    'Cryptography',\n",
    "    'Cluster analysis', \n",
    "    'Image processing',\n",
    "    'Parallel computing'         \n",
    "            ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402d70b-99da-4bce-95a5-7b53d2ed858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113890d2-4c4d-4a1a-a917-695eb7227db9",
   "metadata": {},
   "source": [
    "### BioMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c6d87-a72d-49dc-8b65-53613ef41478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'BioMed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7453344-fc6a-4807-a990-e1fbf6b89556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/BioMed/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92a321-31e8-4718-b728-5d6c99e07101",
   "metadata": {},
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c8bfa-1c35-466a-83e5-e6b4169ad667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "            'Protein structure',\n",
    "            'Genome', \n",
    "            'Peptide sequence',\n",
    "            \"Alzheimer's disease\",\n",
    "            'Neurology',          \n",
    "            'Radiation therapy',\n",
    "            'Chemotherapy'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91566eff-3a94-4682-8be4-1920f6294721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e5ebb-ccaf-42c8-8aac-5d9702f7a76f",
   "metadata": {},
   "source": [
    "## FUNCTIONS DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a73f8-0426-4b3a-8f8b-f23cfd719db1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a0241-79bf-44bf-b650-c0dcec1234bb",
   "metadata": {},
   "source": [
    "### Collaboration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96ee7f-ee21-49d3-b0f6-a2211cdd438a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_collaboration_graph(works_authors,author_ids, start_year, end_year):\n",
    "    \n",
    "    current_work_ids = set(works_authors\n",
    "                            .query('(@start_year <= publication_year < @end_year)')\n",
    "                            .query('author_id.isin(@author_ids)')\n",
    "                            .work_id)\n",
    "\n",
    "    current_work_author_ids = (works_authors\n",
    "        [['work_id', 'author_id']]\n",
    "        .query('work_id.isin(@current_work_ids)'))\n",
    "                              \n",
    "    bip_g = nx.from_pandas_edgelist(\n",
    "        current_work_author_ids,\n",
    "        source='work_id', target='author_id'\n",
    "    )\n",
    "    \n",
    "    author_ids =  author_ids.intersection(set(current_work_author_ids.author_id))\n",
    "    \n",
    "    #return bip_g\n",
    "    collab_graph = nx.bipartite.projected_graph(bip_g, nodes=author_ids)\n",
    "\n",
    "    return collab_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54b84f-3f5b-4e7b-92a5-71e6cc638404",
   "metadata": {},
   "source": [
    "### Delate common neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca0d4a-c44c-435e-8789-863ab8488213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delate_neig_incommon(collab_graph, active_authors):\n",
    "    #infected authors neighbors not in common \n",
    "    nodes = set(collab_graph.nodes())\n",
    "    nodes_active = list(active_authors)\n",
    "    \n",
    "    #any kind exposure\n",
    "    neighbours_list = []\n",
    "    for i in nodes_active:\n",
    "        neighbours_list.extend(list(nx.neighbors(collab_graph,i)))       \n",
    "    dict_count = dict(Counter(neighbours_list)) #count node occurency\n",
    "    \n",
    "    #multiple exposure \n",
    "    mult_exp_list = set([k for k,v in dict_count.items() if v > 1]) - set(nodes_active)\n",
    "    #I consider just nodes with no multiple exposures \n",
    "    nodes = nodes - set(mult_exp_list)\n",
    "    nodes = nodes - set(nodes_active)\n",
    "    \n",
    "    ##statistics\n",
    "    multiple_exp = len(mult_exp_list)\n",
    "    \n",
    "    #single exposure \n",
    "    sing_exp = len(set([k for k,v in dict_count.items() if v == 1]) - set(nodes_active))\n",
    "        \n",
    "    return nodes,multiple_exp,sing_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efc766-eb38-42f9-9c84-56b9753efc0c",
   "metadata": {},
   "source": [
    "### Definition experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07c1a1-de08-47be-99a8-a1b46d65cf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact1 - papers:all, cits:topic\n",
    "def experts_impact_mean_1(works_authors,start_year_i,active_authors_start,works_cit_counts_year_concept):\n",
    "\n",
    "    #papers:all, citations:just tagged with concept \n",
    "    #all papers (with and without concept) written before start_date by active authors\n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('@start_year_i - 5 <= publication_year < @start_year_i', engine='python')\n",
    "                    .query('author_id.isin(@active_authors_start)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_concept_startyear = works_cit_counts_year_concept.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_concept_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10c69d-3f39-4a04-b459-5cd770f965bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact2 - papers:topic, cits:all\n",
    "def experts_impact_mean_2(works_authors,start_year_i,prior_works_ids_tot_5yr,active_authors_start,works_cit_counts_year):\n",
    "\n",
    "    #papers:just tagged with concept, citations:all\n",
    "    #just papers tagged with concept written before start_date by active authors \n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('work_id.isin(@prior_work_ids_5yr)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_startyear = works_cit_counts_year.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4cc50-a1a1-4a04-a314-6e52f22a9bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact3 - papers:topic, cits:topic\n",
    "def experts_impact_mean_3(works_authors,start_year_i,prior_work_ids_5yr,active_authors_start,works_cit_counts_year_concept):\n",
    "\n",
    "    #papers:just tagged with concept, citations:just tagged with concept \n",
    "    #just papers tagged with concept written before start_date by active authors \n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('work_id.isin(@prior_work_ids_5yr)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_concept_startyear = works_cit_counts_year_concept.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_concept_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705dbce3-9060-4ebf-9f83-2ee65c2b7878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experts_productivity(works_authors,prior_work_ids_5yr,active_authors_start):\n",
    "    #count number of works written with topic during exposure window\n",
    "    sorted_author_works_count = (\n",
    "    works_authors\n",
    "    .query('work_id.isin(@prior_work_ids_5yr) & author_id.isin(@active_authors_start)') \n",
    "    .groupby('author_id')\n",
    "    .work_id\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    sorted_author_works_count_len = len(sorted_author_works_count)\n",
    "    \n",
    "    sorted_author_works_count = sorted_author_works_count.to_frame().reset_index()\n",
    "    sorted_author_works_count.columns = ['author_id', 'val']\n",
    "    \n",
    "    return sorted_author_works_count,sorted_author_works_count_len  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d2c2c-650f-44a8-af3a-3fe1feaa3741",
   "metadata": {},
   "source": [
    "### Get author samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720ead4-122f-4ea2-b1f9-9519981c3717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_author_samples(author_stats_df, top_k, debug=False):\n",
    "    \"\"\"\n",
    "    author_stats_df: DataFrame where author_id has active author ids, and val has the productivity/impact values for that author\n",
    "    top_k: either 10 or 20 depending on top 10 or 20%\n",
    "    \n",
    "    Returns a dictionary where keys are class labels, and values are set of author IDs\n",
    "    \"\"\"\n",
    "    # Note highest scoring authors are ranked LAST \n",
    "    author_stats_df.loc[:, 'rank_pct'] = author_stats_df.val.rank(method='min', pct=True)  # rank rows based on val convert to percentiles\n",
    "    #author_stats_df.loc[:, 'rank_pct'] = author_stats_df.val.rank(pct=True)\n",
    "    \n",
    "    if top_k == 10:\n",
    "        bins = [0, 0.1, 0.3, 0.45, 0.55, 0.7, 0.9, 1]\n",
    "        labels=['bottom 10%', '10-30%', '30-45%', 'middle 10%', '55-70%', '70-90%', 'top 10%']\n",
    "    else:\n",
    "        bins = [0, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 1]\n",
    "        labels=['bottom 20%', '20-30%', '30-40%', 'middle 20%', '60-70%', '70-80%', 'top 20%']\n",
    "        \n",
    "    author_stats_df.loc[:, 'rank_cat'] = (  # assign category labels based on rank percentiles \n",
    "        pd.cut(\n",
    "            author_stats_df.rank_pct,\n",
    "            bins=bins,\n",
    "            labels=labels\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    samples_per_class = max(int((top_k / 100) * author_stats_df.author_id.nunique()), 1)\n",
    "    if debug:\n",
    "        print(f'{top_k=} taking {samples_per_class=:,}')\n",
    "        display(author_stats_df.head(2))\n",
    "    \n",
    "    buckets_size = list(author_stats_df.groupby('rank_cat').count()['rank_pct'])\n",
    "    #print(buckets_size)\n",
    "    \n",
    "    samples_dict = {}\n",
    "    \n",
    "    keep = [f'bottom {top_k}%', f'middle {top_k}%', f'top {top_k}%']  # keep only these classes\n",
    "    #keep = [f'bottom {top_k}%', f'top {top_k}%']\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in keep:\n",
    "            continue\n",
    "        \n",
    "        #initial bucket     \n",
    "        candidates = set(author_stats_df[author_stats_df.rank_cat==label].author_id)\n",
    "        candidates_size = buckets_size[i] #len(candidates)\n",
    "        if candidates_size >=  samples_per_class:\n",
    "            if debug:\n",
    "                print(f'{label}: Sampling {samples_per_class:,} from {len(candidates):,} candidates')\n",
    "            samples = set(random.sample(list(candidates), samples_per_class))  # sample here\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f'Insufficient items in {label}. Need {samples_per_class:,} have {len(candidates):,}')\n",
    "            samples = candidates  # pick everyone\n",
    "    \n",
    "        missing = samples_per_class - len(samples)\n",
    "        if missing > 0: \n",
    "            \n",
    "            #1 next bucket \n",
    "            if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i+1]\n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i+1]\n",
    "            else: # for the highest bucket, go one below\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i-1] \n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i-1]\n",
    "\n",
    "            if candidate_size >= missing:    \n",
    "                new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                samples = samples | new_samples  # add these new samples\n",
    "                if debug:\n",
    "                    print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "            else: \n",
    "                new_samples = candidates  # pick everyone\n",
    "                samples = samples | new_samples\n",
    "            \n",
    "            missing = samples_per_class - len(samples)\n",
    "            if missing > 0: \n",
    "\n",
    "                #2 next bucket \n",
    "                if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i+2]\n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i+2]\n",
    "                else: # for the highest bucket, go one below\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i-2] \n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i-2]\n",
    "                \n",
    "                if candidate_size >= missing:    \n",
    "                    new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                    samples = samples | new_samples  # add these new samples\n",
    "                    if debug:\n",
    "                        print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "                else: \n",
    "                    new_samples = candidates  # pick everyone\n",
    "                    samples = samples | new_samples\n",
    "    \n",
    "        assert len(samples) == samples_per_class, f'Count mismatch {len(samples)=} {samples_per_class=} for samples {label}'\n",
    "        samples_dict[label] = samples\n",
    "        \n",
    "    return samples_dict,samples_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8910ea1-070a-4391-917a-068e5106c528",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81686f87-2e40-43b5-a54f-48df789f7bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scores_high(author_ids, collab_graph,first_time_authors,prior_author_ids,nodes,authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict,high_active_authors_bin1,high_active_authors_bin2):\n",
    "\n",
    "    #just first time authors one exposure\n",
    "    first_time_authors = first_time_authors & nodes\n",
    "    \n",
    "    numerators_A = {}\n",
    "    denominators_A = {}\n",
    "    fractions_A = {}\n",
    "    numerators_B = {}\n",
    "    denominators_B = {}\n",
    "    fractions_B = {}\n",
    "    for author_id in author_ids: \n",
    "\n",
    "        # Exp2 - A \n",
    "        neighbors = set(collab_graph.neighbors(author_id))\n",
    "        denominator = len((neighbors - prior_author_ids) & nodes)\n",
    "        \n",
    "        if denominator != 0:\n",
    "            denominators_A[author_id] = denominator\n",
    "            neigh_activated = neighbors & first_time_authors\n",
    "            numerator = len(neigh_activated)\n",
    "            numerators_A[author_id] = numerator\n",
    "            fractions_A[author_id] = numerator / denominator\n",
    "        else: #to have same number for each sample\n",
    "            numerator = np.nan\n",
    "            numerators_A[author_id] = numerator\n",
    "            denominators_A[author_id] = np.nan\n",
    "            fractions_A[author_id] = np.nan\n",
    "            \n",
    "        # Exp2 - B\n",
    "        denominator = numerator\n",
    "\n",
    "        if denominator!=0 and not np.isnan(denominator):\n",
    "            denominators_B[author_id] = denominator\n",
    "            author_id_1paper = authors_active_start_1paper_id_dict[author_id] #1 papers in which author_id coauthor         \n",
    "            numerator = 0\n",
    "            for na in neigh_activated:           \n",
    "                first_time_authors_1paper_na = first_time_authors_1paper_id_dict[na]\n",
    "                if first_time_authors_1paper_na in author_id_1paper:\n",
    "                    numerator += 1\n",
    "            numerators_B[author_id] = numerator\n",
    "            fractions_B[author_id] = numerator / denominator\n",
    "        else:\n",
    "            numerators_B[author_id] = np.nan\n",
    "            denominators_B[author_id] = np.nan\n",
    "            fractions_B[author_id] = np.nan       \n",
    "\n",
    "    \n",
    "    # Exp2 - C \n",
    "    # numerators_A_bin1 = {key: numerators_A[key] for key in high_active_authors_bin1}\n",
    "    # denominators_A_bin1 = {key: denominators_A[key] for key in high_active_authors_bin1}\n",
    "    fractions_A_bin1 = {key: fractions_A[key] for key in high_active_authors_bin1}\n",
    "    # numerators_A_bin2 = {key: numerators_A[key] for key in high_active_authors_bin2}\n",
    "    # denominators_A_bin2 = {key: denominators_A[key] for key in high_active_authors_bin2}\n",
    "    fractions_A_bin2 = {key: fractions_A[key] for key in high_active_authors_bin2} \n",
    "     \n",
    "    return [fractions_A,fractions_B,fractions_A_bin1,fractions_A_bin2]                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59db8d-73dc-4601-8bb7-62b699a3d6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scores_low(author_ids, collab_graph,first_time_authors,prior_author_ids,nodes,authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict):\n",
    "\n",
    "    #just first time authors one exposure\n",
    "    first_time_authors = first_time_authors & nodes\n",
    "    \n",
    "    numerators_A = {}\n",
    "    denominators_A = {}\n",
    "    fractions_A = {}\n",
    "    numerators_B = {}\n",
    "    denominators_B = {}\n",
    "    fractions_B = {}\n",
    "    for author_id in author_ids: \n",
    "\n",
    "        # Exp2 - A \n",
    "        neighbors = set(collab_graph.neighbors(author_id))\n",
    "        denominator = len((neighbors - prior_author_ids) & nodes)\n",
    "        \n",
    "        if denominator != 0:\n",
    "            denominators_A[author_id] = denominator\n",
    "            neigh_activated = neighbors & first_time_authors\n",
    "            numerator = len(neigh_activated)\n",
    "            numerators_A[author_id] = numerator\n",
    "            fractions_A[author_id] = numerator / denominator\n",
    "        else: #to have same number for each sample\n",
    "            numerator = np.nan\n",
    "            numerators_A[author_id] = numerator\n",
    "            denominators_A[author_id] = np.nan\n",
    "            fractions_A[author_id] = np.nan\n",
    "            \n",
    "        # Exp2 - B\n",
    "        denominator = numerator\n",
    "\n",
    "        if denominator!=0 and not np.isnan(denominator):\n",
    "            denominators_B[author_id] = denominator\n",
    "            author_id_1paper = authors_active_start_1paper_id_dict[author_id] #1 papers in which author_id coauthor         \n",
    "            numerator = 0\n",
    "            for na in neigh_activated:           \n",
    "                first_time_authors_1paper_na = first_time_authors_1paper_id_dict[na]\n",
    "                if first_time_authors_1paper_na in author_id_1paper:\n",
    "                    numerator += 1\n",
    "            numerators_B[author_id] = numerator\n",
    "            fractions_B[author_id] = numerator / denominator\n",
    "        else:\n",
    "            numerators_B[author_id] = np.nan\n",
    "            denominators_B[author_id] = np.nan\n",
    "            fractions_B[author_id] = np.nan       \n",
    "     \n",
    "    return [fractions_A,fractions_B]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56462819-9423-49a9-aa25-fb6c363e63e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bins_C(works_authors_active,work_id_valid,high_active_authors1):\n",
    "          \n",
    "    high1_dilution_df = ((works_authors_active.query('work_id.isin(@work_id_valid)').groupby('author_id')['n_coauthors'].mean()).to_frame()).sort_values(by=['n_coauthors'],ascending=False).reset_index(level=0)\n",
    "    high1_dilution_df = high1_dilution_df.query('author_id.isin(@high_active_authors1)')\n",
    "    \n",
    "    high1_dilution_df.columns = ['author_id', 'val']\n",
    "    \n",
    "    samples_dict_1,n_1 = get_author_samples(high1_dilution_df, top_k=20, debug=False)\n",
    "     \n",
    "    high_active_authors1_bin1 = samples_dict_1['top 20%']\n",
    "    high_active_authors1_bin2 = samples_dict_1['bottom 20%']\n",
    "\n",
    "    return high_active_authors1_bin1,high_active_authors1_bin2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f804e-b3b9-4109-a900-9bddb062261b",
   "metadata": {},
   "source": [
    "### Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2364c05-290f-484d-a085-c8e1289e5a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "my_path = os.path.join(discipline, 'Impact_mean1')\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "    \n",
    "my_path = os.path.join(discipline, 'Productivity')\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1aedf-2933-465e-bf6c-0d16bee1eb27",
   "metadata": {},
   "source": [
    "## EXP2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae8a8d-7a6b-498a-b7e0-fcc0cdc970f8",
   "metadata": {},
   "source": [
    "### Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca56df-f0e6-469e-ae68-b777433a7a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(os.path.join(discipline,'Info'),os.path.split(os.path.split(my_path)[0])[1])\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995     \n",
    "    \n",
    "    #my_path4 = os.path.join(os.path.split(my_path)[0],'Exp1_ver1')\n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)  \n",
    "                \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "                  \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "\n",
    "    works_authors_active_union = pd.merge(works_authors_activation_date,works[['n_coauthors']], left_on=\"work_id\", right_index=True)\n",
    "    works_authors_activation_set = set(works_authors_active_union.work_id)\n",
    "    my_file = 'works_authors_active_union_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(works_authors_active_union,fp)\n",
    "    works_authors_activation = works_authors.query('work_id.isin(@works_authors_activation_set)')\n",
    "    my_file = 'works_authors_activation_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(works_authors_activation,fp)\n",
    "    # my_file = 'works_authors_activation_'+topic       \n",
    "    # with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "    #     works_authors_activation = pickle.load(fp)\n",
    "    \n",
    "    #first paper first_time_authors\n",
    "    works_authors_conc = pd.merge(works_authors, works_concepts_conc[['work_id','concept_name']], on=\"work_id\")\n",
    "    works_authors_concept_period = works_authors_conc.query('@start_year <= publication_year & concept_name==@topic', engine='python') #papers with concept written during the windows\n",
    "    work_ids_authors_new_df = works_authors_concept_period[works_authors_concept_period.author_id.isin(first_time_authors_union)].sort_values(by='publication_date').drop_duplicates('author_id') #chronological order and not repetition \n",
    "    first_time_authors_work_ids = set(work_ids_authors_new_df.work_id) #list works with new authors in the field, in chronological order\n",
    "    #dictionary = {first_time_author_id : first_paper_id}\n",
    "    work_ids_authors_new_df2 = work_ids_authors_new_df[['work_id','author_id']]\n",
    "    #work_ids_authors_new_df2 = work_ids_authors_new_df.work_id.to_frame().join(work_ids_authors_new_df.author_id)\n",
    "    first_time_authors_1paper_id_dict = work_ids_authors_new_df2.set_index('author_id').to_dict()['work_id']\n",
    "    my_file = 'first_time_authors_1paper_id_dict_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_1paper_id_dict,fp)\n",
    "    \n",
    "    #active_authors_start_union 1 papers of which they are coauthors\n",
    "    work_ids_authors_active_df = works_authors_concept_period[works_authors_concept_period.work_id.isin(first_time_authors_work_ids)] #1 papers ids \n",
    "    work_ids_authors_active_df_ = work_ids_authors_active_df[work_ids_authors_active_df.author_id.isin(active_authors_start_union)] #just active authors  \n",
    "    #dictionary: {authors_active_start_id : list first_paper_id coauthor}\n",
    "    authors_active_start_1paper_id_dict = (\n",
    "        work_ids_authors_active_df_\n",
    "        .groupby('author_id')\n",
    "        .work_id\n",
    "        .apply(lambda g: list(g))\n",
    "        .to_dict()\n",
    "    )\n",
    "    for aanc in list(active_authors_start_union - set(authors_active_start_1paper_id_dict.keys())):  #active authors coauthors no 1 paper\n",
    "        authors_active_start_1paper_id_dict[aanc] = []\n",
    "    my_file = 'authors_active_start_1paper_id_dict_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(authors_active_start_1paper_id_dict,fp)\n",
    "    my_file = 'work_ids_authors_active_df_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(work_ids_authors_active_df,fp)\n",
    "    \n",
    "    info_df_  = pd.DataFrame()\n",
    "    frac_vec = {} \n",
    "    for w in tqdm(range(0,23)): \n",
    "        \n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "        \n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            prior_author_ids = prior_author_ids_list[w]\n",
    "              \n",
    "            #collaboration graph\n",
    "            collab_graph = make_collaboration_graph(works_authors_activation,active_authors_start,start_year=start_year_w-5, end_year=start_year_w)\n",
    "            #keep nodes with just single exposures\n",
    "            nodes,multiple_exp,sing_exp = delate_neig_incommon(collab_graph=collab_graph, active_authors=active_authors_start) \n",
    "            #high and low infected authors \n",
    "            #papers written by infected authors in exposure window (5 years before)\n",
    "            works_authors_active = (works_authors_active_union.query('@start_year_w - 5 <= publication_year < @start_year_w ')).query('author_id.isin(@active_authors_start)')\n",
    "            works_authors_active_set = set(works_authors_active.work_id)\n",
    "            #just works written with eligible coauthors\n",
    "            nodes_prior = nodes - prior_author_ids\n",
    "            work_id_valid = set(((works_authors.query('work_id.isin(@works_authors_active_set)')).query('author_id.isin(@nodes_prior)')).work_id)\n",
    "\n",
    "            #Exp2 - C #two bins higly active authors depending on mean of number of coauthors\n",
    "            high_active_authors1_bin1,high_active_authors1_bin2 = get_bins_C(works_authors_active,work_id_valid,high_active_authors1)\n",
    "\n",
    "            #Exp2 - A and B\n",
    "            #list of dictionaries [high1_A,high1_B,high1_bin1_A,high1_bin2_A]      \n",
    "            frac_vec_high1 = get_scores_high(author_ids=high_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict,high_active_authors_bin1=high_active_authors1_bin1,high_active_authors_bin2=high_active_authors1_bin2)\n",
    "            frac_vec_low1 = get_scores_low(author_ids=low_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict)\n",
    "            \n",
    "            frac_vec[start_year_w] = [frac_vec_high1,frac_vec_low1]\n",
    "\n",
    "            #save on files #info\n",
    "            info_w_dict = { '#NODES MULTIPLE EXPOSURES':multiple_exp, \n",
    "                           '#NODES SINGLE EXPOSURE':sing_exp\n",
    "                          }\n",
    "\n",
    "            info_w = pd.DataFrame(data=[info_w_dict])\n",
    "            info_w.insert(0, 'T_0', start_year_w)\n",
    "            info_df_ = pd.concat([info_df_, info_w], ignore_index = True, axis = 0)\n",
    "\n",
    "    #save on file : concept - year_start \n",
    "    my_file = 'info_df_'+topic+'_windows.csv'\n",
    "    info_df_.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    my_file = 'frac_vec_'+topic+'_windows'\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(frac_vec,fp)\n",
    "\n",
    "    #save all concept dataframes in one file\n",
    "    info_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return info_df_,frac_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2d300-944b-46f7-b6b6-c41b71c51e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Productivity/Exp2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "info_df = pd.DataFrame() \n",
    "frac_vec = {}\n",
    "for topic in topic_list:\n",
    "    info_df_top,frac_vec_top = Exp2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    frac_vec[topic] = frac_vec_top\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "my_file = 'frac_vec_windows'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(frac_vec,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f412a5-e207-4304-8625-4a64515dfe55",
   "metadata": {},
   "source": [
    "### Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369a4ab-2fdc-4e52-aaf6-853187092f02",
   "metadata": {},
   "source": [
    "#### Def. impact 1 - papers:all, cits:topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25602bc-93ce-4bfd-9e87-ce27283e3b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp2_1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "  \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean1')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995      \n",
    "    \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)  \n",
    "     \n",
    "                \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "                 \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "\n",
    "    my_path5 = os.path.join(discipline, 'Productivity/Exp2')    \n",
    "    my_file = 'works_authors_activation_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_activation = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_active_union_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_active_union = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'first_time_authors_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        first_time_authors_1paper_id_dict = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'authors_active_start_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        authors_active_start_1paper_id_dict = pickle.load(fp)\n",
    "    my_file = 'work_ids_authors_active_df_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        work_ids_authors_active_df = pickle.load(fp)\n",
    "    \n",
    "    info_df_  = pd.DataFrame()\n",
    "    frac_vec = {} \n",
    "    for w in tqdm(range(0,23)): \n",
    "        \n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "        \n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            prior_author_ids = prior_author_ids_list[w]\n",
    "              \n",
    "            #collaboration graph\n",
    "            collab_graph = make_collaboration_graph(works_authors_activation,active_authors_start,start_year=start_year_w-5, end_year=start_year_w)\n",
    "            #keep nodes with just single exposures\n",
    "            nodes,multiple_exp,sing_exp = delate_neig_incommon(collab_graph=collab_graph, active_authors=active_authors_start) \n",
    "            #high and low infected authors \n",
    "            #papers written by infected authors in exposure window (5 years before)\n",
    "            works_authors_active = (works_authors_active_union.query('@start_year_w - 5 <= publication_year < @start_year_w ')).query('author_id.isin(@active_authors_start)')\n",
    "            works_authors_active_set = set(works_authors_active.work_id)\n",
    "            #just works written with eligible coauthors\n",
    "            nodes_prior = nodes - prior_author_ids\n",
    "            work_id_valid = set(((works_authors.query('work_id.isin(@works_authors_active_set)')).query('author_id.isin(@nodes_prior)')).work_id)\n",
    "\n",
    "            #Exp2 - C #two bins higly active authors depending on number of coauthors\n",
    "            high_active_authors1_bin1,high_active_authors1_bin2 = get_bins_C(works_authors_active,work_id_valid,high_active_authors1)\n",
    "\n",
    "            #highly infected\n",
    "            #list of dictionaries [high1_A,high1_B,high1_bin1_A,high1_bin2_A]      \n",
    "            frac_vec_high1 = get_scores_high(author_ids=high_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict,high_active_authors_bin1=high_active_authors1_bin1,high_active_authors_bin2=high_active_authors1_bin2)\n",
    "            frac_vec_low1 = get_scores_low(author_ids=low_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict)\n",
    "            \n",
    "            frac_vec[start_year_w] = [frac_vec_high1,frac_vec_low1]\n",
    "\n",
    "            #save on files #info\n",
    "            info_w_dict = { '#NODES MULTIPLE EXPOSURE':multiple_exp, \n",
    "                           '#NODES SINGLE EXPOSURE':sing_exp\n",
    "                          }\n",
    "\n",
    "            info_w = pd.DataFrame(data=[info_w_dict])\n",
    "            info_w.insert(0, 'T_0', start_year_w)\n",
    "            info_df_ = pd.concat([info_df_, info_w], ignore_index = True, axis = 0)\n",
    "\n",
    "    #save on file : concept - year_start \n",
    "    my_file = 'info_df_'+topic+'_windows.csv'\n",
    "    info_df_.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    my_file = 'frac_vec_'+topic+'_windows'\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(frac_vec,fp)\n",
    "\n",
    "    #save all concept dataframes in one file\n",
    "    info_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return info_df_,frac_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccba893-952c-4d30-a2a5-b144ce06b13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean1/Exp2_1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "info_df = pd.DataFrame() \n",
    "frac_vec = {}\n",
    "for topic in topic_list:\n",
    "    info_df_top,frac_vec_top = Exp2_1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    frac_vec[topic] = frac_vec_top\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "my_file = 'frac_vec_windows'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(frac_vec,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da9bf3-17e1-417c-b837-7a0df544e5ab",
   "metadata": {},
   "source": [
    "#### Def. impact 2 - papers:topic, cits:all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a1dfe-e9a7-4da4-a0fb-55ef48b7fefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp2_2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "  \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean2')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995      \n",
    "    \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)  \n",
    "     \n",
    "                \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "                 \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "\n",
    "    my_path5 = os.path.join(discipline, 'Productivity/Exp2')    \n",
    "    my_file = 'works_authors_activation_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_activation = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_active_union_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_active_union = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'first_time_authors_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        first_time_authors_1paper_id_dict = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'authors_active_start_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        authors_active_start_1paper_id_dict = pickle.load(fp)\n",
    "    my_file = 'work_ids_authors_active_df_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        work_ids_authors_active_df = pickle.load(fp)\n",
    "    \n",
    "    info_df_  = pd.DataFrame()\n",
    "    frac_vec = {} \n",
    "    for w in tqdm(range(0,23)): \n",
    "        \n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "        \n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            prior_author_ids = prior_author_ids_list[w]\n",
    "              \n",
    "            #collaboration graph\n",
    "            collab_graph = make_collaboration_graph(works_authors_activation,active_authors_start,start_year=start_year_w-5, end_year=start_year_w)\n",
    "            #keep nodes with just single exposures\n",
    "            nodes,multiple_exp,sing_exp = delate_neig_incommon(collab_graph=collab_graph, active_authors=active_authors_start) \n",
    "            #high and low infected authors \n",
    "            #papers written by infected authors in exposure window (5 years before)\n",
    "            works_authors_active = (works_authors_active_union.query('@start_year_w - 5 <= publication_year < @start_year_w ')).query('author_id.isin(@active_authors_start)')\n",
    "            works_authors_active_set = set(works_authors_active.work_id)\n",
    "            #just works written with eligible coauthors\n",
    "            nodes_prior = nodes - prior_author_ids\n",
    "            work_id_valid = set(((works_authors.query('work_id.isin(@works_authors_active_set)')).query('author_id.isin(@nodes_prior)')).work_id)\n",
    "\n",
    "            #Exp2 - C #two bins higly active authors depending on number of coauthors\n",
    "            high_active_authors1_bin1,high_active_authors1_bin2 = get_bins_C(works_authors_active,work_id_valid,high_active_authors1)\n",
    "\n",
    "            #highly infected\n",
    "            #list of dictionaries [high1_A,high1_B,high1_bin1_A,high1_bin2_A]      \n",
    "            frac_vec_high1 = get_scores_high(author_ids=high_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict,high_active_authors_bin1=high_active_authors1_bin1,high_active_authors_bin2=high_active_authors1_bin2)\n",
    "            frac_vec_low1 = get_scores_low(author_ids=low_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict)\n",
    "            \n",
    "            frac_vec[start_year_w] = [frac_vec_high1,frac_vec_low1]\n",
    "\n",
    "            #save on files #info\n",
    "            info_w_dict = { '#NODES MULTIPLE EXPOSURE':multiple_exp, \n",
    "                           '#NODES SINGLE EXPOSURE':sing_exp\n",
    "                          }\n",
    "\n",
    "            info_w = pd.DataFrame(data=[info_w_dict])\n",
    "            info_w.insert(0, 'T_0', start_year_w)\n",
    "            info_df_ = pd.concat([info_df_, info_w], ignore_index = True, axis = 0)\n",
    "\n",
    "    #save on file : concept - year_start \n",
    "    my_file = 'info_df_'+topic+'_windows.csv'\n",
    "    info_df_.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    my_file = 'frac_vec_'+topic+'_windows'\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(frac_vec,fp)\n",
    "\n",
    "    #save all concept dataframes in one file\n",
    "    info_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return info_df_,frac_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32189941-f7b2-49c7-a8cd-bfe0976fa652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean2/Exp2_2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "info_df = pd.DataFrame() \n",
    "frac_vec = {}\n",
    "for topic in topic_list:\n",
    "    info_df_top,frac_vec_top = Exp2_2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    frac_vec[topic] = frac_vec_top\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "my_file = 'frac_vec_windows'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(frac_vec,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b973175-f7b5-4990-a831-d30594f05adc",
   "metadata": {},
   "source": [
    "#### Def. impact 3 - papers:topic, cits:topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df169c-bcbc-41b1-a51f-d2294f97fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp2_3(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    " \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean3')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995      \n",
    "    \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)  \n",
    "     \n",
    "                \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "                 \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "\n",
    "    my_path5 = os.path.join(discipline, 'Productivity/Exp2')    \n",
    "    my_file = 'works_authors_activation_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_activation = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_active_union_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        works_authors_active_union = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'first_time_authors_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        first_time_authors_1paper_id_dict = pickle.load(fp)\n",
    "    \n",
    "    my_file = 'authors_active_start_1paper_id_dict_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        authors_active_start_1paper_id_dict = pickle.load(fp)\n",
    "    my_file = 'work_ids_authors_active_df_'+topic       \n",
    "    with open(os.path.join(my_path5, my_file),\"rb\") as fp:\n",
    "        work_ids_authors_active_df = pickle.load(fp)\n",
    "    \n",
    "    info_df_  = pd.DataFrame()\n",
    "    frac_vec = {} \n",
    "    for w in tqdm(range(0,23)): \n",
    "        \n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "        \n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            prior_author_ids = prior_author_ids_list[w]\n",
    "              \n",
    "            #collaboration graph\n",
    "            collab_graph = make_collaboration_graph(works_authors_activation,active_authors_start,start_year=start_year_w-5, end_year=start_year_w)\n",
    "            #keep nodes with just single exposures\n",
    "            nodes,multiple_exp,sing_exp = delate_neig_incommon(collab_graph=collab_graph, active_authors=active_authors_start) \n",
    "            #high and low infected authors \n",
    "            #papers written by infected authors in exposure window (5 years before)\n",
    "            works_authors_active = (works_authors_active_union.query('@start_year_w - 5 <= publication_year < @start_year_w ')).query('author_id.isin(@active_authors_start)')\n",
    "            works_authors_active_set = set(works_authors_active.work_id)\n",
    "            #just works written with eligible coauthors\n",
    "            nodes_prior = nodes - prior_author_ids\n",
    "            work_id_valid = set(((works_authors.query('work_id.isin(@works_authors_active_set)')).query('author_id.isin(@nodes_prior)')).work_id)\n",
    "\n",
    "            #Exp2 - C #two bins higly active authors depending on number of coauthors\n",
    "            high_active_authors1_bin1,high_active_authors1_bin2 = get_bins_C(works_authors_active,work_id_valid,high_active_authors1)\n",
    "\n",
    "            #highly infected\n",
    "            #list of dictionaries [high1_A,high1_B,high1_bin1_A,high1_bin2_A]      \n",
    "            frac_vec_high1 = get_scores_high(author_ids=high_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict,high_active_authors_bin1=high_active_authors1_bin1,high_active_authors_bin2=high_active_authors1_bin2)\n",
    "            frac_vec_low1 = get_scores_low(author_ids=low_active_authors1, collab_graph=collab_graph, first_time_authors=first_time_authors,prior_author_ids=prior_author_ids,nodes=nodes,authors_active_start_1paper_id_dict=authors_active_start_1paper_id_dict,first_time_authors_1paper_id_dict=first_time_authors_1paper_id_dict)\n",
    "            \n",
    "            frac_vec[start_year_w] = [frac_vec_high1,frac_vec_low1]\n",
    "\n",
    "            #save on files #info\n",
    "            info_w_dict = { '#NODES MULTIPLE EXPOSURE':multiple_exp, \n",
    "                           '#NODES SINGLE EXPOSURE':sing_exp\n",
    "                          }\n",
    "\n",
    "            info_w = pd.DataFrame(data=[info_w_dict])\n",
    "            info_w.insert(0, 'T_0', start_year_w)\n",
    "            info_df_ = pd.concat([info_df_, info_w], ignore_index = True, axis = 0)\n",
    "\n",
    "    #save on file : concept - year_start \n",
    "    my_file = 'info_df_'+topic+'_windows.csv'\n",
    "    info_df_.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    my_file = 'frac_vec_'+topic+'_windows'\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(frac_vec,fp)\n",
    "\n",
    "    #save all concept dataframes in one file\n",
    "    info_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return info_df_,frac_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41051c0-cc8a-4363-83e4-1de6d5c1ac43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean3/Exp2_3')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "info_df = pd.DataFrame() \n",
    "frac_vec = {}\n",
    "for topic in topic_list:\n",
    "    info_df_top,frac_vec_top = Exp2_3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    frac_vec[topic] = frac_vec_top\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "my_file = 'frac_vec_windows'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(frac_vec,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc2bd6-d769-481b-83ae-58e434063cdc",
   "metadata": {},
   "source": [
    "## CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bd434-6296-421c-953f-3caad421214b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path_list = [\n",
    "os.path.join(discipline, 'Productivity/Exp2'),\n",
    "os.path.join(discipline, 'Impact_mean1/Exp2_1'),\n",
    "os.path.join(discipline, 'Impact_mean2/Exp2_2'),\n",
    "os.path.join(discipline, 'Impact_mean3/Exp2_3')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a54d5-fed2-4b9e-a115-5362708eae3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union results\n",
    "for my_path in my_path_list:\n",
    "    \n",
    "    topics_df = pd.DataFrame()\n",
    "    for topic in topic_list:\n",
    "        my_file = 'info_df_'+topic+'_windows.csv'\n",
    "        topic_df_top = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';') \n",
    "        topic_df_top.insert(0, 'topic', topic)\n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)  \n",
    "    my_file = 'info_windows.csv'    \n",
    "    topics_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    \n",
    "    frac_vec = {}\n",
    "    for topic in topic_list:\n",
    "        my_file = 'frac_vec_'+topic+'_windows'\n",
    "        with open(os.path.join(my_path, my_file),\"rb\") as fp:\n",
    "            frac_vec_top = pickle.load(fp)\n",
    "        frac_vec[topic] = frac_vec_top\n",
    "    my_file = 'frac_vec_windows'\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(frac_vec,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf144d21-6984-42ca-a70a-a7508622d442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdda2d-68ba-4f3a-b2db-80c07f973e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def percentage(frac_vec,c,perc_values):\n",
    "    high = np.array(list(frac_vec[c].values()))\n",
    "    high_nan = high[~np.isnan(high)]\n",
    "    high_nan_len = len(high_nan) #high active not Nan #with eligible neighbors\n",
    "    high_perc = []\n",
    "    high_perc_num = [] \n",
    "    for perc in perc_values: #percentage\n",
    "        high_p = len(high[high>=perc]) #number authors activation fraction above percentage \n",
    "        high_perc_num.append(high_p)\n",
    "        if high_nan_len!=0:\n",
    "            high_perc.append(high_p/high_nan_len)\n",
    "        else:\n",
    "            high_perc.append(np.nan)\n",
    "    \n",
    "    return high_perc,high_perc_num,high_nan_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3db117-e3be-4e24-8762-0fae26f8a75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate percentage\n",
    "def Exp2_calculation(frac_vec,topic,my_path,info_topics):\n",
    "    \n",
    "    frac_vec_topic = frac_vec[topic]\n",
    "    topic_df_  = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "    \n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for window in start_year_window_list: #each window     \n",
    "    #for window in list(frac_vec_topic.keys()):\n",
    "        frac_vec_topic_window = frac_vec_topic[window] #[frac_vec_high1,frac_vec_high2,frac_vec_base1,frac_vec_base2]\n",
    "\n",
    "        \n",
    "        #percentage \n",
    "        #high active\n",
    "        #10% #frac_vec_topic_window[0] = fractions_A,fractions_B,fractions_A_bin1,fractions_A_bin2\n",
    "        frac_vec_high1 = np.array(list(frac_vec_topic_window[0]))\n",
    "        #A\n",
    "        high1_A_perc,high1_A_perc_num,high1_A_nan_len = percentage(frac_vec_high1,0,perc_values)\n",
    "        #B\n",
    "        high1_B_perc,high1_B_perc_num,high1_B_nan_len = percentage(frac_vec_high1,1,perc_values)\n",
    "        #C\n",
    "        high1_bin1_A_perc,high1_bin1_A_perc_num,high1_bin1_A_nan_len = percentage(frac_vec_high1,2,perc_values)\n",
    "        high1_bin2_A_perc,high1_bin2_A_perc_num,high1_bin2_A_nan_len = percentage(frac_vec_high1,3,perc_values)\n",
    "        #low active\n",
    "        frac_vec_low1 = np.array(list(frac_vec_topic_window[1]))\n",
    "        #A\n",
    "        low1_A_perc,low1_A_perc_num,low1_A_nan_len = percentage(frac_vec_low1,0,perc_values)\n",
    "        #B\n",
    "        low1_B_perc,low1_B_perc_num,low1_B_nan_len = percentage(frac_vec_low1,1,perc_values)\n",
    "\n",
    "        #dataframe\n",
    "        topic_df_w  = pd.DataFrame(list(zip(perc_values,\n",
    "                      high1_A_perc,low1_A_perc,\n",
    "                      high1_bin1_A_perc,high1_bin2_A_perc,\n",
    "                      high1_B_perc,low1_B_perc,\n",
    "                      high1_A_perc_num,low1_A_perc_num,\n",
    "                      high1_bin1_A_perc_num,high1_bin2_A_perc_num,\n",
    "                      high1_B_perc_num,low1_B_perc_num)),\n",
    "             columns =['Perc',\n",
    "                       'high1_A','low1_A','high1_bin1_A','high1_bin2_A',\n",
    "                       'high1_B','low1_B',\n",
    "                       'high1_A_num','low1_A_num','high1_bin1_A_num','high1_bin2_A_num',\n",
    "                       'high1_B_num','low1_B_num',\n",
    "                      ])  \n",
    "        topic_df_w.insert(0, 'T_0', window)\n",
    "        topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "        \n",
    "        #save info\n",
    "        info_df_w_dict = {\n",
    "        'HIGH ACTIVE - NOT NAN - EXP A - 10%': high1_A_nan_len,\n",
    "        'LOW ACTIVE - NOT NAN - EXP A - 10%':low1_A_nan_len,\n",
    "        'HIGH ACTIVE - BIN1 - NOT NAN - EXP C - 10%': high1_bin1_A_nan_len,\n",
    "        'HIGH ACTIVE - BIN2 - NOT NAN - EXP C - 10%': high1_bin2_A_nan_len,\n",
    "        'HIGH ACTIVE - NOT NAN - EXP B - 10%': high1_B_nan_len,\n",
    "        'LOW ACTIVE - NOT NAN - EXP B - 10%':low1_B_nan_len}\n",
    "        info_df_w = pd.DataFrame(data=[info_df_w_dict])\n",
    "        info_df_w.insert(0, 'T_0', window)\n",
    "        info_df = pd.concat([info_df, info_df_w], ignore_index = True, axis = 0)\n",
    "\n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    return topic_df_,info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982153f-affa-4384-baf7-f9f13de06325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perc_values = np.array(range(0,105,5))/100 #percentages studied\n",
    "for my_path in my_path_list:\n",
    "    #load saved results\n",
    "    my_file_vector = 'frac_vec_windows'\n",
    "    with open(os.path.join(my_path, my_file_vector),\"rb\") as fp:\n",
    "        frac_vec = pickle.load(fp)\n",
    "        \n",
    "    topics_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "    #save info\n",
    "    my_file = 'info_windows.csv'\n",
    "    info_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "\n",
    "    for topic in topic_list:\n",
    "        topic_df_,info_df_ = Exp2_calculation(frac_vec,topic,my_path,info_topics)\n",
    "        topics_df = pd.concat([topics_df, topic_df_], ignore_index = True, axis = 0) \n",
    "        info_df = pd.concat([info_df, info_df_], ignore_index = True, axis = 0) \n",
    "\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))\n",
    "    \n",
    "    my_file = 'info_windows.csv'\n",
    "    info_df_old = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "    info_df_old.drop_duplicates(subset=['topic','T_0'], inplace=True)\n",
    "\n",
    "    info_df_old = info_df_old.merge(info_df, on=['topic','T_0'])\n",
    "    my_file = 'info_all_windows.csv'\n",
    "    info_df_old.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e7173-603f-4048-8186-047a099e6231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#average across windows for each topic \n",
    "for my_path in my_path_list:\n",
    "   \n",
    "    #info\n",
    "    my_file = 'info_all_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "    df_topics.drop_duplicates(subset=['topic','T_0'], inplace=True)\n",
    "    df_topics = df_topics.set_index('topic')\n",
    "    topic_df_ = pd.DataFrame()\n",
    "    for topic in topic_list: \n",
    "        df_topic = df_topics.query('topic == @topic')\n",
    "        num_windows_topic = len(df_topic)\n",
    "        df_topic_mean = df_topic[['#NODES MULTIPLE EXPOSURE','#NODES SINGLE EXPOSURE']].mean(axis=0).to_frame() \n",
    "        df_topic_mean = df_topic_mean.rename(index={'#NODES MULTIPLE EXPOSURE' : '#NODES MULTIPLE EXPOSURE - MEAN','#NODES SINGLE EXPOSURE':'#NODES SINGLE EXPOSURE - MEAN'})       \n",
    "        df_topic_sterr = (df_topic[['#NODES MULTIPLE EXPOSURE','#NODES SINGLE EXPOSURE'  ]].std(axis=0)/sqrt(num_windows_topic)).to_frame()\n",
    "        df_topic_sterr = df_topic_sterr.rename(index={'#NODES MULTIPLE EXPOSURE' : '#NODES MULTIPLE EXPOSURE - STERR','#NODES SINGLE EXPOSURE':'#NODES SINGLE EXPOSURE - STERR', })\n",
    "        df_topic_mean = pd.concat([df_topic_mean, df_topic_sterr], axis = 0)\n",
    "        df_topic_mean.columns = [topic]\n",
    "        df_topic_mean = df_topic_mean.T\n",
    "        topic_df_ = pd.concat([topic_df_, df_topic_mean], axis = 0)\n",
    "    my_file = 'info_table.csv'\n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file), sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecec40-15cf-486c-8561-98bddf2d679b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# statistics\n",
    "def Exp2_stat(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic') #topic\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list)\n",
    "    \n",
    "    df_topic_mean = pd.DataFrame(columns=[\n",
    "                           'high1_A','low1_A','high1_bin1_A','high1_bin2_A',\n",
    "                           'high1_B','low1_B',\n",
    "                            'high1_A_num','low1_A_num','high1_bin1_A_num','high1_bin2_A_num',\n",
    "                           'high1_B_num','low1_B_num',\n",
    "                            ])\n",
    "    \n",
    "    df_topic_sterr = pd.DataFrame(columns=[\n",
    "                           'high1_A','low1_A','high1_bin1_A','high1_bin2_A',\n",
    "                           'high1_B','low1_B',\n",
    "                            'high1_A_num','low1_A_num','high1_bin1_A_num','high1_bin2_A_num',\n",
    "                           'high1_B_num','low1_B_num',\n",
    "    ])\n",
    "    \n",
    "    for perc in perc_values:\n",
    "        df_topic_perc = df_topic.query('Perc == @perc')\n",
    "        df_topic_perc = df_topic_perc.drop(columns=['topic', 'T_0', 'Perc'])\n",
    "        df_topic_perc_mean = df_topic_perc.mean()\n",
    "        df_topic_perc_sterr = df_topic_perc.std()/sqrt(num_windows_topic)\n",
    "        \n",
    "        df_topic_mean.loc[perc] = df_topic_perc_mean\n",
    "        df_topic_sterr.loc[perc] = df_topic_perc_sterr\n",
    "        \n",
    "    \n",
    "    df_topic_mean.rename(columns = {\n",
    "                           'high1_A':'high1_A_mean','low1_A':'low1_A_mean','high1_bin1_A':'high1_bin1_A_mean','high1_bin2_A':'high1_bin2_A_mean',\n",
    "                           'high1_B':'high1_B_mean','low1_B':'low1_B_mean',\n",
    "                            'high1_A_num':'high1_A_num_mean','low1_A_num':'low1_A_num_mean','high1_bin1_A_num':'high1_bin1_A_num_mean','high1_bin2_A_num':'high1_bin2_A_num_mean',\n",
    "                           'high1_B_num':'high1_B_num_mean','low1_B_num':'low1_B_num_mean',\n",
    "    }, inplace = True)\n",
    "\n",
    "    df_topic_sterr.rename(columns = {\n",
    "                           'high1_A':'high1_A_sterr','low1_A':'low1_A_sterr','high1_bin1_A':'high1_bin1_A_sterr','high1_bin2_A':'high1_bin2_A_sterr',\n",
    "                           'high1_B':'high1_B_sterr','low1_B':'low1_B_sterr',\n",
    "                            'high1_A_num':'high1_A_num_sterr','low1_A_num':'low1_A_num_sterr','high1_bin1_A_num':'high1_bin1_A_num_sterr','high1_bin2_A_num':'high1_bin2_A_num_sterr',\n",
    "                           'high1_B_num':'high1_B_num_sterr','low1_B_num':'low1_B_num_sterr',    \n",
    "    }, inplace = True)\n",
    "    \n",
    "    df_topic = df_topic_mean.merge(df_topic_sterr, left_index=True, right_index=True)\n",
    "    df_topic = df_topic[[\n",
    "        'high1_A_mean','high1_A_sterr',\n",
    "        'low1_A_mean','low1_A_sterr',\n",
    "        'high1_bin1_A_mean','high1_bin1_A_sterr',\n",
    "        'high1_bin2_A_mean','high1_bin2_A_sterr',\n",
    "        'high1_B_mean','high1_B_sterr',\n",
    "        'low1_B_mean','low1_B_sterr',\n",
    "        'high1_A_num_mean','high1_A_num_sterr',\n",
    "        'low1_A_num_mean','low1_A_num_sterr',\n",
    "        'high1_bin1_A_num_mean','high1_bin1_A_num_sterr',\n",
    "        'high1_bin2_A_num_mean','high1_bin2_A_num_sterr',\n",
    "        'high1_B_num_mean','high1_B_num_sterr',\n",
    "        'low1_B_num_mean','low1_B_num_sterr',\n",
    "    ]]\n",
    "    df_topic = df_topic.rename_axis('Perc').reset_index()\n",
    "    df_topic.insert(0, 'topic', topic)\n",
    "        \n",
    "    return df_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b0aff-7d28-4c7b-be1d-cc3bedf2643c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perc_values = np.array(range(0,105,5))/100 #percentages studied\n",
    "for my_path in my_path_list:\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    topics_df = pd.DataFrame()\n",
    "    for topic in topic_list:  \n",
    "        topic_df = Exp2_stat(df_topics=df_topics,topic=topic,my_path=my_path)  \n",
    "        topics_df = pd.concat([topics_df, topic_df], ignore_index = True, axis = 0)\n",
    "    my_file = 'df_topic_stat.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
