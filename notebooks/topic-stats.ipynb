{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d6ddb6-ad33-4c4e-a5a6-e298fc433a5d",
   "metadata": {},
   "source": [
    "# OVERLAPPING COEFFICIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0d486e-e215-49f4-81b0-7666385538e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:38:46.317892Z",
     "iopub.status.busy": "2023-03-28T10:38:46.317392Z",
     "iopub.status.idle": "2023-03-28T10:38:47.235297Z",
     "shell.execute_reply": "2023-03-28T10:38:47.234567Z",
     "shell.execute_reply.started": "2023-03-28T10:38:46.317695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2cc802-b95c-41ab-8513-0e266e39fc5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:38:47.236884Z",
     "iopub.status.busy": "2023-03-28T10:38:47.236678Z",
     "iopub.status.idle": "2023-03-28T10:39:28.943368Z",
     "shell.execute_reply": "2023-03-28T10:39:28.942694Z",
     "shell.execute_reply.started": "2023-03-28T10:38:47.236849Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "import ujson as json\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import requests \n",
    "from scipy.stats import entropy\n",
    "\n",
    "tqdm.pandas()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "# plt.style.use(\"dark_background\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates.default = 'presentation'\n",
    "\n",
    "import rich\n",
    "from itertools import combinations\n",
    "import sys \n",
    "from statistics import mean, stdev\n",
    "import struct, io, string\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from scipy.stats import chisquare,kstest\n",
    "from scipy import stats \n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad07d2d-d113-4db6-8958-38b67b90f0e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:39:28.944877Z",
     "iopub.status.busy": "2023-03-28T10:39:28.944692Z",
     "iopub.status.idle": "2023-03-28T10:39:31.929955Z",
     "shell.execute_reply": "2023-03-28T10:39:31.929188Z",
     "shell.execute_reply.started": "2023-03-28T10:39:28.944852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_parquet(name, **args):\n",
    "    path = basepath / f'{name}'\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    if 'publication_year' in df.columns:\n",
    "        df.loc[:, 'publication_year'] = pd.to_numeric(df.publication_year)\n",
    "        df = df[df.publication_year != 0]  # discard works with missing years\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r}')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa8fea-d29c-4b1e-9bcd-d11c8bbe2106",
   "metadata": {},
   "source": [
    "## LOAD FIELDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af131f79-ab62-473b-b07e-734a94c75213",
   "metadata": {},
   "source": [
    "### Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5065ef-9d28-44f2-aaaa-9e7ba7fc4431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:25:24.177383Z",
     "iopub.status.busy": "2023-03-28T00:25:24.177030Z",
     "iopub.status.idle": "2023-03-28T00:25:24.612341Z",
     "shell.execute_reply": "2023-03-28T00:25:24.611316Z",
     "shell.execute_reply.started": "2023-03-28T00:25:24.177339Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'Physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b95f2-b37a-42ab-a54e-8e5b9697e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load \n",
    "basepath = Path('/N/project/openalex/slices/Physics/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "#works_concepts = read_parquet('works_concepts')\n",
    "#works_referenced_works = read_parquet('works_referenced_works')\n",
    "\n",
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "# works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "# works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c4aad8-365f-46b6-afc0-fe870d29fa2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:25:24.615740Z",
     "iopub.status.busy": "2023-03-28T00:25:24.615343Z",
     "iopub.status.idle": "2023-03-28T00:25:24.802549Z",
     "shell.execute_reply": "2023-03-28T00:25:24.801765Z",
     "shell.execute_reply.started": "2023-03-28T00:25:24.615692Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Gravitational wave',\n",
    "    'Dark matter',\n",
    "    'Fluid dynamics',\n",
    "    'Soliton',\n",
    "    'Supersymmetry',\n",
    "    'Statistical physics',          \n",
    "    'Superconductivity' \n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f1055-9568-48c6-99c5-7c4b2fb61c5c",
   "metadata": {},
   "source": [
    "### Computer Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2222a9b-8a8d-43b1-9f05-6889aa954db8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:36:30.603717Z",
     "iopub.status.busy": "2023-03-28T00:36:30.603288Z",
     "iopub.status.idle": "2023-03-28T00:36:30.954558Z",
     "shell.execute_reply": "2023-03-28T00:36:30.953531Z",
     "shell.execute_reply.started": "2023-03-28T00:36:30.603679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'CS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d28399ad-7fc4-4838-afdf-46ad6f43caa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T10:42:53.241158Z",
     "iopub.status.busy": "2023-03-16T10:42:53.238107Z",
     "iopub.status.idle": "2023-03-16T10:45:46.432789Z",
     "shell.execute_reply": "2023-03-16T10:45:46.432027Z",
     "shell.execute_reply.started": "2023-03-16T10:42:53.241103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 27,680,033 rows from 'works'\n",
      "Read 83,048,887 rows from 'works_authorships'\n"
     ]
    }
   ],
   "source": [
    "basepath = Path('/N/project/openalex/slices/CS/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "# works_concepts = read_parquet('works_concepts')\n",
    "# works_referenced_works = read_parquet('works_referenced_works')\n",
    "\n",
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "# works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "# works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08dfa9b-f3fb-43c1-aaa8-9639c3c46cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:36:30.956792Z",
     "iopub.status.busy": "2023-03-28T00:36:30.956113Z",
     "iopub.status.idle": "2023-03-28T00:36:31.138908Z",
     "shell.execute_reply": "2023-03-28T00:36:31.137920Z",
     "shell.execute_reply.started": "2023-03-28T00:36:30.956746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Compiler',\n",
    "    'Mobile computing',\n",
    "    'Cryptography',\n",
    "    'Cluster analysis', \n",
    "    'Image processing',\n",
    "    'Parallel computing'         \n",
    "            ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfd4d7-455f-4d5e-8221-370d27393d07",
   "metadata": {},
   "source": [
    "### BioMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8122b2c-6644-48a6-8371-c1671586de57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:39:31.931475Z",
     "iopub.status.busy": "2023-03-28T10:39:31.931263Z",
     "iopub.status.idle": "2023-03-28T10:39:33.101808Z",
     "shell.execute_reply": "2023-03-28T10:39:33.100849Z",
     "shell.execute_reply.started": "2023-03-28T10:39:31.931449Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'BioMed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b117cd0-9379-41f2-9bcb-324f02e5b2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-20T10:17:54.536541Z",
     "iopub.status.busy": "2023-03-20T10:17:54.536221Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 43,528,045 rows from 'works'\n"
     ]
    }
   ],
   "source": [
    "basepath = Path('/N/project/openalex/slices/BioMed/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "# works_concepts = read_parquet('works_concepts')\n",
    "# works_referenced_works = read_parquet('works_referenced_works')\n",
    "\n",
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "# works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "# works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33ae445-1c83-497a-b429-83300cfcbb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:39:33.106239Z",
     "iopub.status.busy": "2023-03-28T10:39:33.105753Z",
     "iopub.status.idle": "2023-03-28T10:39:34.324468Z",
     "shell.execute_reply": "2023-03-28T10:39:34.323369Z",
     "shell.execute_reply.started": "2023-03-28T10:39:33.106212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "            'Protein structure',\n",
    "            'Genome', \n",
    "            'Peptide sequence',\n",
    "            \"Alzheimer's disease\",\n",
    "            'Neurology',          \n",
    "            'Radiation therapy',\n",
    "            'Chemotherapy'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b4ae0-3cd4-4c39-826f-45395ee1586b",
   "metadata": {},
   "source": [
    "## FUNCTIONS DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351e0dd-7744-4295-8d7b-edb49a067a86",
   "metadata": {},
   "source": [
    "### Overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "650834d4-40db-4d7b-9126-adf242d31605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:39:39.223670Z",
     "iopub.status.busy": "2023-03-28T10:39:39.223271Z",
     "iopub.status.idle": "2023-03-28T10:39:39.858404Z",
     "shell.execute_reply": "2023-03-28T10:39:39.857643Z",
     "shell.execute_reply.started": "2023-03-28T10:39:39.223623Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#overlapping sets\n",
    "def sets_overlap(A, B):\n",
    "    #Find intersection of two sets\n",
    "    nominator = A.intersection(B)\n",
    "\n",
    "    #Find union of two sets\n",
    "    #denominator = A.union(B)\n",
    "    denominator = min(len(A),len(B)) #A and B same size\n",
    "\n",
    "    #Take the ratio of sizes\n",
    "    index = len(nominator)/denominator\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1a73e-aaae-41e2-bfbb-8b499acec052",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f1a42-f3cf-4b1d-b1df-42596c507a70",
   "metadata": {},
   "source": [
    "#### Productivity - impact mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c725e555-adf0-4221-ba3c-0adc00f086f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T16:30:22.339429Z",
     "iopub.status.busy": "2023-03-15T16:30:22.337857Z",
     "iopub.status.idle": "2023-03-15T16:30:22.482916Z",
     "shell.execute_reply": "2023-03-15T16:30:22.482259Z",
     "shell.execute_reply.started": "2023-03-15T16:30:22.339399Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Sets_overlap_calc(discipline,topic,my_path):\n",
    "\n",
    "    #load windows\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #load active authors\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    \n",
    "    my_path3 = os.path.join(my_path2,'Productivity')    \n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_prod = pickle.load(fp) \n",
    "        \n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean1')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp1 = pickle.load(fp) \n",
    "    \n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean2')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp2 = pickle.load(fp) \n",
    "        \n",
    "    my_path3 = os.path.join(my_path2,'Impact_mean3')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp3 = pickle.load(fp) \n",
    "    \n",
    "    start_year = 1995\n",
    "    Sets_overlap_dict = {}\n",
    "    for w in range(0,23):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "        \n",
    "            [active_authors_start_prod,samples_dict_prod,n_prod] = active_authors_classes_prod[w]\n",
    "            high_active_authors_prod = samples_dict_prod['top 10%']\n",
    "            low_active_authors_prod = samples_dict_prod['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp1,samples_dict_imp1,n_imp1] = active_authors_classes_imp1[w]\n",
    "            high_active_authors_imp1 = samples_dict_imp1['top 10%']\n",
    "            low_active_authors_imp1 = samples_dict_imp1['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp2,samples_dict_imp2,n_imp2] = active_authors_classes_imp2[w]\n",
    "            high_active_authors_imp2 = samples_dict_imp2['top 10%']\n",
    "            low_active_authors_imp2 = samples_dict_imp2['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp3,samples_dict_imp3,n_imp3] = active_authors_classes_imp3[w]\n",
    "            high_active_authors_imp3 = samples_dict_imp3['top 10%']\n",
    "            low_active_authors_imp3 = samples_dict_imp3['bottom 10%']\n",
    "\n",
    "            #Jaccard index    \n",
    "            ji_impact1_high = sets_overlap(high_active_authors_prod, high_active_authors_imp1)\n",
    "            ji_impact2_high = sets_overlap(high_active_authors_prod, high_active_authors_imp2)\n",
    "            ji_impact3_high = sets_overlap(high_active_authors_prod, high_active_authors_imp3)\n",
    "            \n",
    "            ji_impact1_low = sets_overlap(low_active_authors_prod, low_active_authors_imp1)\n",
    "            ji_impact2_low = sets_overlap(low_active_authors_prod, low_active_authors_imp2)\n",
    "            ji_impact3_low = sets_overlap(low_active_authors_prod, low_active_authors_imp3)\n",
    "   \n",
    "\n",
    "            Sets_overlap_dict[start_year_w] = [ji_impact1_high,ji_impact2_high,ji_impact3_high,ji_impact1_low,ji_impact2_low,ji_impact3_low]  \n",
    "        \n",
    "    Sets_overlap_df = (pd.DataFrame.from_dict(Sets_overlap_dict, orient='index')).rename_axis('T_0').reset_index()\n",
    "    # my_file = 'Sets_overlap_df_windows.csv'\n",
    "    # Sets_overlap_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    \n",
    "    Sets_overlap_df.insert(0, 'topic', topic)\n",
    "\n",
    "    return Sets_overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e8e47c-c73e-4851-96ae-58617bd9ad8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T16:24:33.557997Z",
     "iopub.status.busy": "2023-03-09T16:24:33.557781Z",
     "iopub.status.idle": "2023-03-09T16:24:49.408859Z",
     "shell.execute_reply": "2023-03-09T16:24:49.407370Z",
     "shell.execute_reply.started": "2023-03-09T16:24:33.557970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Sets_overlap_mean')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "Sets_overlap_df = pd.DataFrame() \n",
    "for topic in topic_list:\n",
    "    Sets_overlap_df_top = Sets_overlap_calc(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    Sets_overlap_df = pd.concat([Sets_overlap_df, Sets_overlap_df_top], axis = 0)\n",
    "\n",
    "my_file = 'Sets_overlap_windows.csv'\n",
    "Sets_overlap_df = Sets_overlap_df.rename(columns={0: \"prod_impact1_high\", 1: \"prod_impact2_high\", 2: \"prod_impact3_high\", 3: \"prod_impact1_low\", 4: \"prod_impact2_low\", 5: \"prod_impact3_low\"})\n",
    "Sets_overlap_df.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ff802-d05e-41d3-8a5f-3f812ce00427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a7f4e3-251f-4075-af9a-3a80e82e4345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T14:13:00.841531Z",
     "iopub.status.busy": "2023-03-17T14:13:00.840926Z",
     "iopub.status.idle": "2023-03-17T14:13:01.442219Z",
     "shell.execute_reply": "2023-03-17T14:13:01.440876Z",
     "shell.execute_reply.started": "2023-03-17T14:13:00.841279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#average across windows for each topic\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m my_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(discipline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSets_overlap_mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m my_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSets_overlap_windows.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m Sets_overlap_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(my_path, my_file),index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#average across windows for each topic\n",
    "\n",
    "my_path = os.path.join(discipline, 'Sets_overlap_mean')\n",
    "\n",
    "my_file = 'Sets_overlap_windows.csv'\n",
    "Sets_overlap_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "Sets_overlap_df.drop_duplicates(subset=['topic','T_0'], inplace=True)\n",
    "Sets_overlap_df = Sets_overlap_df.set_index('topic')\n",
    "Sets_overlap_df_ = pd.DataFrame()\n",
    "\n",
    "for topic in topic_list:\n",
    "    df_concept = Sets_overlap_df.query('topic == @topic')\n",
    "    df_concept_mean = df_concept[[\"prod_impact1_high\", \"prod_impact2_high\", \"prod_impact3_high\", \"prod_impact1_low\", \"prod_impact2_low\", \"prod_impact3_low\"]].mean(axis=0).to_frame().rename(columns={0:'mean'}) \n",
    "    df_concept_std = df_concept[[\"prod_impact1_high\", \"prod_impact2_high\", \"prod_impact3_high\", \"prod_impact1_low\", \"prod_impact2_low\", \"prod_impact3_low\"]].std(axis=0).to_frame().rename(columns={0:'std'}) \n",
    "    df_concept_mean = pd.concat([df_concept_mean, df_concept_std], axis = 1)\n",
    "    df_concept_mean = df_concept_mean.transpose().rename_axis('stat').reset_index()\n",
    "\n",
    "    df_concept_mean.insert(0, 'topic', topic)\n",
    "\n",
    "    Sets_overlap_df_ = pd.concat([Sets_overlap_df_, df_concept_mean], axis = 0)\n",
    "\n",
    "my_file = 'Sets_overlap_table.csv'\n",
    "Sets_overlap_df_.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f53da2f-00b4-4874-912f-f3ff5272fb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-15T16:36:12.842140Z",
     "iopub.status.busy": "2023-03-15T16:36:12.841809Z",
     "iopub.status.idle": "2023-03-15T16:36:13.276250Z",
     "shell.execute_reply": "2023-03-15T16:36:13.275708Z",
     "shell.execute_reply.started": "2023-03-15T16:36:12.842102Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tables paper\n",
    "my_path = os.path.join(discipline, 'Sets_overlap_mean')\n",
    "\n",
    "my_file = 'Sets_overlap_table.csv'\n",
    "Sets_overlap_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "Sets_overlap_df = Sets_overlap_df.reset_index(drop=True)\n",
    "\n",
    "Sets_overlap_mean_ = pd.DataFrame()\n",
    "for r in list(range(0, len(Sets_overlap_df),2)):\n",
    "    Sets_overlap_mean = pd.DataFrame()\n",
    "    Sets_overlap_mean[\"prod_impact1_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact1_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact1_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean[\"prod_impact2_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact2_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact2_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean[\"prod_impact3_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact3_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact3_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean[\"prod_impact1_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact1_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact1_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean[\"prod_impact2_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact2_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact2_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean[\"prod_impact3_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact3_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact3_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_mean.insert(0, \"topic\", Sets_overlap_df.loc[[r]][\"topic\"].reset_index(drop=True)[0])\n",
    "    Sets_overlap_mean_ = pd.concat([Sets_overlap_mean_, Sets_overlap_mean], axis=0)\n",
    "my_file = 'Sets_overlap_mean.csv'\n",
    "Sets_overlap_mean_.to_csv(os.path.join(my_path, my_file), sep='&', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0a681-f365-49c2-9daa-dba40a575bf1",
   "metadata": {},
   "source": [
    "#### Productivity - impact sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8e8d1b-6232-46ad-860c-f546862a8408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:40:07.702069Z",
     "iopub.status.busy": "2023-03-28T10:40:07.701652Z",
     "iopub.status.idle": "2023-03-28T10:40:08.038993Z",
     "shell.execute_reply": "2023-03-28T10:40:08.038166Z",
     "shell.execute_reply.started": "2023-03-28T10:40:07.702009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Sets_overlap_calc2(discipline,topic,my_path):\n",
    "\n",
    "    #load windows\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #load active authors\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    \n",
    "    my_path3 = os.path.join(my_path2,'Productivity')    \n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_prod = pickle.load(fp) \n",
    "        \n",
    "    my_path3 = os.path.join(my_path2,'Impact_sum1')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp1 = pickle.load(fp) \n",
    "    \n",
    "    my_path3 = os.path.join(my_path2,'Impact_sum2')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp2 = pickle.load(fp) \n",
    "        \n",
    "    my_path3 = os.path.join(my_path2,'Impact_sum3')\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes_imp3 = pickle.load(fp) \n",
    "    \n",
    "    start_year = 1995\n",
    "    Sets_overlap_dict = {}\n",
    "    for w in range(0,23):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "        \n",
    "            [active_authors_start_prod,samples_dict_prod,n_prod] = active_authors_classes_prod[w]\n",
    "            high_active_authors_prod = samples_dict_prod['top 10%']\n",
    "            low_active_authors_prod = samples_dict_prod['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp1,samples_dict_imp1,n_imp1] = active_authors_classes_imp1[w]\n",
    "            high_active_authors_imp1 = samples_dict_imp1['top 10%']\n",
    "            low_active_authors_imp1 = samples_dict_imp1['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp2,samples_dict_imp2,n_imp2] = active_authors_classes_imp2[w]\n",
    "            high_active_authors_imp2 = samples_dict_imp2['top 10%']\n",
    "            low_active_authors_imp2 = samples_dict_imp2['bottom 10%']\n",
    "            \n",
    "            [active_authors_start_imp3,samples_dict_imp3,n_imp3] = active_authors_classes_imp3[w]\n",
    "            high_active_authors_imp3 = samples_dict_imp3['top 10%']\n",
    "            low_active_authors_imp3 = samples_dict_imp3['bottom 10%']\n",
    "\n",
    "            #Jaccard index    \n",
    "            ji_impact1_high = sets_overlap(high_active_authors_prod, high_active_authors_imp1)\n",
    "            ji_impact2_high = sets_overlap(high_active_authors_prod, high_active_authors_imp2)\n",
    "            ji_impact3_high = sets_overlap(high_active_authors_prod, high_active_authors_imp3)\n",
    "            \n",
    "            ji_impact1_low = sets_overlap(low_active_authors_prod, low_active_authors_imp1)\n",
    "            ji_impact2_low = sets_overlap(low_active_authors_prod, low_active_authors_imp2)\n",
    "            ji_impact3_low = sets_overlap(low_active_authors_prod, low_active_authors_imp3)\n",
    "   \n",
    "\n",
    "            Sets_overlap_dict[start_year_w] = [ji_impact1_high,ji_impact2_high,ji_impact3_high,ji_impact1_low,ji_impact2_low,ji_impact3_low]  \n",
    "        \n",
    "    Sets_overlap_df = (pd.DataFrame.from_dict(Sets_overlap_dict, orient='index')).rename_axis('T_0').reset_index()\n",
    "    # my_file = 'Sets_overlap_df_windows.csv'\n",
    "    # Sets_overlap_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    \n",
    "    Sets_overlap_df.insert(0, 'topic', topic)\n",
    "\n",
    "    return Sets_overlap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "215856fc-e2a8-4935-bbba-c4734557a3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:40:08.127349Z",
     "iopub.status.busy": "2023-03-28T10:40:08.126866Z",
     "iopub.status.idle": "2023-03-28T10:40:19.646375Z",
     "shell.execute_reply": "2023-03-28T10:40:19.644989Z",
     "shell.execute_reply.started": "2023-03-28T10:40:08.127299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Sets_overlap_sum')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "Sets_overlap_df = pd.DataFrame() \n",
    "for topic in topic_list:\n",
    "    Sets_overlap_df_top = Sets_overlap_calc2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    Sets_overlap_df = pd.concat([Sets_overlap_df, Sets_overlap_df_top], axis = 0)\n",
    "\n",
    "my_file = 'Sets_overlap_windows.csv'\n",
    "Sets_overlap_df = Sets_overlap_df.rename(columns={0: \"prod_impact1_high\", 1: \"prod_impact2_high\", 2: \"prod_impact3_high\", 3: \"prod_impact1_low\", 4: \"prod_impact2_low\", 5: \"prod_impact3_low\"})\n",
    "Sets_overlap_df.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf22b08-bca9-40d5-a389-486ea334fe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ddd5869-9d26-4de6-a783-21fd5650d698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:40:19.649432Z",
     "iopub.status.busy": "2023-03-28T10:40:19.648962Z",
     "iopub.status.idle": "2023-03-28T10:40:20.046454Z",
     "shell.execute_reply": "2023-03-28T10:40:20.045575Z",
     "shell.execute_reply.started": "2023-03-28T10:40:19.649387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#average across windows for each topic\n",
    "\n",
    "my_path = os.path.join(discipline, 'Sets_overlap_sum')\n",
    "\n",
    "my_file = 'Sets_overlap_windows.csv'\n",
    "Sets_overlap_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "Sets_overlap_df.drop_duplicates(subset=['topic','T_0'], inplace=True)\n",
    "Sets_overlap_df = Sets_overlap_df.set_index('topic')\n",
    "Sets_overlap_df_ = pd.DataFrame()\n",
    "\n",
    "for topic in topic_list:\n",
    "    df_concept = Sets_overlap_df.query('topic == @topic')\n",
    "    df_concept_mean = df_concept[[\"prod_impact1_high\", \"prod_impact2_high\", \"prod_impact3_high\", \"prod_impact1_low\", \"prod_impact2_low\", \"prod_impact3_low\"]].mean(axis=0).to_frame().rename(columns={0:'mean'}) \n",
    "    df_concept_std = df_concept[[\"prod_impact1_high\", \"prod_impact2_high\", \"prod_impact3_high\", \"prod_impact1_low\", \"prod_impact2_low\", \"prod_impact3_low\"]].std(axis=0).to_frame().rename(columns={0:'std'}) \n",
    "    df_concept_mean = pd.concat([df_concept_mean, df_concept_std], axis = 1)\n",
    "    df_concept_mean = df_concept_mean.transpose().rename_axis('stat').reset_index()\n",
    "\n",
    "    df_concept_mean.insert(0, 'topic', topic)\n",
    "\n",
    "    Sets_overlap_df_ = pd.concat([Sets_overlap_df_, df_concept_mean], axis = 0)\n",
    "\n",
    "my_file = 'Sets_overlap_table.csv'\n",
    "Sets_overlap_df_.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0e09cc-7f5a-4b20-9f46-2ce2af58e35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T10:40:20.048505Z",
     "iopub.status.busy": "2023-03-28T10:40:20.048031Z",
     "iopub.status.idle": "2023-03-28T10:40:20.477353Z",
     "shell.execute_reply": "2023-03-28T10:40:20.476736Z",
     "shell.execute_reply.started": "2023-03-28T10:40:20.048455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tables paper\n",
    "my_path = os.path.join(discipline, 'Sets_overlap_sum')\n",
    "\n",
    "my_file = 'Sets_overlap_table.csv'\n",
    "Sets_overlap_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "Sets_overlap_df = Sets_overlap_df.reset_index(drop=True)\n",
    "\n",
    "Sets_overlap_sum_ = pd.DataFrame()\n",
    "for r in list(range(0, len(Sets_overlap_df),2)):\n",
    "    Sets_overlap_sum = pd.DataFrame()\n",
    "    Sets_overlap_sum[\"prod_impact1_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact1_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact1_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum[\"prod_impact2_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact2_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact2_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum[\"prod_impact3_high\"] = Sets_overlap_df.loc[[r]][\"prod_impact3_high\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact3_high\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum[\"prod_impact1_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact1_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact1_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum[\"prod_impact2_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact2_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact2_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum[\"prod_impact3_low\"] = Sets_overlap_df.loc[[r]][\"prod_impact3_low\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + Sets_overlap_df.loc[[r+1]][\"prod_impact3_low\"].reset_index(drop=True).round(2).astype(str)\n",
    "    Sets_overlap_sum.insert(0, \"topic\", Sets_overlap_df.loc[[r]][\"topic\"].reset_index(drop=True)[0])\n",
    "    Sets_overlap_sum_ = pd.concat([Sets_overlap_sum_, Sets_overlap_sum], axis=0)\n",
    "my_file = 'Sets_overlap_sum.csv'\n",
    "Sets_overlap_sum_.to_csv(os.path.join(my_path, my_file), sep='&', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695972a-a7d1-40e7-bbb6-c762ddff1f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215b97c-dbe9-4074-910e-ccd10c4f227a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e85bd-f05f-40d9-b0cc-71dad1e75d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211a0a1-8bca-4387-a84a-abf7539a3e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c6040-c5c2-4d9b-afc1-b21a8f20f014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f81959fb-bf54-42ee-8add-95319fbbe65b",
   "metadata": {},
   "source": [
    "# TOPIC CONNECTEDNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4271c9-88e3-416e-b5dc-4a8a5dd738fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T12:04:35.115260Z",
     "iopub.status.busy": "2023-03-22T12:04:35.114847Z",
     "iopub.status.idle": "2023-03-22T12:04:35.339241Z",
     "shell.execute_reply": "2023-03-22T12:04:35.338573Z",
     "shell.execute_reply.started": "2023-03-22T12:04:35.115210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def topic_conn_calc(discipline,topic,my_path):\n",
    "    \n",
    "    #load windows\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "\n",
    "    #load active authors\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    my_path3 = os.path.join(my_path2,'Productivity')    \n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)\n",
    "        \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "\n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "        \n",
    "    start_year = 1995\n",
    "    topics_conn_dict = {}\n",
    "    for w in tqdm(range(0,23)): \n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).drop_duplicates(subset=['work_id', 'author_id']).reset_index(drop=True)\n",
    "            \n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  all_coauthors.intersection(set(works_authors_collab.author_id))\n",
    "            G = nx.bipartite.weighted_projected_graph(bip_g, nodes=author_ids_supp)  #no weights\n",
    "            \n",
    "            \n",
    "            #analysis\n",
    "            conn_comps = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "            conn_comps_len =  [len(c) for c in conn_comps]\n",
    "            largest_cc = conn_comps[0]\n",
    "            S = G.subgraph(largest_cc).copy()\n",
    "            \n",
    "            topics_conn_dict[start_year_w] = [\n",
    "                G.number_of_nodes(),\n",
    "                G.number_of_edges(),\n",
    "                len(active_authors_start),\n",
    "                #conn_comps_len,\n",
    "                len(conn_comps_len),\n",
    "                conn_comps_len[0],\n",
    "                S.number_of_edges(),\n",
    "                nx.average_clustering(G)    \n",
    "            ] \n",
    "            \n",
    "    topics_conn_df = (pd.DataFrame.from_dict(topics_conn_dict, orient='index')).rename_axis('T_0').reset_index()\n",
    "    my_file = 'topics_conn_windows_'+topic+'.csv'\n",
    "    topics_conn_df.to_csv(os.path.join(my_path, my_file), sep=';')\n",
    "    \n",
    "    topics_conn_df.insert(0, 'topic', topic)\n",
    "\n",
    "    return topics_conn_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdda52-b40f-4453-9774-88eff0d32f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'topics_conn_mean')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_conn_df = pd.DataFrame() \n",
    "for topic in topic_list:\n",
    "    topics_conn_df_top = topic_conn_calc(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_conn_df = pd.concat([topics_conn_df, topics_conn_df_top], axis = 0)\n",
    "\n",
    "my_file = 'topics_conn_windows.csv'\n",
    "topics_conn_df = topics_conn_df.rename(columns={0: 'N',1: 'E',2: '#active_authors',3: '#cc',4: 'lcc_N',5: 'lcc_E',6: 'avg_clust'})\n",
    "topics_conn_df.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc18d6d-ce7c-4594-8ab8-ddb8611bd26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ceeafba-0136-41a6-b829-9ad5e10491f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T12:04:37.012281Z",
     "iopub.status.busy": "2023-03-22T12:04:37.012037Z",
     "iopub.status.idle": "2023-03-22T12:04:37.405339Z",
     "shell.execute_reply": "2023-03-22T12:04:37.404402Z",
     "shell.execute_reply.started": "2023-03-22T12:04:37.012255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#average across windows for each topic\n",
    "\n",
    "my_path = os.path.join(discipline, 'topics_conn_mean')\n",
    "\n",
    "my_file = 'topics_conn_windows.csv'\n",
    "topics_conn_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "topics_conn_df.drop_duplicates(subset=['topic','T_0'], inplace=True)\n",
    "topics_conn_df = topics_conn_df.set_index('topic')\n",
    "topics_conn_df_ = pd.DataFrame()\n",
    "\n",
    "for topic in topic_list:\n",
    "    df_concept = topics_conn_df.query('topic == @topic')\n",
    "    df_concept_mean = df_concept[['N','E','#active_authors','#cc','lcc_N','lcc_E','avg_clust']].mean(axis=0).to_frame().rename(columns={0:'mean'}) \n",
    "    df_concept_std = df_concept[['N','E','#active_authors','#cc','lcc_N','lcc_E','avg_clust']].std(axis=0).to_frame().rename(columns={0:'std'}) \n",
    "    df_concept_mean = pd.concat([df_concept_mean, df_concept_std], axis = 1)\n",
    "    df_concept_mean = df_concept_mean.transpose().rename_axis('stat').reset_index()\n",
    "\n",
    "    df_concept_mean.insert(0, 'topic', topic)\n",
    "\n",
    "    topics_conn_df_ = pd.concat([topics_conn_df_, df_concept_mean], axis = 0)\n",
    "\n",
    "my_file = 'topics_conn_table.csv'\n",
    "topics_conn_df_.to_csv(os.path.join(my_path, my_file), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f57ba78-426b-4ea9-9f95-d1b4e5c02395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-22T12:04:37.597707Z",
     "iopub.status.busy": "2023-03-22T12:04:37.594669Z",
     "iopub.status.idle": "2023-03-22T12:04:38.037140Z",
     "shell.execute_reply": "2023-03-22T12:04:38.036215Z",
     "shell.execute_reply.started": "2023-03-22T12:04:37.597631Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tables paper\n",
    "my_path = os.path.join(discipline, 'topics_conn_mean')\n",
    "\n",
    "my_file = 'topics_conn_table.csv'\n",
    "topics_conn_df = pd.read_csv(os.path.join(my_path, my_file),index_col=0, sep=';')\n",
    "topics_conn_df = topics_conn_df.reset_index(drop=True)\n",
    "\n",
    "topics_conn_mean_ = pd.DataFrame()\n",
    "for r in list(range(0, len(topics_conn_df),2)):\n",
    "    topics_conn_mean = pd.DataFrame()\n",
    "    topics_conn_mean[\"N\"] = topics_conn_df.loc[[r]][\"N\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"N\"].reset_index(drop=True).round(2).astype(str)\n",
    "    topics_conn_mean[\"E\"] = topics_conn_df.loc[[r]][\"E\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"E\"].reset_index(drop=True).round(2).astype(str)   \n",
    "    topics_conn_mean[\"#active_authors\"] = topics_conn_df.loc[[r]][\"#active_authors\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"#active_authors\"].reset_index(drop=True).round(2).astype(str)    \n",
    "    topics_conn_mean[\"#cc\"] = topics_conn_df.loc[[r]][\"#cc\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"#cc\"].reset_index(drop=True).round(2).astype(str)    \n",
    "    topics_conn_mean[\"lcc_N\"] = topics_conn_df.loc[[r]][\"lcc_N\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"lcc_N\"].reset_index(drop=True).round(2).astype(str)    \n",
    "    topics_conn_mean[\"lcc_E\"] = topics_conn_df.loc[[r]][\"lcc_E\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"lcc_E\"].reset_index(drop=True).round(2).astype(str)    \n",
    "    topics_conn_mean[\"avg_clust\"] = topics_conn_df.loc[[r]][\"avg_clust\"].reset_index(drop=True).round(2).astype(str) + \"$\\pm$\" + topics_conn_df.loc[[r+1]][\"avg_clust\"].reset_index(drop=True).round(2).astype(str)\n",
    "    topics_conn_mean.insert(0, \"topic\", topics_conn_df.loc[[r]][\"topic\"].reset_index(drop=True)[0])\n",
    "    topics_conn_mean_ = pd.concat([topics_conn_mean_, topics_conn_mean], axis=0)\n",
    "    \n",
    "my_file = 'topics_conn_mean.csv'\n",
    "topics_conn_mean_.to_csv(os.path.join(my_path, my_file), sep='&', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
