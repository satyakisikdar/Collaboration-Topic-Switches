{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058c9df4-6c38-4d2e-ba71-fae65f9671d0",
   "metadata": {},
   "source": [
    "# EXPERIMENT I - CLOSURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408913d6-638b-4b59-bcfa-25b75100b7a2",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "* Create a minimal environment to run the notebooks \n",
    "* Add scripts to generate the slices \n",
    "* Add a step by step instruction to run the resutls \n",
    "* Add notebook to generate the plots -- **upload aggregated CSVs to regenerate Figures in the paper**\n",
    "* Upload the final CSVs and the directory structure to github "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46eb29f-edb5-403b-8d27-a1b3f393ae37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:36:05.162316Z",
     "iopub.status.busy": "2023-04-17T19:36:05.161906Z",
     "iopub.status.idle": "2023-04-17T19:36:05.206856Z",
     "shell.execute_reply": "2023-04-17T19:36:05.205861Z",
     "shell.execute_reply.started": "2023-04-17T19:36:05.162284Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7198631-5bc8-4611-89a2-b0cc1fe4c165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:36:05.652400Z",
     "iopub.status.busy": "2023-04-17T19:36:05.651784Z",
     "iopub.status.idle": "2023-04-17T19:36:05.797603Z",
     "shell.execute_reply": "2023-04-17T19:36:05.796897Z",
     "shell.execute_reply.started": "2023-04-17T19:36:05.652370Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/N/slate/ssikdar/envs/topic-switch/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0bb736c-63ca-46b9-a186-74b26225f2c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:43:59.455988Z",
     "iopub.status.busy": "2023-04-17T21:43:59.455738Z",
     "iopub.status.idle": "2023-04-17T21:44:00.231359Z",
     "shell.execute_reply": "2023-04-17T21:44:00.230145Z",
     "shell.execute_reply.started": "2023-04-17T21:43:59.455966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import requests \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from itertools import combinations\n",
    "import sys \n",
    "from statistics import mean, stdev\n",
    "import struct, io, string\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from scipy import stats \n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d46c2-e2b2-4b09-9174-161484699e82",
   "metadata": {},
   "source": [
    "## LOAD FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb25ba38-2ace-432b-bdd1-4b8eee90dbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:52:14.816452Z",
     "iopub.status.busy": "2023-04-17T19:52:14.816124Z",
     "iopub.status.idle": "2023-04-17T19:52:14.882289Z",
     "shell.execute_reply": "2023-04-17T19:52:14.881082Z",
     "shell.execute_reply.started": "2023-04-17T19:52:14.816418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'Physics'  # 'CS'  or 'BioMed'\n",
    "basepath = Path('..') # base location of parquets/results\n",
    "\n",
    "datapath = basepath / 'data' / discipline \n",
    "assert datapath.exists(), f'{datapath} does not exists. Please make sure to make the directories and download the Zip files from Zenodo' \n",
    "for files in ['works', 'works_authorships', 'works_concepts', 'works_referenced_works']:\n",
    "    assert (datapath / f'{files}.parquet').exists()\n",
    "\n",
    "resultspath = basepath / 'results' / discipline\n",
    "\n",
    "if not resultspath.exists():\n",
    "    print(f'Creating {resultspath}') \n",
    "    resultspath.mkdir(parents=True)\n",
    "    \n",
    "topic_list = get_topic_list(discipline)[: 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dc23a0-c63d-4626-b8c9-697a5d7b56dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:41:08.910825Z",
     "iopub.status.busy": "2023-04-17T19:41:08.910407Z",
     "iopub.status.idle": "2023-04-17T19:43:10.662249Z",
     "shell.execute_reply": "2023-04-17T19:43:10.661005Z",
     "shell.execute_reply.started": "2023-04-17T19:41:08.910785Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 19,767,236 rows from 'works'\n",
      "Read 71,608,089 rows from 'works_authorships'\n",
      "Read 301,614,148 rows from 'works_concepts'\n",
      "Read 156,179,638 rows from 'works_referenced_works'\n"
     ]
    }
   ],
   "source": [
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c11c007-78d9-4293-af3a-f5ee3a5c3aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:43:15.546915Z",
     "iopub.status.busy": "2023-04-17T19:43:15.544661Z",
     "iopub.status.idle": "2023-04-17T19:45:19.940549Z",
     "shell.execute_reply": "2023-04-17T19:45:19.939107Z",
     "shell.execute_reply.started": "2023-04-17T19:43:15.546879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "works['num_authors'] = works['num_authors'].astype('int64')  # set the datatype of num_authors to int64\n",
    "works['n_coauthors'] = works['num_authors'] - 1  # add new column for number of coauthors for a work\n",
    "\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")  # add publication date to works authors table\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)  # drop multiple affiliations for the same author \n",
    "\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')  # filter out rows with scores < 0.3 \n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")  # add publication date to the works concepts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aabcc567-3d16-4cec-aa16-98c30302f009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:02:16.684367Z",
     "iopub.status.busy": "2023-04-17T20:02:16.683868Z",
     "iopub.status.idle": "2023-04-17T20:05:16.745129Z",
     "shell.execute_reply": "2023-04-17T20:05:16.743349Z",
     "shell.execute_reply.started": "2023-04-17T20:02:16.684332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#used in def. impact 2 \n",
    "\n",
    "works_cit_counts_year = works_referenced_works.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "works_cit_counts_year.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "\n",
    "index = pd.MultiIndex.from_product(works_cit_counts_year.index.levels)\n",
    "works_cit_counts_year = works_cit_counts_year.reindex(index)\n",
    "works_cit_counts_year = works_cit_counts_year.reset_index(level=0).reset_index(level=0)\n",
    "works_cit_counts_year = works_cit_counts_year.fillna(0)\n",
    "works_cit_counts_year['cit_count_cum'] = works_cit_counts_year.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "\n",
    "works_cit_counts_year = works_cit_counts_year.rename(columns = {'referenced_work_id':'work_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc67e2f-1046-437f-847c-fa8d23f97c6c",
   "metadata": {},
   "source": [
    "## FUNCTIONS DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4e712-b795-4723-8d60-7e6bf976c88d",
   "metadata": {},
   "source": [
    "### Definition experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9bbcd91-f913-4ed5-9951-fc740999bc2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:42.128989Z",
     "iopub.status.busy": "2023-04-17T19:45:42.128605Z",
     "iopub.status.idle": "2023-04-17T19:45:42.196708Z",
     "shell.execute_reply": "2023-04-17T19:45:42.195877Z",
     "shell.execute_reply.started": "2023-04-17T19:45:42.128956Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact1 - papers:all, cits:topic\n",
    "def experts_impact_mean_1(works_authors,start_year_i,active_authors_start,works_cit_counts_year_concept):\n",
    "\n",
    "    #papers:all, citations:just tagged with concept \n",
    "    #all papers (with and without concept) written before start_date by active authors\n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('@start_year_i - 5 <= publication_year < @start_year_i', engine='python')\n",
    "                    .query('author_id.isin(@active_authors_start)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_concept_startyear = works_cit_counts_year_concept.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_concept_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3049e452-a496-4bb5-b98e-c00a9bb9244b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:42.467661Z",
     "iopub.status.busy": "2023-04-17T19:45:42.467401Z",
     "iopub.status.idle": "2023-04-17T19:45:42.518929Z",
     "shell.execute_reply": "2023-04-17T19:45:42.518233Z",
     "shell.execute_reply.started": "2023-04-17T19:45:42.467638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experts_productivity(works_authors,prior_work_ids_5yr,active_authors_start):\n",
    "    #count number of works written with topic during exposure window\n",
    "    sorted_author_works_count = (\n",
    "    works_authors\n",
    "    .query('work_id.isin(@prior_work_ids_5yr) & author_id.isin(@active_authors_start)') \n",
    "    .groupby('author_id')\n",
    "    .work_id\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    sorted_author_works_count_len = len(sorted_author_works_count)\n",
    "    \n",
    "    sorted_author_works_count = sorted_author_works_count.to_frame().reset_index()\n",
    "    sorted_author_works_count.columns = ['author_id', 'val']\n",
    "    \n",
    "    return sorted_author_works_count,sorted_author_works_count_len "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2601b15-ea02-4060-8612-b0eaba005771",
   "metadata": {},
   "source": [
    "### Get author samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa85152-5ca8-45a3-ac7e-3254f26fcaf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:44.921496Z",
     "iopub.status.busy": "2023-04-17T19:45:44.921191Z",
     "iopub.status.idle": "2023-04-17T19:45:44.983751Z",
     "shell.execute_reply": "2023-04-17T19:45:44.982975Z",
     "shell.execute_reply.started": "2023-04-17T19:45:44.921470Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_author_samples(author_stats_df, top_k, debug=False):\n",
    "    \"\"\"\n",
    "    author_stats_df: DataFrame where author_id has active author ids, and val has the productivity/impact values for that author\n",
    "    top_k: either 10 or 20 depending on top 10 or 20%\n",
    "    \n",
    "    Returns a dictionary where keys are class labels, and values are set of author IDs\n",
    "    \"\"\"\n",
    "    # Note highest scoring authors are ranked LAST \n",
    "    author_stats_df.loc[:, 'rank_pct'] = author_stats_df.val.rank(method='min', pct=True)  # rank rows based on val convert to percentiles\n",
    "    \n",
    "    if top_k == 10:\n",
    "        bins = [0, 0.1, 0.3, 0.45, 0.55, 0.7, 0.9, 1]\n",
    "        labels=['bottom 10%', '10-30%', '30-45%', 'middle 10%', '55-70%', '70-90%', 'top 10%']\n",
    "    else:\n",
    "        bins = [0, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 1]\n",
    "        labels=['bottom 20%', '20-30%', '30-40%', 'middle 20%', '60-70%', '70-80%', 'top 20%']\n",
    "        \n",
    "    author_stats_df.loc[:, 'rank_cat'] = (  # assign category labels based on rank percentiles \n",
    "        pd.cut(\n",
    "            author_stats_df.rank_pct,\n",
    "            bins=bins,\n",
    "            labels=labels\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    samples_per_class = max(int((top_k / 100) * author_stats_df.author_id.nunique()), 1)\n",
    "    if debug:\n",
    "        print(f'{top_k=} taking {samples_per_class=:,}')\n",
    "        display(author_stats_df.head(2))\n",
    "    \n",
    "    buckets_size = list(author_stats_df.groupby('rank_cat').count()['rank_pct'])\n",
    "    #print(buckets_size)\n",
    "    \n",
    "    samples_dict = {}\n",
    "    \n",
    "    #keep = [f'bottom {top_k}%', f'middle {top_k}%', f'top {top_k}%']  # keep only these classes\n",
    "    keep = [f'bottom {top_k}%', f'top {top_k}%']\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in keep:\n",
    "            continue\n",
    "        \n",
    "        #initial bucket     \n",
    "        candidates = set(author_stats_df[author_stats_df.rank_cat==label].author_id)\n",
    "        candidates_size = buckets_size[i] #len(candidates)\n",
    "        if candidates_size >=  samples_per_class:\n",
    "            if debug:\n",
    "                print(f'{label}: Sampling {samples_per_class:,} from {len(candidates):,} candidates')\n",
    "            samples = set(random.sample(list(candidates), samples_per_class))  # sample here\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f'Insufficient items in {label}. Need {samples_per_class:,} have {len(candidates):,}')\n",
    "            samples = candidates  # pick everyone\n",
    "    \n",
    "        missing = samples_per_class - len(samples)\n",
    "        if missing > 0: \n",
    "            \n",
    "            #1 next bucket \n",
    "            if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i+1]\n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i+1]\n",
    "            else: # for the highest bucket, go one below\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i-1] \n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i-1]\n",
    "\n",
    "            if candidate_size >= missing:    \n",
    "                new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                samples = samples | new_samples  # add these new samples\n",
    "                if debug:\n",
    "                    print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "            else: \n",
    "                new_samples = candidates  # pick everyone\n",
    "                samples = samples | new_samples\n",
    "            \n",
    "            missing = samples_per_class - len(samples)\n",
    "            if missing > 0: \n",
    "\n",
    "                #2 next bucket \n",
    "                if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i+2]\n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i+2]\n",
    "                else: # for the highest bucket, go one below\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i-2] \n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i-2]\n",
    "                \n",
    "                if candidate_size >= missing:    \n",
    "                    new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                    samples = samples | new_samples  # add these new samples\n",
    "                    if debug:\n",
    "                        print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "                else: \n",
    "                    new_samples = candidates  # pick everyone\n",
    "                    samples = samples | new_samples\n",
    "    \n",
    "        assert len(samples) == samples_per_class, f'Count mismatch {len(samples)=} {samples_per_class=} for samples {label}'\n",
    "        samples_dict[label] = samples\n",
    "        \n",
    "    return samples_dict,samples_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e6510-9823-40e0-82d0-c8def3edd997",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588ba6e1-f45a-41ed-946b-ea410f50bed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:46.755375Z",
     "iopub.status.busy": "2023-04-17T19:45:46.755143Z",
     "iopub.status.idle": "2023-04-17T19:45:46.805372Z",
     "shell.execute_reply": "2023-04-17T19:45:46.804592Z",
     "shell.execute_reply.started": "2023-04-17T19:45:46.755354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_support_graph_ver1(bip_g, author_ids_supp):\n",
    "    support_graph_ = nx.bipartite.weighted_projected_graph(bip_g, nodes=author_ids_supp)\n",
    "    return support_graph_\n",
    "\n",
    "def get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final):\n",
    "    \n",
    "    neighbors_active = n_anas & active_authors_start #for each neighbors at the beginning that is active \n",
    "    if len(neighbors_active)!=0:      \n",
    "        #consider just active neighbors\n",
    "        neighbors_active.add(anas)\n",
    "        ego_active = support_graph_.subgraph(neighbors_active).copy()\n",
    "        #sum weights #number contacts with active authors in exposure window from activation date \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final.keys():\n",
    "            dict_final[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final[exposure_anas_start] = [anas]\n",
    "    else:\n",
    "        ego_active = nx.empty_graph() #empty\n",
    "        #add info to dictionary\n",
    "        if 0 in dict_final.keys():\n",
    "            dict_final[0].append(anas)\n",
    "        else:\n",
    "            dict_final[0] = [anas]\n",
    "                   \n",
    "    return dict_final,ego_active\n",
    "\n",
    "def get_scores_B_ver1(anas,n_anas, high_active_authors,low_active_authors,ego_active_total,dict_final_high,dict_final_low):\n",
    "    \n",
    "    neighbors_active_high = n_anas & high_active_authors   \n",
    "    neighbors_active_low = n_anas & low_active_authors  \n",
    "    \n",
    "    if len(neighbors_active_low)==0 and len(neighbors_active_high)!=0: #just contact with high (not low)\n",
    "        #consider just active neighbors\n",
    "        neighbors_active_high.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_high).copy()\n",
    "        #sum weights #number papers written with active authors in exposure window from activation date  \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final_high.keys():\n",
    "            dict_final_high[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_high[exposure_anas_start] = [anas]\n",
    "            \n",
    "    #low active 10%         \n",
    "    if len(neighbors_active_high)==0 and len(neighbors_active_low)!=0: \n",
    "        neighbors_active_low.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_low).copy() \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        if exposure_anas_start in dict_final_low.keys():\n",
    "            dict_final_low[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_low[exposure_anas_start] = [anas] \n",
    "            \n",
    "    return dict_final_high,dict_final_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5135f82-4f0e-4d5f-bb4d-4b210318a923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:47.740215Z",
     "iopub.status.busy": "2023-04-17T19:45:47.739866Z",
     "iopub.status.idle": "2023-04-17T19:45:47.800953Z",
     "shell.execute_reply": "2023-04-17T19:45:47.800083Z",
     "shell.execute_reply.started": "2023-04-17T19:45:47.740188Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_works(G, u, v):\n",
    "    w = set(G[u]) & set(G[v]) #works written together #G[u] neighbors of u in bipartite graph #weights are sets of works written by the two authors\n",
    "    return w\n",
    "\n",
    "def get_support_graph_ver2(bip_g, author_ids_supp,list_works):\n",
    "    #weighted graph number papers\n",
    "    support_graph_ = nx.bipartite.generic_weighted_projected_graph(bip_g, nodes=author_ids_supp, weight_function=list_works)\n",
    "    return support_graph_\n",
    "\n",
    "def get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final):\n",
    "    \n",
    "    neighbors_active = n_anas & active_authors_start #for each neighbors at the beginning that is active \n",
    "    #ego network anas\n",
    "    #ego = nx.ego_graph(support_graph_, anas)\n",
    "    if len(neighbors_active)!=0:      \n",
    "        #consider just active neighbors\n",
    "        neighbors_active.add(anas)\n",
    "        ego_active = support_graph_.subgraph(neighbors_active).copy()\n",
    "        #sum weights #number papers written with active authors in exposure window from activation date \n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final.keys():\n",
    "            dict_final[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final[exposure_anas_start] = [anas]\n",
    "    else:\n",
    "        ego_active = nx.empty_graph() #empty\n",
    "        #add info to dictionary\n",
    "        if 0 in dict_final.keys():\n",
    "            dict_final[0].append(anas)\n",
    "        else:\n",
    "            dict_final[0] = [anas]\n",
    "                   \n",
    "    #return dict_final,ego\n",
    "    return dict_final,ego_active\n",
    "\n",
    "def get_scores_B_ver2(anas,n_anas,high_active_authors,low_active_authors,ego_active_total,dict_final_high,dict_final_low):\n",
    "    \n",
    "    \n",
    "    neighbors_active_high = n_anas & high_active_authors   \n",
    "    neighbors_active_low = n_anas & low_active_authors  \n",
    "    \n",
    "    if len(neighbors_active_low)==0 and len(neighbors_active_high)!=0: #just contact with high (not low)\n",
    "        #consider just active neighbors\n",
    "        neighbors_active_high.add(anas)\n",
    "        #ego_active = ego.subgraph(neighbors_active_high).copy()\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_high).copy()\n",
    "        #sum weights #number papers written with infected authors in exposure window from activation date\n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final_high.keys():\n",
    "            dict_final_high[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_high[exposure_anas_start] = [anas]\n",
    "            \n",
    "    #low active 10%         \n",
    "    if len(neighbors_active_high)==0 and len(neighbors_active_low)!=0: \n",
    "        neighbors_active_low.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_low).copy() \n",
    "        #sum weights #number papers written with infected authors in exposure window from activation date\n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        if exposure_anas_start in dict_final_low.keys():\n",
    "            dict_final_low[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_low[exposure_anas_start] = [anas] \n",
    "            \n",
    "    return dict_final_high,dict_final_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57b155-d7da-4d9d-91df-4af921748d69",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a7e6d10-e33c-43d5-8e87-f4543acbf395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:45:50.591909Z",
     "iopub.status.busy": "2023-04-17T19:45:50.591572Z",
     "iopub.status.idle": "2023-04-17T19:45:50.651532Z",
     "shell.execute_reply": "2023-04-17T19:45:50.650843Z",
     "shell.execute_reply.started": "2023-04-17T19:45:50.591880Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculation_A(i,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated):   \n",
    "    \n",
    "    dict_k_frac = {}\n",
    "    dict_k_num = {} #numerator\n",
    "    dict_k_den = {} #denumerator\n",
    "    \n",
    "    #key 0   #add authors not considered  \n",
    "    author_ids_ = set().union(*author_ids_tot_list[i:i+5]) #all authors windows restricted to eligible ones\n",
    "    author_ids_ = author_ids_ - prior_author_ids_list[i]\n",
    "    author_ids_ = author_ids_  - all_coauthors_list[i] #already considered\n",
    "    authors_k = author_ids_ | authors_isolated  \n",
    "    len_k = len(authors_k) #all authors zero contacts\n",
    "    new_auth_k = len(authors_k & first_time_authors_tot) #number of authors become infected first time during the observation window\n",
    "    dict_k_frac[0] = new_auth_k/len_k\n",
    "    dict_k_num[0] = new_auth_k\n",
    "    dict_k_den[0] = len_k\n",
    "\n",
    "    #key != 0      \n",
    "    dict_final_keys = list(dict_final.keys())\n",
    "    dict_final_keys.sort()\n",
    "    for k in dict_final_keys: #for each class k\n",
    "        authors_k = set(dict_final[k])\n",
    "        len_k = len(authors_k) #number of authors\n",
    "        new_auth_k = len(authors_k & first_time_authors) #number of authors become infected first time during the observation window\n",
    "        dict_k_frac[k] = new_auth_k/len_k\n",
    "        dict_k_num[k] = new_auth_k\n",
    "        dict_k_den[k] = len_k\n",
    "        \n",
    "    #order dictionary by key\n",
    "    dict_k_frac_ord = collections.OrderedDict(sorted(dict_k_frac.items()))\n",
    "    dict_k_frac_num = collections.OrderedDict(sorted(dict_k_num.items()))\n",
    "    dict_k_frac_den = collections.OrderedDict(sorted(dict_k_den.items()))\n",
    "    \n",
    "    dict_final_list.append(dict_k_frac_ord)\n",
    "    dict_final_num_list.append(dict_k_frac_num)\n",
    "    dict_final_den_list.append(dict_k_frac_den)\n",
    "          \n",
    "    return dict_final_list,dict_final_num_list,dict_final_den_list\n",
    "\n",
    "def calculation_B(first_time_authors,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list):\n",
    "         \n",
    "    dict_k_frac = {}\n",
    "    dict_k_num = {} #numerator\n",
    "    dict_k_den = {} #denumerator            \n",
    "    \n",
    "    dict_final_keys = list(dict_final.keys())\n",
    "    dict_final_keys.sort()\n",
    "    for k in dict_final_keys: #for each class k\n",
    "        authors_k = set(dict_final[k])\n",
    "        len_k = len(authors_k) #number of authors\n",
    "        new_auth_k = len(authors_k & first_time_authors) #number of authors become active first time during the period\n",
    "        dict_k_frac[k] = new_auth_k/len_k\n",
    "        dict_k_num[k] = new_auth_k\n",
    "        dict_k_den[k] = len_k \n",
    "        \n",
    "    #order dictionary by key\n",
    "    dict_k_frac_ord = collections.OrderedDict(sorted(dict_k_frac.items()))\n",
    "    dict_k_frac_num = collections.OrderedDict(sorted(dict_k_num.items()))\n",
    "    dict_k_frac_den = collections.OrderedDict(sorted(dict_k_den.items()))\n",
    "        \n",
    "    dict_final_list.append(dict_k_frac_ord)\n",
    "    dict_final_num_list.append(dict_k_frac_num)\n",
    "    dict_final_den_list.append(dict_k_frac_den)\n",
    "    \n",
    "        \n",
    "    return dict_final_list,dict_final_num_list,dict_final_den_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238a5a5-14fc-45ec-a6d9-68d6ecf89d34",
   "metadata": {},
   "source": [
    "### Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91f44425-3a0b-4542-b4b3-a4cc2dbfdd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:47:34.275051Z",
     "iopub.status.busy": "2023-04-17T19:47:34.274724Z",
     "iopub.status.idle": "2023-04-17T19:47:34.334743Z",
     "shell.execute_reply": "2023-04-17T19:47:34.333985Z",
     "shell.execute_reply.started": "2023-04-17T19:47:34.275022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folders \n",
    "for metric in ['Impact', 'Productivity']:\n",
    "    my_path = resultspath / metric\n",
    "    if not my_path.exists():\n",
    "        my_path.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22305cc-d087-4e76-bdd7-db6102776328",
   "metadata": {},
   "source": [
    "## INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "685d29d1-760d-43d6-845d-ef19e8e0db21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:06:07.674291Z",
     "iopub.status.busy": "2023-04-17T20:06:07.674032Z",
     "iopub.status.idle": "2023-04-17T20:06:07.737647Z",
     "shell.execute_reply": "2023-04-17T20:06:07.736966Z",
     "shell.execute_reply.started": "2023-04-17T20:06:07.674265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info(topic,my_path):\n",
    "\n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic') \n",
    "\n",
    "    #each year: work and authors topic\n",
    "    start_year = 1990 \n",
    "    work_ids_list =  []\n",
    "    author_ids_list =  []\n",
    "    for w in tqdm(range(0,32), desc='Finding authors and works list'): \n",
    "        start_year_w = start_year+w\n",
    "\n",
    "        work_ids = set(\n",
    "            works_concepts_conc\n",
    "            .query('publication_year == @start_year_w')\n",
    "            .work_id\n",
    "        )\n",
    "        work_ids_list.append(work_ids)\n",
    "        # corrispondent authors\n",
    "        author_ids = set(\n",
    "            works_authors\n",
    "            .query('work_id.isin(@work_ids)', engine='python')\n",
    "            .author_id\n",
    "        )\n",
    "        author_ids_list.append(author_ids) \n",
    "        \n",
    "    #save\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(work_ids_list,fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(author_ids_list,fp)\n",
    "        \n",
    "    #each year: works and authors (with and without topic) \n",
    "    work_ids_tot_list =  []\n",
    "    author_ids_tot_list =  []\n",
    "    for w in tqdm(range(0,28), desc='saving author and work ids'):\n",
    "        start_year_w = start_year+w\n",
    "\n",
    "        work_ids = (\n",
    "            works\n",
    "            .query('publication_year == @start_year_w')\n",
    "            .index\n",
    "        )\n",
    "        work_ids_tot_list.append(work_ids)\n",
    "\n",
    "        author_ids = set(\n",
    "            works_authors\n",
    "            .query('work_id.isin(@work_ids)', engine='python')\n",
    "            .author_id\n",
    "        )\n",
    "        author_ids_tot_list.append(author_ids)  \n",
    "    #save\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(work_ids_tot_list,fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(author_ids_tot_list,fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    windows_cond = [] \n",
    "    for w in range(0,23):\n",
    "        start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "        # work and authors topic in EW\n",
    "        prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "        prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "        # work and authors topic in OW\n",
    "        work_ids = set().union(*work_ids_list[w+5:w+5+5]) \n",
    "        author_ids = set().union(*author_ids_list[w+5:w+5+5])\n",
    "\n",
    "        #active authors start observation window\n",
    "        active_authors_start = prior_author_ids_5yr\n",
    "        \n",
    "        info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'EW-papers topic': len(prior_work_ids_5yr),\n",
    "                'EW-authors topic - active authors': len(prior_author_ids_5yr),\n",
    "                'OW-papers topic': len(work_ids),\n",
    "                'OW-authors topic': len(author_ids),\n",
    "                  }\n",
    "        \n",
    "        #consider just windows with at least 3000 papers in EW and OW \n",
    "        windows_cond.append((len(prior_work_ids_5yr)>=3000) and (len(work_ids)>=3000))\n",
    "            \n",
    "        info_i = pd.DataFrame(data=[info_i_dict])\n",
    "        info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)\n",
    "        \n",
    "    my_file = 'info_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    #save\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(windows_cond,fp)\n",
    "\n",
    "    return info_df,windows_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf9e83a6-6393-4e92-9ad2-356d5f7e7e91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:48:21.543319Z",
     "iopub.status.busy": "2023-04-17T19:48:21.543075Z",
     "iopub.status.idle": "2023-04-17T19:48:21.598035Z",
     "shell.execute_reply": "2023-04-17T19:48:21.597299Z",
     "shell.execute_reply.started": "2023-04-17T19:48:21.543297Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../results/Physics')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultspath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5410d-4109-4d8e-b49e-890cde5c1d25",
   "metadata": {},
   "source": [
    "## Save work and author IDs for each window for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede038f2-3760-4d7f-b43e-366cf2ba00b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:55:32.902344Z",
     "iopub.status.busy": "2023-04-17T19:55:32.901998Z",
     "iopub.status.idle": "2023-04-17T19:57:35.024562Z",
     "shell.execute_reply": "2023-04-17T19:57:35.023709Z",
     "shell.execute_reply.started": "2023-04-17T19:55:32.902316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609447304a24cf39ef91d0d02fb9610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a015e7e11eda4c4c921ada470fb92e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding authors and works list:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6fde2bfe804f36b361d37edf9b290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "saving author and work ids:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "windows_cond = {}\n",
    "my_path = resultspath / 'Info'\n",
    "if not my_path.exists(): #create folder\n",
    "    my_path.mkdir()\n",
    "    \n",
    "for topic in tqdm(topic_list):\n",
    "    info_df_top,windows_cond_top = info(topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    windows_cond[topic] = windows_cond_top\n",
    "\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(my_path / my_file, sep=';', index=False)  \n",
    "\n",
    "my_file = 'windows_cond'\n",
    "with open(my_path / my_file, \"wb\") as fp:\n",
    "    pickle.dump(windows_cond,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bc92c-6de7-4774-882a-7fe2906f4243",
   "metadata": {},
   "source": [
    "### Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "528ac839-1f00-406d-b4f6-9831441a5e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:58:58.752730Z",
     "iopub.status.busy": "2023-04-17T19:58:58.752238Z",
     "iopub.status.idle": "2023-04-17T19:58:58.823875Z",
     "shell.execute_reply": "2023-04-17T19:58:58.823106Z",
     "shell.execute_reply.started": "2023-04-17T19:58:58.752695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_productivity(discipline,topic,my_path):\n",
    "       \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in tqdm(range(0,23), desc='Getting window statistics..'):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_productivity(works_authors,prior_work_ids_5yr,active_authors_start)\n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] \n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)  \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563ac62-f642-4e71-9798-eb81b8328e0a",
   "metadata": {},
   "source": [
    "## Saving summary info for Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b86b725-2634-4db5-804e-16d6ee523b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T19:59:44.921611Z",
     "iopub.status.busy": "2023-04-17T19:59:44.921259Z",
     "iopub.status.idle": "2023-04-17T20:00:10.158555Z",
     "shell.execute_reply": "2023-04-17T20:00:10.157761Z",
     "shell.execute_reply.started": "2023-04-17T19:59:44.921579Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c991014bf14bd9b83770ba12d5f8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting window statistics..:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k=10 taking samples_per_class=456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979152311</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1920053454</td>\n",
       "      <td>31</td>\n",
       "      <td>0.999562</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  1979152311   32  1.000000  top 10%\n",
       "1  1920053454   31  0.999562  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 456 from 2,958 candidates\n",
      "Insufficient items in top 10%. Need 456 have 350\n",
      "Missing 106 samples for top 10%. Expanding the range to 70-90%, Acquired 106 new samples.\n",
      "top_k=10 taking samples_per_class=476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2110062665</td>\n",
       "      <td>32</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2107282753</td>\n",
       "      <td>31</td>\n",
       "      <td>0.99979</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2110062665   32   1.00000  top 10%\n",
       "1  2107282753   31   0.99979  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 476 from 3,119 candidates\n",
      "top 10%: Sampling 476 from 476 candidates\n",
      "top_k=10 taking samples_per_class=493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2107282753</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3175291849</td>\n",
       "      <td>29</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2107282753   32  1.000000  top 10%\n",
       "1  3175291849   29  0.999595  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 493 from 3,220 candidates\n",
      "Insufficient items in top 10%. Need 493 have 472\n",
      "Missing 21 samples for top 10%. Expanding the range to 70-90%, Acquired 21 new samples.\n",
      "top_k=10 taking samples_per_class=524\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2107282753</td>\n",
       "      <td>34</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3175291849</td>\n",
       "      <td>34</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2107282753   34  0.999809  top 10%\n",
       "1  3175291849   34  0.999809  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 524 from 3,463 candidates\n",
      "Insufficient items in top 10%. Need 524 have 471\n",
      "Missing 53 samples for top 10%. Expanding the range to 70-90%, Acquired 53 new samples.\n",
      "top_k=10 taking samples_per_class=535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2070155610</td>\n",
       "      <td>31</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3175291849</td>\n",
       "      <td>29</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2070155610   31  1.000000  top 10%\n",
       "1  3175291849   29  0.999627  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 535 from 3,493 candidates\n",
      "Insufficient items in top 10%. Need 535 have 485\n",
      "Missing 50 samples for top 10%. Expanding the range to 70-90%, Acquired 50 new samples.\n",
      "top_k=10 taking samples_per_class=559\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2340803116</td>\n",
       "      <td>37</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2110062665</td>\n",
       "      <td>35</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2340803116   37  1.000000  top 10%\n",
       "1  2110062665   35  0.999821  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 559 from 3,651 candidates\n",
      "Insufficient items in top 10%. Need 559 have 524\n",
      "Missing 35 samples for top 10%. Expanding the range to 70-90%, Acquired 35 new samples.\n",
      "top_k=10 taking samples_per_class=589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824096837</td>\n",
       "      <td>38</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2340803116</td>\n",
       "      <td>36</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  1824096837   38   1.00000  top 10%\n",
       "1  2340803116   36   0.99983  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 589 from 3,880 candidates\n",
      "Insufficient items in top 10%. Need 589 have 527\n",
      "Missing 62 samples for top 10%. Expanding the range to 70-90%, Acquired 62 new samples.\n",
      "top_k=10 taking samples_per_class=644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1824096837</td>\n",
       "      <td>43</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2340803116</td>\n",
       "      <td>39</td>\n",
       "      <td>0.999845</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  1824096837   43  1.000000  top 10%\n",
       "1  2340803116   39  0.999845  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 644 from 4,299 candidates\n",
      "Insufficient items in top 10%. Need 644 have 564\n",
      "Missing 80 samples for top 10%. Expanding the range to 70-90%, Acquired 80 new samples.\n",
      "top_k=10 taking samples_per_class=685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2340803116</td>\n",
       "      <td>47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1824096837</td>\n",
       "      <td>38</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2340803116   47  1.000000  top 10%\n",
       "1  1824096837   38  0.999854  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 685 from 4,531 candidates\n",
      "Insufficient items in top 10%. Need 685 have 584\n",
      "Missing 101 samples for top 10%. Expanding the range to 70-90%, Acquired 101 new samples.\n",
      "top_k=10 taking samples_per_class=779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2340803116</td>\n",
       "      <td>41</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2426567721</td>\n",
       "      <td>38</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  val  rank_pct rank_cat\n",
       "0  2340803116   41  1.000000  top 10%\n",
       "1  2426567721   38  0.999872  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 779 from 5,206 candidates\n",
      "Insufficient items in top 10%. Need 779 have 652\n",
      "Missing 127 samples for top 10%. Expanding the range to 70-90%, Acquired 127 new samples.\n"
     ]
    }
   ],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(resultspath, 'Info/Productivity')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "    \n",
    "for topic in topic_list:\n",
    "    info_df_top,active_authors_classes_top = info_productivity(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291336b-ef23-4e1b-b30b-704ae4c58f02",
   "metadata": {},
   "source": [
    "### Impact (mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c040f-d09b-4bb4-ba22-f19d7baf1cdd",
   "metadata": {},
   "source": [
    "#### Def. impact 1 - papers:all, cits:topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "135f1244-f69b-45bd-b3c5-04506646dd07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:05:16.749932Z",
     "iopub.status.busy": "2023-04-17T20:05:16.749609Z",
     "iopub.status.idle": "2023-04-17T20:05:16.840324Z",
     "shell.execute_reply": "2023-04-17T20:05:16.838298Z",
     "shell.execute_reply.started": "2023-04-17T20:05:16.749898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " def info_impact1(discipline,topic,my_path):\n",
    "    \n",
    "    works_concepts_conc_tot = works_concepts.query('concept_name==@topic') \n",
    "    \n",
    "    work_ids_concept = set(works_concepts_conc_tot.work_id)\n",
    "    works_referenced_works_concept = works_referenced_works.query('work_id.isin(@work_ids_concept)', engine='python')\n",
    "    works_cit_counts_year_concept = works_referenced_works_concept.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "    works_cit_counts_year_concept.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "    index = pd.MultiIndex.from_product(works_cit_counts_year_concept.index.levels)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reindex(index)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reset_index(level=0).reset_index(level=0)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.fillna(0)\n",
    "    works_cit_counts_year_concept['cit_count_cum'] = works_cit_counts_year_concept.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.rename(columns = {'referenced_work_id':'work_id'})\n",
    "       \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in tqdm(range(0,23), desc='Saving work and author IDs...'):\n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_impact_mean_1(works_authors,start_year_w,active_authors_start,works_cit_counts_year_concept) \n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            # mid_active_authors1 = samples_dict_1['middle 10%']\n",
    "            # mid_active_authors1_val = sorted_author_works_count.query('author_id.isin(@mid_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] # [[samples_dict_1,n_1],[samples_dict_2,n_2]]\n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)   \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e981741c-35cb-47f2-9244-da40eca4271c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:05:16.842035Z",
     "iopub.status.busy": "2023-04-17T20:05:16.841212Z",
     "iopub.status.idle": "2023-04-17T20:06:07.672177Z",
     "shell.execute_reply": "2023-04-17T20:06:07.671469Z",
     "shell.execute_reply.started": "2023-04-17T20:05:16.842012Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84296ef8b8b420f9d658fde35f59d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f798b996ded6477e8971be454cc8ace0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving work and author IDs...:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_k=10 taking samples_per_class=456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2118573323</td>\n",
       "      <td>23.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2157543303</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  2118573323  23.8  1.000000  top 10%\n",
       "1  2157543303  20.0  0.999781  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 456 from 1,646 candidates\n",
      "Insufficient items in top 10%. Need 456 have 450\n",
      "Missing 6 samples for top 10%. Expanding the range to 70-90%, Acquired 6 new samples.\n",
      "top_k=10 taking samples_per_class=476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2578571484</td>\n",
       "      <td>25.5</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3126624835</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.99979</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  2578571484  25.5   1.00000  top 10%\n",
       "1  3126624835  24.0   0.99979  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 476 from 1,652 candidates\n",
      "Insufficient items in top 10%. Need 476 have 471\n",
      "Missing 5 samples for top 10%. Expanding the range to 70-90%, Acquired 5 new samples.\n",
      "top_k=10 taking samples_per_class=493\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2110816957</td>\n",
       "      <td>39.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2170622773</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id        val  rank_pct rank_cat\n",
       "0  2110816957  39.142857  1.000000  top 10%\n",
       "1  2170622773  39.000000  0.999797  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 493 from 1,600 candidates\n",
      "Insufficient items in top 10%. Need 493 have 477\n",
      "Missing 16 samples for top 10%. Expanding the range to 70-90%, Acquired 16 new samples.\n",
      "top_k=10 taking samples_per_class=524\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2170622773</td>\n",
       "      <td>56.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2110816957</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id        val  rank_pct rank_cat\n",
       "0  2170622773  56.750000  1.000000  top 10%\n",
       "1  2110816957  56.666667  0.999809  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 524 from 1,802 candidates\n",
      "Insufficient items in top 10%. Need 524 have 503\n",
      "Missing 21 samples for top 10%. Expanding the range to 70-90%, Acquired 21 new samples.\n",
      "top_k=10 taking samples_per_class=535\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2969242109</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2170622773</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  2969242109  48.0  0.999813  top 10%\n",
       "1  2170622773  48.0  0.999813  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 535 from 1,736 candidates\n",
      "Insufficient items in top 10%. Need 535 have 532\n",
      "Missing 3 samples for top 10%. Expanding the range to 70-90%, Acquired 3 new samples.\n",
      "top_k=10 taking samples_per_class=559\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3159832477</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2463495070</td>\n",
       "      <td>50.666667</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id        val  rank_pct rank_cat\n",
       "0  3159832477  56.000000  1.000000  top 10%\n",
       "1  2463495070  50.666667  0.999821  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 559 from 1,781 candidates\n",
      "Insufficient items in top 10%. Need 559 have 548\n",
      "Missing 11 samples for top 10%. Expanding the range to 70-90%, Acquired 11 new samples.\n",
      "top_k=10 taking samples_per_class=589\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2303322644</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3159832477</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99983</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  2303322644  66.0   1.00000  top 10%\n",
       "1  3159832477  60.0   0.99983  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 589 from 1,950 candidates\n",
      "Insufficient items in top 10%. Need 589 have 578\n",
      "Missing 11 samples for top 10%. Expanding the range to 70-90%, Acquired 11 new samples.\n",
      "top_k=10 taking samples_per_class=644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2303322644</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2147395010</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.99969</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  2303322644  84.0   1.00000  top 10%\n",
       "1  2147395010  75.0   0.99969  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 644 from 2,218 candidates\n",
      "top 10%: Sampling 644 from 644 candidates\n",
      "top_k=10 taking samples_per_class=685\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998799128</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2206015203</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id   val  rank_pct rank_cat\n",
       "0  1998799128  95.0  0.999854  top 10%\n",
       "1  2206015203  95.0  0.999854  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 685 from 2,352 candidates\n",
      "Insufficient items in top 10%. Need 685 have 657\n",
      "Missing 28 samples for top 10%. Expanding the range to 70-90%, Acquired 28 new samples.\n",
      "top_k=10 taking samples_per_class=779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>val</th>\n",
       "      <th>rank_pct</th>\n",
       "      <th>rank_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998799128</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4221689052</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>top 10%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id    val  rank_pct rank_cat\n",
       "0  1998799128  123.0  1.000000  top 10%\n",
       "1  4221689052   93.0  0.999872  top 10%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10%: Sampling 779 from 2,623 candidates\n",
      "Insufficient items in top 10%. Need 779 have 738\n",
      "Missing 41 samples for top 10%. Expanding the range to 70-90%, Acquired 41 new samples.\n"
     ]
    }
   ],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(resultspath, 'Info/Impact')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "    \n",
    "for topic in tqdm(topic_list):\n",
    "    info_df_top,active_authors_classes_top = info_impact1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57ae07-2b94-438a-8e6c-aef40e0254f0",
   "metadata": {},
   "source": [
    "## EXP1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b6cb3-12a7-4596-af67-fca9de29a049",
   "metadata": {},
   "source": [
    "### Productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7e13f-fd84-4e31-80bc-0d0d9613d920",
   "metadata": {},
   "source": [
    "#### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a3bb6c5-d928-4256-9c6e-96793a168303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:44:00.233680Z",
     "iopub.status.busy": "2023-04-17T21:44:00.233188Z",
     "iopub.status.idle": "2023-04-17T21:44:00.317979Z",
     "shell.execute_reply": "2023-04-17T21:44:00.317247Z",
     "shell.execute_reply.started": "2023-04-17T21:44:00.233653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic')   \n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Productivity')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    all_coauthors_list = [] #coauthors collaboration graph\n",
    "    active_authors_start_union = set() #union active authors all windows\n",
    "    for w in tqdm(range(23), desc='Exp 1. Coauthors..'):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW (5 years before)\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "   \n",
    "            # all coauthors\n",
    "            all_coauthors_w = set(\n",
    "                works_authors\n",
    "                [\n",
    "                    works_authors.work_id.isin(\n",
    "                        works_authors\n",
    "                        .query('(author_id.isin(@prior_author_ids_5yr)) & (@start_year_w - 5 <= publication_year < @start_year_w)', engine='python')\n",
    "                        .work_id)\n",
    "                ]\n",
    "                .author_id\n",
    "            )\n",
    "            #save\n",
    "            all_coauthors_list.append(all_coauthors_w)\n",
    "\n",
    "            #union active authors\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            active_authors_start_union = active_authors_start_union.union(active_authors_start)\n",
    "        else:\n",
    "            all_coauthors_list.append(np.nan) \n",
    "            \n",
    "    active_authors_start_union_list = list(active_authors_start_union)     \n",
    "    \n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(all_coauthors_list,fp)\n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_start_union,fp)\n",
    "            \n",
    "    prior_work_ids_list =  [] #paper written with topic before start year\n",
    "    prior_author_ids_list =  []\n",
    "    first_time_authors_list = [] #authors write first paper during observation window\n",
    "    first_time_authors_tot_list = [] \n",
    "    not_active_authors_start_list = [] #authors not already active at the beginning \n",
    "    first_time_authors_union = set() #first time authors all windows \n",
    "    for w in tqdm(range(23), desc='Exp 1. Dump pickles..'):\n",
    "            start_year_w = start_year+w\n",
    "            #authors written at least one paper with concept before start_date --> already active nodes at the beginning\n",
    "            if w==0:\n",
    "                prior_work_ids_df = works_concepts_conc.query('publication_year < @start_year_w')\n",
    "                prior_work_ids = set(\n",
    "                prior_work_ids_df\n",
    "                .work_id\n",
    "                )\n",
    "                prior_work_ids_list.append(prior_work_ids)\n",
    "                prior_author_ids = set(\n",
    "                    works_authors\n",
    "                    .query('work_id.isin(@prior_work_ids)')\n",
    "                    .author_id\n",
    "                )\n",
    "                prior_author_ids_list.append(prior_author_ids)\n",
    "\n",
    "            else: \n",
    "                prior_work_ids = (prior_work_ids_list[w-1]).union(work_ids_list[w+5-1])\n",
    "                prior_work_ids_list.append(prior_work_ids)\n",
    "                prior_author_ids = (prior_author_ids_list[w-1]).union(author_ids_list[w+5-1])\n",
    "                prior_author_ids_list.append(prior_author_ids) \n",
    "            \n",
    "            windows_cond_w = windows_cond[w]   \n",
    "            if windows_cond_w:\n",
    "            \n",
    "                all_coauthors = all_coauthors_list[w] # all coauthors         \n",
    "                author_ids = set().union(*author_ids_list[w+5:w+5+5]) # work and authors topic in OW\n",
    "                first_time_authors = (all_coauthors & author_ids) - prior_author_ids #authors write first paper during observation window\n",
    "                first_time_authors_list.append(first_time_authors)               \n",
    "                first_time_authors_tot = author_ids - prior_author_ids #authors write first paper during observation window\n",
    "                first_time_authors_tot_list.append(first_time_authors)\n",
    "                not_active_authors_start = all_coauthors - prior_author_ids\n",
    "                not_active_authors_start_list.append(not_active_authors_start)                \n",
    "                first_time_authors_union = first_time_authors_union.union(first_time_authors)  \n",
    "            else:\n",
    "                first_time_authors_list.append(np.nan) \n",
    "                not_active_authors_start_list.append(np.nan) \n",
    "                first_time_authors_tot_list.append(np.nan)  # added by Satyaki (Apr 17)\n",
    "   \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(prior_work_ids_list,fp)\n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(prior_author_ids_list,fp)\n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_list,fp)\n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_tot_list,fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(not_active_authors_start_list,fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_union,fp)\n",
    "        \n",
    "    #for authors infected at the beginning: dictionary {author : date_infection/date first paper with concept} \n",
    "    date_activation_df = (\n",
    "                        works_authors\n",
    "                        [works_authors.work_id.isin(\n",
    "                         works_concepts_conc\n",
    "                        .work_id\n",
    "                        )]\n",
    "                        .query('author_id.isin(@active_authors_start_union)')\n",
    "                        .sort_values(by='publication_date')\n",
    "                        .drop_duplicates('author_id')     \n",
    "            )\n",
    "    dict_date_act_start = pd.Series(date_activation_df.publication_date.values,index=date_activation_df.author_id).to_dict()  \n",
    "    \n",
    "    #for each infected author keep just papers written after their infection date\n",
    "    works_authors_active = works_authors[works_authors.author_id.isin(active_authors_start_union)] #restrict to active authors\n",
    "    works_authors_aa_list = []\n",
    "    for aa in tqdm(active_authors_start_union):\n",
    "        works_authors_aa = works_authors_active[ (works_authors_active.author_id == aa) & (works_authors_active.publication_date >= dict_date_act_start[aa])] #select just works before activation year\n",
    "        works_authors_aa_list.append(works_authors_aa) \n",
    "    works_authors_activation_date = pd.concat(works_authors_aa_list)  \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(works_authors_activation_date,fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(23), desc='Running Exp 1'): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            \n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start, unit_scale=True, desc='Iterating over inactive authors'): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in tqdm(range(23), desc='Save stats'): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957f7bc-8bf3-4a60-ae28-c1146ca212c3",
   "metadata": {},
   "source": [
    "## Run Exp 1 version 1: (Main manuscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4c2d300-944b-46f7-b6b6-c41b71c51e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T20:23:42.344043Z",
     "iopub.status.busy": "2023-04-17T20:23:42.343782Z",
     "iopub.status.idle": "2023-04-17T21:00:03.760672Z",
     "shell.execute_reply": "2023-04-17T21:00:03.759894Z",
     "shell.execute_reply.started": "2023-04-17T20:23:42.344020Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308548fc501b4ce9a98e7afd86bb4a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6260c824e6cf46fbbbeac5381551e70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exp 1. Coauthors..:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621e7becb10740b382b418db62f80f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exp 1. Dump pickles..:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9889e80460964f928ae9b208aedb4ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe9747711404e52bd96fd1f52e2717e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Exp 1:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4980d749ff8b4be2a2239dea8fb6d90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1856cff48eba46b8862be8d2f1f9b427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795b1a346cc8450f9530c18a0ee62d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126882 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5a491ff6754a7e848e24187faf90d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4340a7c94787462fa1d727fec3552f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0e8325e6574f8483b37c971bf3b4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c607077af471a89bd34ead0e845ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8984afa71b346228eb1d9d76928f138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b0fb6074a49c6bdfc93a33b39de38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dda96df5a9f475098fc66150a3c60a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2320cb0e5f4d18a49ec30222007252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Save stats:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_path = os.path.join(resultspath, 'Productivity/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in tqdm(topic_list):\n",
    "    topic_df_top = Exp1_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b25c2-d93e-4a29-95fb-9a983cb1cab4",
   "metadata": {},
   "source": [
    "#### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34f9366f-a7a7-4c7f-a3db-d72162299c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:02:43.647474Z",
     "iopub.status.busy": "2023-04-17T21:02:43.647214Z",
     "iopub.status.idle": "2023-04-17T21:02:43.951210Z",
     "shell.execute_reply": "2023-04-17T21:02:43.950483Z",
     "shell.execute_reply.started": "2023-04-17T21:02:43.647448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic')  \n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Productivity')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(resultspath, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start, unit_scale=True, desc='Iterating over inactive authors'): #for each author not active at the beginning  \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in tqdm(range(23), desc='Dumping stats.'): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6be36ae7-390b-4e09-9889-44a213c8ae21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:04:22.014534Z",
     "iopub.status.busy": "2023-04-17T21:04:22.010983Z",
     "iopub.status.idle": "2023-04-17T21:43:59.381879Z",
     "shell.execute_reply": "2023-04-17T21:43:59.381106Z",
     "shell.execute_reply.started": "2023-04-17T21:04:22.014498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3475adb8635d4bba8320eae2217e1288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dfa73eede4404d90162012d131160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/103k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be87db3999f41d3ad7bb49be87de80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/120k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b907e94377be4e8cbfba74840c2f3815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/127k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ae35ecd7e94e86989946f5d14d374b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/137k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d9d068caf94e6da4a546239d0e8e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/149k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0618054ccf324cedb54782268ff1ef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/174k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d724276232b04fd1b225479b01cb1956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/189k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdb69b4e6334b90883c43f44eef33f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/206k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff90d2e1e214ec9a4ae897b9e388cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/224k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f0df46577422da93048f04f1d6070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterating over inactive authors:   0%|          | 0.00/253k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb35372fe63f4e20a9e650f482f970d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dumping stats.:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_path = os.path.join(resultspath, 'Productivity/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d284b-31ac-4eb2-a675-0b365c3b563f",
   "metadata": {},
   "source": [
    "### Impact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc91f41-45f5-4594-a4b2-e6171b80f8e2",
   "metadata": {},
   "source": [
    "#### Def. impact 1 - papers:all, cits:topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6889e-cf5c-45be-9735-467ccdd90257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:15:53.540418Z",
     "iopub.status.busy": "2023-03-07T13:15:53.539935Z",
     "iopub.status.idle": "2023-03-07T13:15:53.546204Z",
     "shell.execute_reply": "2023-03-07T13:15:53.545261Z",
     "shell.execute_reply.started": "2023-03-07T13:15:53.540286Z"
    }
   },
   "source": [
    "##### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d5fd308-6525-4c2e-bd77-b1c95862292f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:43:59.383806Z",
     "iopub.status.busy": "2023-04-17T21:43:59.383553Z",
     "iopub.status.idle": "2023-04-17T21:43:59.454886Z",
     "shell.execute_reply": "2023-04-17T21:43:59.454203Z",
     "shell.execute_reply.started": "2023-04-17T21:43:59.383782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_1_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(resultspath, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start, unit_scale=True, desc='Inactive authors'): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "            \n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)  \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in tqdm(range(23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92f0e7cf-6fac-4d96-9280-97455a69058b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:46:02.460958Z",
     "iopub.status.busy": "2023-04-17T21:46:02.460505Z",
     "iopub.status.idle": "2023-04-17T22:20:11.986085Z",
     "shell.execute_reply": "2023-04-17T22:20:11.985239Z",
     "shell.execute_reply.started": "2023-04-17T21:46:02.460926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a14e57a5fc422dadfc1e6169f08db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c50236819334eb499cf21c34346a3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/103k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5d0cc4e08c475a9d4cd9b07f8c51ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/120k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d19dd1c749e4fc8a4e37546c8b2eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/127k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd7a89e71844555b3c9c7fe9685d965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/137k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b17b08eb5fb48d9985f6e9fb1a8dcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/149k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3635f140d64648909b1e272153f98a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/174k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c61e2a0e7ce4c17af916aa31e7cff69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/189k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb66333eb0e4c5da0794ec6aa3e07de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/206k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2eb1b6acb74fd69a95c78ec2294caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/224k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cd3b0806ac4debad510a4016d5f1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inactive authors:   0%|          | 0.00/253k [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4ea9d2fec940028a5edfe1be92136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_path = os.path.join(resultspath, 'Impact/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list[:1]:\n",
    "    topic_df_top = Exp1_1_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d450b17-5868-4186-8245-e83bd7f7f985",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f74b8271-acc4-4aca-96a4-748db902ce0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T22:20:59.840858Z",
     "iopub.status.busy": "2023-04-17T22:20:59.840546Z",
     "iopub.status.idle": "2023-04-17T22:20:59.924058Z",
     "shell.execute_reply": "2023-04-17T22:20:59.923311Z",
     "shell.execute_reply.started": "2023-04-17T22:20:59.840828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_1_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(resultspath, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning  \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc392d01-1c17-4ed7-973c-29dcf74b5353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T22:21:01.967254Z",
     "iopub.status.busy": "2023-04-17T22:21:01.967014Z",
     "iopub.status.idle": "2023-04-17T23:00:34.688756Z",
     "shell.execute_reply": "2023-04-17T23:00:34.687839Z",
     "shell.execute_reply.started": "2023-04-17T22:21:01.967232Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94f71300dc5450291258f27ca31e6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3603ace023a4dc683ee956f17d173e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a68e2d127497482ed7828f9f7de9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e942d4e029d64e7e8382abd2b4e51f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126882 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa762656d55d4c6d8c8ea8710c8d9781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136728 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f678fc10e0e467ea44f3390522dd2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/149479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6eb417bb5084816a2ac69879a5442f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52857ba1050140649e56ea56e14b386b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba82e18371894debad75067cf930758a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8719f3ca249649c5acdb7ba2dfaeb6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf2477fdba246de96076bac12efe350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_path = os.path.join(resultspath, 'Impact/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_1_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d548d35-4398-43b8-93bc-0bbf11ac5f10",
   "metadata": {},
   "source": [
    "## CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3af4d296-2662-4442-8f3b-4fbf050ec21d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:34.938712Z",
     "iopub.status.busy": "2023-04-17T23:00:34.938439Z",
     "iopub.status.idle": "2023-04-17T23:00:35.101552Z",
     "shell.execute_reply": "2023-04-17T23:00:35.101004Z",
     "shell.execute_reply.started": "2023-04-17T23:00:34.938685Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#select window according to condition minimum number of papers with concept in EW and OW \n",
    "from itertools import compress\n",
    "start_year = 1995\n",
    "end_year = 2017\n",
    "years_list = list(range(start_year,end_year+1)) #list T_0\n",
    "\n",
    "def windows_selection(topic,my_path,years_list,N):\n",
    "    \n",
    "    #load works with concepts each year\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    windows_cond = [] \n",
    "    for w in range(23):\n",
    "        start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "        # work and authors topic in EW\n",
    "        prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "\n",
    "        # work and authors topic in OW\n",
    "        work_ids = set().union(*work_ids_list[w+5:w+5+5]) \n",
    "\n",
    "        #consider just windows with at least N papers in EW and OW \n",
    "        windows_cond.append((len(prior_work_ids_5yr)>=N) and (len(work_ids)>=N))\n",
    "\n",
    "\n",
    "    #save\n",
    "    windows_list = list(compress(years_list, windows_cond)) \n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(windows_list,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4fd8497-a5e0-4303-8b75-efdb5365a50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.102679Z",
     "iopub.status.busy": "2023-04-17T23:00:35.102377Z",
     "iopub.status.idle": "2023-04-17T23:00:35.173936Z",
     "shell.execute_reply": "2023-04-17T23:00:35.173106Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.102657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 3000 #thereshold\n",
    "my_path = os.path.join(resultspath, 'Info')\n",
    "#windows_selection\n",
    "for topic in topic_list:        \n",
    "    windows_selection(topic=topic,my_path=my_path,years_list=years_list,N=N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1574b7e9-b598-48b3-852a-eba7fe034671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.175681Z",
     "iopub.status.busy": "2023-04-17T23:00:35.175426Z",
     "iopub.status.idle": "2023-04-17T23:00:35.222414Z",
     "shell.execute_reply": "2023-04-17T23:00:35.221858Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.175661Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path_list = [\n",
    "        os.path.join(resultspath, 'Productivity/Exp1_ver1'),\n",
    "        os.path.join(resultspath, 'Productivity/Exp1_ver2'),\n",
    "        os.path.join(resultspath, 'Impact/Exp1_ver1'),\n",
    "        os.path.join(resultspath, 'Impact/Exp1_ver2'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd9dea48-15f7-4fd2-bc64-a3a0fa134b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.223493Z",
     "iopub.status.busy": "2023-04-17T23:00:35.223233Z",
     "iopub.status.idle": "2023-04-17T23:00:35.465523Z",
     "shell.execute_reply": "2023-04-17T23:00:35.464933Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.223471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union results\n",
    "for my_path in my_path_list:\n",
    "    topics_df = pd.DataFrame();\n",
    "    for topic in topic_list: \n",
    "        my_file = 'df_'+topic+'_windows.csv'\n",
    "        topic_df_top = pd.read_csv(os.path.join(my_path, my_file),index_col=0) \n",
    "        topic_df_top.insert(0, 'topic', topic)\n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)  \n",
    "    my_file = 'df_topic_windows.csv'    \n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b41dcee8-764d-49e1-8143-0504059397d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.466652Z",
     "iopub.status.busy": "2023-04-17T23:00:35.466395Z",
     "iopub.status.idle": "2023-04-17T23:00:35.521829Z",
     "shell.execute_reply": "2023-04-17T23:00:35.521240Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.466630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_cumulative(dict_final_num_list,dict_final_den_list,i,dict_final_num_list_cum,dict_final_list_cum,dict_final_den_list_cum):\n",
    "    \n",
    "    dataframe_num_i = pd.DataFrame(dict_final_num_list[i].items(), columns=['k', 'num'])\n",
    "    dataframe_den_i = pd.DataFrame(dict_final_den_list[i].items(), columns=['k', 'den'])\n",
    "    dataframe_i = dataframe_num_i.merge(dataframe_den_i)\n",
    "    dataframe_i_rev = dataframe_i.loc[::-1] \n",
    "    dataframe_i['num_cum'] = dataframe_i_rev['num'].cumsum().loc[::-1]\n",
    "    dataframe_i['den_cum'] = dataframe_i_rev['den'].cumsum().loc[::-1]\n",
    "    dataframe_i['prob_cum'] = (dataframe_i.num_cum / dataframe_i.den_cum).loc[::-1]\n",
    "    dict_final_num_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.num_cum)))    \n",
    "    dict_final_den_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.den_cum))) \n",
    "    dict_final_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.prob_cum)))\n",
    "\n",
    "    return dict_final_list_cum,dict_final_num_list_cum,dict_final_den_list_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efeb202d-e466-4e9a-ae14-94ee50f4e186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.522993Z",
     "iopub.status.busy": "2023-04-17T23:00:35.522731Z",
     "iopub.status.idle": "2023-04-17T23:00:35.570995Z",
     "shell.execute_reply": "2023-04-17T23:00:35.570422Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.522972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_calculation(dict_final_list_cum,dict_final_den_list_cum,dict_final_num_list_cum,num_windows_concept,dict_final_list_mean,dict_final_list_std,dict_final_den_list_mean,dict_final_den_list_std,dict_final_num_list_mean,dict_final_num_list_std,j):\n",
    "\n",
    "        values_j =  [d[j] for d in dict_final_list_cum]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_cum]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_cum]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        \n",
    "        return dict_final_list_mean,dict_final_list_std,dict_final_den_list_mean,dict_final_den_list_std,dict_final_num_list_mean,dict_final_num_list_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b2297a9-4773-490e-9550-70e9fe43aa31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:35.572320Z",
     "iopub.status.busy": "2023-04-17T23:00:35.571870Z",
     "iopub.status.idle": "2023-04-17T23:00:35.635728Z",
     "shell.execute_reply": "2023-04-17T23:00:35.635084Z",
     "shell.execute_reply.started": "2023-04-17T23:00:35.572297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#average windows periods \n",
    "def Exp1_stat(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "    \n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    \n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(len(start_year_window_list)):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(0,len(start_year_window_list)):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                               \n",
    "    #average and std dictionaries\n",
    "    dict_final_list_mean = {}\n",
    "    dict_final_list_std = {}\n",
    "    dict_final_den_list_mean = {}\n",
    "    dict_final_den_list_std = {}\n",
    "    dict_final_num_list_mean = {}\n",
    "    dict_final_num_list_std = {}\n",
    "    dict_final_list_high1_mean = {}\n",
    "    dict_final_list_high1_std = {}\n",
    "    dict_final_den_list_high1_mean = {}\n",
    "    dict_final_den_list_high1_std = {}\n",
    "    dict_final_num_list_high1_mean = {}\n",
    "    dict_final_num_list_high1_std = {}\n",
    "    dict_final_list_low1_mean = {}\n",
    "    dict_final_list_low1_std = {}\n",
    "    dict_final_den_list_low1_mean = {}\n",
    "    dict_final_den_list_low1_std = {}\n",
    "    dict_final_num_list_low1_mean = {}\n",
    "    dict_final_num_list_low1_std = {}\n",
    "    for j in range(max_keys+1):\n",
    "        values_j =  [d[j] for d in dict_final_list]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_high1]\n",
    "        dict_final_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list_high1]\n",
    "        dict_final_den_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list_high1]\n",
    "        dict_final_num_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_low1]\n",
    "        dict_final_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)  \n",
    "        values_j =  [d[j] for d in dict_final_den_list_low1]\n",
    "        dict_final_den_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list_low1]\n",
    "        dict_final_num_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "    \n",
    "    #save on file \n",
    "    my_file = 'df_'+topic+'_stat.csv'\n",
    "    Prob_mean_df=pd.DataFrame(dict_final_list_mean.items(), columns=['k', 'prob_mean'])\n",
    "    Prob_std_df=pd.DataFrame(dict_final_list_std.items(), columns=['k', 'prob_std'])\n",
    "    Den_mean_df=pd.DataFrame(dict_final_den_list_mean.items(), columns=['k', 'den_mean'])\n",
    "    Den_std_df=pd.DataFrame(dict_final_den_list_std.items(), columns=['k', 'den_std'])\n",
    "    Num_mean_df=pd.DataFrame(dict_final_num_list_mean.items(), columns=['k', 'num_mean'])\n",
    "    Num_std_df=pd.DataFrame(dict_final_num_list_std.items(), columns=['k', 'num_std'])\n",
    "    \n",
    "    Prob_mean_high1_df=pd.DataFrame(dict_final_list_high1_mean.items(), columns=['k', 'prob_mean_high1'])\n",
    "    Prob_std_high1_df=pd.DataFrame(dict_final_list_high1_std.items(), columns=['k', 'prob_std_high1'])\n",
    "    Den_mean_high1_df=pd.DataFrame(dict_final_den_list_high1_mean.items(), columns=['k', 'den_mean_high1'])\n",
    "    Den_std_high1_df=pd.DataFrame(dict_final_den_list_high1_std.items(), columns=['k', 'den_std_high1'])\n",
    "    Num_mean_high1_df=pd.DataFrame(dict_final_num_list_high1_mean.items(), columns=['k', 'num_mean_high1'])\n",
    "    Num_std_high1_df=pd.DataFrame(dict_final_num_list_high1_std.items(), columns=['k', 'num_std_high1'])\n",
    "    Prob_mean_low1_df=pd.DataFrame(dict_final_list_low1_mean.items(), columns=['k', 'prob_mean_low1'])\n",
    "    Prob_std_low1_df=pd.DataFrame(dict_final_list_low1_std.items(), columns=['k', 'prob_std_low1'])\n",
    "    Den_mean_low1_df=pd.DataFrame(dict_final_den_list_low1_mean.items(), columns=['k', 'den_mean_low1'])\n",
    "    Den_std_low1_df=pd.DataFrame(dict_final_den_list_low1_std.items(), columns=['k', 'den_std_low1'])\n",
    "    Num_mean_low1_df=pd.DataFrame(dict_final_num_list_low1_mean.items(), columns=['k', 'num_mean_low1'])\n",
    "    Num_std_low1_df=pd.DataFrame(dict_final_num_list_low1_std.items(), columns=['k', 'num_std_low1'])\n",
    "       \n",
    "    topic_df = ((Prob_mean_df.merge(Prob_std_df)).merge(Den_mean_df.merge(Den_std_df)).merge(Num_mean_df.merge(Num_std_df)))   \n",
    "    topic_high1_df = ((Prob_mean_high1_df.merge(Prob_std_high1_df)).merge(Den_mean_high1_df.merge(Den_std_high1_df)).merge(Num_mean_high1_df.merge(Num_std_high1_df))) \n",
    "    topic_low1_df = ((Prob_mean_low1_df.merge(Prob_std_low1_df)).merge(Den_mean_low1_df.merge(Den_std_low1_df)).merge(Num_mean_low1_df.merge(Num_std_low1_df))) \n",
    "    \n",
    "    topic_df_  = (topic_df.merge(topic_high1_df)).merge(topic_low1_df)\n",
    "    \n",
    "    #save all concept dataframes in one file \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cc16e04-57b3-4a2c-9e65-a088991f694e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:01:03.133972Z",
     "iopub.status.busy": "2023-04-17T23:01:03.133598Z",
     "iopub.status.idle": "2023-04-17T23:01:03.214506Z",
     "shell.execute_reply": "2023-04-17T23:01:03.213751Z",
     "shell.execute_reply.started": "2023-04-17T23:01:03.133941Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cumulative each window save - for p-values caluculation\n",
    "def Exp1_stat_cum_windows(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "\n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(0,num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(num_windows_topic):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(0,num_windows_topic):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                \n",
    "        #order dictionary by key\n",
    "        dict_final_num_list[w] = collections.OrderedDict(sorted(dict_final_num_list[w].items()))\n",
    "        dict_final_den_list[w] = collections.OrderedDict(sorted(dict_final_den_list[w].items()))\n",
    "        dict_final_list[w] = collections.OrderedDict(sorted(dict_final_list[w].items()))\n",
    "        dict_final_num_list_high1[w] = collections.OrderedDict(sorted(dict_final_num_list_high1[w].items()))\n",
    "        dict_final_den_list_high1[w] = collections.OrderedDict(sorted(dict_final_den_list_high1[w].items()))\n",
    "        dict_final_list_high1[w] = collections.OrderedDict(sorted(dict_final_list_high1[w].items()))\n",
    "        dict_final_num_list_low1[w] = collections.OrderedDict(sorted(dict_final_num_list_low1[w].items()))\n",
    "        dict_final_den_list_low1[w] = collections.OrderedDict(sorted(dict_final_den_list_low1[w].items()))\n",
    "        dict_final_list_low1[w] = collections.OrderedDict(sorted(dict_final_list_low1[w].items()))\n",
    "                \n",
    "                \n",
    "    #cumulative distributions (at least one)\n",
    "    dict_final_num_list_cum = []\n",
    "    dict_final_den_list_cum = []\n",
    "    dict_final_list_cum = []\n",
    "    dict_final_num_list_high1_cum = []\n",
    "    dict_final_den_list_high1_cum = []\n",
    "    dict_final_list_high1_cum = []\n",
    "    dict_final_num_list_low1_cum = []\n",
    "    dict_final_den_list_low1_cum = []\n",
    "    dict_final_list_low1_cum = []\n",
    "    for w in range(num_windows_topic):\n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum))) \n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    start_year = 1995\n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(num_windows_topic): \n",
    "            start_year_window = start_year_window_list[w]    \n",
    "            dict_final_df=pd.DataFrame(dict_final_list_cum[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list_cum[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list_cum[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1_cum[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1_cum[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1_cum[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1_cum[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1_cum[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1_cum[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_window)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "            \n",
    "    my_file = 'df_'+topic+'_windows_cum.csv'          \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93d33c26-56e4-4945-9c56-c7ec499559db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:00:47.325500Z",
     "iopub.status.busy": "2023-04-17T23:00:47.325217Z",
     "iopub.status.idle": "2023-04-17T23:00:53.584443Z",
     "shell.execute_reply": "2023-04-17T23:00:53.583521Z",
     "shell.execute_reply.started": "2023-04-17T23:00:47.325473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for my_path in my_path_list:  \n",
    "    #Exp1_stat_cum\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)    \n",
    "    for topic in topic_list:  \n",
    "        Exp1_stat_cum_windows(df_topics=df_topics,topic=topic,my_path=my_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdbe5c96-cb32-483f-9629-b3fb81581cea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:01:08.719138Z",
     "iopub.status.busy": "2023-04-17T23:01:08.718804Z",
     "iopub.status.idle": "2023-04-17T23:01:08.803373Z",
     "shell.execute_reply": "2023-04-17T23:01:08.802644Z",
     "shell.execute_reply.started": "2023-04-17T23:01:08.719110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cumulative \n",
    "def Exp1_stat_cum(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "    \n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    my_path2 = os.path.join(resultspath, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(num_windows_topic):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(num_windows_topic):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                \n",
    "        #order dictionary by key\n",
    "        dict_final_num_list[w] = collections.OrderedDict(sorted(dict_final_num_list[w].items()))\n",
    "        dict_final_den_list[w] = collections.OrderedDict(sorted(dict_final_den_list[w].items()))\n",
    "        dict_final_list[w] = collections.OrderedDict(sorted(dict_final_list[w].items()))\n",
    "        dict_final_num_list_high1[w] = collections.OrderedDict(sorted(dict_final_num_list_high1[w].items()))\n",
    "        dict_final_den_list_high1[w] = collections.OrderedDict(sorted(dict_final_den_list_high1[w].items()))\n",
    "        dict_final_list_high1[w] = collections.OrderedDict(sorted(dict_final_list_high1[w].items()))\n",
    "        dict_final_num_list_low1[w] = collections.OrderedDict(sorted(dict_final_num_list_low1[w].items()))\n",
    "        dict_final_den_list_low1[w] = collections.OrderedDict(sorted(dict_final_den_list_low1[w].items()))\n",
    "        dict_final_list_low1[w] = collections.OrderedDict(sorted(dict_final_list_low1[w].items()))\n",
    "                \n",
    "                \n",
    "    #cumulative distributions (at least one)\n",
    "    dict_final_num_list_cum = []\n",
    "    dict_final_den_list_cum = []\n",
    "    dict_final_list_cum = []\n",
    "    dict_final_num_list_high1_cum = []\n",
    "    dict_final_den_list_high1_cum = []\n",
    "    dict_final_list_high1_cum = []\n",
    "    dict_final_num_list_low1_cum = []\n",
    "    dict_final_den_list_low1_cum = []\n",
    "    dict_final_list_low1_cum = []\n",
    "    for w in range(num_windows_topic):\n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum))) \n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "\n",
    "                                             \n",
    "    #average and std dictionaries\n",
    "    dict_final_list_mean = {}\n",
    "    dict_final_list_std = {}\n",
    "    dict_final_den_list_mean = {}\n",
    "    dict_final_den_list_std = {}\n",
    "    dict_final_num_list_mean = {}\n",
    "    dict_final_num_list_std = {}\n",
    "    dict_final_list_high1_mean = {}\n",
    "    dict_final_list_high1_std = {}\n",
    "    dict_final_den_list_high1_mean = {}\n",
    "    dict_final_den_list_high1_std = {}\n",
    "    dict_final_num_list_high1_mean = {}\n",
    "    dict_final_num_list_high1_std = {}\n",
    "    dict_final_list_low1_mean = {}\n",
    "    dict_final_list_low1_std = {}\n",
    "    dict_final_den_list_low1_mean = {}\n",
    "    dict_final_den_list_low1_std = {}\n",
    "    dict_final_num_list_low1_mean = {}\n",
    "    dict_final_num_list_low1_std = {}\n",
    "    for j in range(max_keys+1):\n",
    "        values_j =  [d[j] for d in dict_final_list_cum]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_cum]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_cum]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_high1_cum]\n",
    "        dict_final_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list_high1_cum]\n",
    "        dict_final_den_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_high1_cum]\n",
    "        dict_final_num_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_low1_cum]\n",
    "        dict_final_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_low1_cum]\n",
    "        dict_final_den_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_low1_cum]\n",
    "        dict_final_num_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #save on file \n",
    "    Prob_mean_df=pd.DataFrame(dict_final_list_mean.items(), columns=['k', 'prob_mean'])\n",
    "    Prob_std_df=pd.DataFrame(dict_final_list_std.items(), columns=['k', 'prob_std'])\n",
    "    Den_mean_df=pd.DataFrame(dict_final_den_list_mean.items(), columns=['k', 'den_mean'])\n",
    "    Den_std_df=pd.DataFrame(dict_final_den_list_std.items(), columns=['k', 'den_std'])\n",
    "    Num_mean_df=pd.DataFrame(dict_final_num_list_mean.items(), columns=['k', 'num_mean'])\n",
    "    Num_std_df=pd.DataFrame(dict_final_num_list_std.items(), columns=['k', 'num_std'])\n",
    "    \n",
    "    Prob_mean_high1_df=pd.DataFrame(dict_final_list_high1_mean.items(), columns=['k', 'prob_mean_high1'])\n",
    "    Prob_std_high1_df=pd.DataFrame(dict_final_list_high1_std.items(), columns=['k', 'prob_std_high1'])\n",
    "    Den_mean_high1_df=pd.DataFrame(dict_final_den_list_high1_mean.items(), columns=['k', 'den_mean_high1'])\n",
    "    Den_std_high1_df=pd.DataFrame(dict_final_den_list_high1_std.items(), columns=['k', 'den_std_high1'])\n",
    "    Num_mean_high1_df=pd.DataFrame(dict_final_num_list_high1_mean.items(), columns=['k', 'num_mean_high1'])\n",
    "    Num_std_high1_df=pd.DataFrame(dict_final_num_list_high1_std.items(), columns=['k', 'num_std_high1'])\n",
    "    Prob_mean_low1_df=pd.DataFrame(dict_final_list_low1_mean.items(), columns=['k', 'prob_mean_low1'])\n",
    "    Prob_std_low1_df=pd.DataFrame(dict_final_list_low1_std.items(), columns=['k', 'prob_std_low1'])\n",
    "    Den_mean_low1_df=pd.DataFrame(dict_final_den_list_low1_mean.items(), columns=['k', 'den_mean_low1'])\n",
    "    Den_std_low1_df=pd.DataFrame(dict_final_den_list_low1_std.items(), columns=['k', 'den_std_low1'])\n",
    "    Num_mean_low1_df=pd.DataFrame(dict_final_num_list_low1_mean.items(), columns=['k', 'num_mean_low1'])\n",
    "    Num_std_low1_df=pd.DataFrame(dict_final_num_list_low1_std.items(), columns=['k', 'num_std_low1'])\n",
    "       \n",
    "    topic_df = ((Prob_mean_df.merge(Prob_std_df)).merge(Den_mean_df.merge(Den_std_df)).merge(Num_mean_df.merge(Num_std_df)))   \n",
    "    topic_high1_df = ((Prob_mean_high1_df.merge(Prob_std_high1_df)).merge(Den_mean_high1_df.merge(Den_std_high1_df)).merge(Num_mean_high1_df.merge(Num_std_high1_df))) \n",
    "    topic_low1_df = ((Prob_mean_low1_df.merge(Prob_std_low1_df)).merge(Den_mean_low1_df.merge(Den_std_low1_df)).merge(Num_mean_low1_df.merge(Num_std_low1_df))) \n",
    "\n",
    "    topic_df_  = (topic_df.merge(topic_high1_df)).merge(topic_low1_df)\n",
    "    \n",
    "    #save all concept dataframes in one file \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "219627ed-8f40-41e5-aef8-8055d3added1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:06:51.661940Z",
     "iopub.status.busy": "2023-04-17T23:06:51.661510Z",
     "iopub.status.idle": "2023-04-17T23:06:51.728743Z",
     "shell.execute_reply": "2023-04-17T23:06:51.727961Z",
     "shell.execute_reply.started": "2023-04-17T23:06:51.661905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#simple contagion\n",
    "def Exp1_stat_sc(df_topics,df_topics_cum,topic):\n",
    "\n",
    "    df_topic = df_topics_cum.query('topic==@topic').reset_index()\n",
    "    a, b = df_topic.k[0:11],df_topic.prob_mean[0:11]\n",
    "    df_topic_baselines = df_topics.query('topic==@topic').reset_index()\n",
    "    p = df_topic_baselines.prob_mean[1]\n",
    "    prob_baselines = np.array([(1 - (1 - p)**k) for k in range(0, 11)])\n",
    "    den = np.array(df_topic_baselines.den_mean)[0:11]\n",
    "    num = prob_baselines * den \n",
    "    dataframe_num_baselines = pd.DataFrame(num, columns=['num'])\n",
    "    dataframe_den_baselines = pd.DataFrame(den, columns=['den'])\n",
    "    dataframe_baselines = dataframe_num_baselines.merge(dataframe_den_baselines, left_index=True,right_index=True)\n",
    "    dataframe_baselines_rev = dataframe_baselines.loc[::-1] \n",
    "    dataframe_baselines['num_cum'] = dataframe_baselines_rev['num'].cumsum().loc[::-1]\n",
    "    dataframe_baselines['den_cum'] = dataframe_baselines_rev['den'].cumsum().loc[::-1]\n",
    "    dataframe_baselines['prob_cum'] = (dataframe_baselines.num_cum / dataframe_baselines.den_cum).loc[::-1]\n",
    "    y_1 = list(dataframe_baselines['prob_cum'])     \n",
    "    baseline_df = pd.DataFrame(list(zip(range(0, 11), y_1)), columns =['k', 'val'])\n",
    "    \n",
    "    baseline_df.insert(0, 'topic', topic)\n",
    "    return baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eac39c93-67d7-4b27-b581-905aa32dda66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T23:06:53.855621Z",
     "iopub.status.busy": "2023-04-17T23:06:53.855320Z",
     "iopub.status.idle": "2023-04-17T23:08:09.876875Z",
     "shell.execute_reply": "2023-04-17T23:08:09.876211Z",
     "shell.execute_reply.started": "2023-04-17T23:06:53.855594Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf00396b6eb4e0a86321d9983f8e35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravitational wave\n",
      "Gravitational wave\n",
      "Gravitational wave\n",
      "Gravitational wave\n"
     ]
    }
   ],
   "source": [
    "for my_path in tqdm(my_path_list):\n",
    "    \n",
    "    #Exp1_stat\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    topics_df = pd.DataFrame();\n",
    "    \n",
    "    for topic in topic_list:        \n",
    "        topic_df_top = Exp1_stat(df_topics=df_topics,topic=topic,my_path=my_path) \n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "    my_file = 'df_topic_stat.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))\n",
    "    \n",
    "    #Exp1_stat_cum\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    topics_df = pd.DataFrame()\n",
    "    \n",
    "    for topic in topic_list:  \n",
    "        topic_df_top = Exp1_stat_cum(df_topics=df_topics,topic=topic,my_path=my_path) \n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "    my_file = 'df_topic_stat_cumulative.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))\n",
    "    \n",
    "    #Exp1_stat_sc\n",
    "    my_file = 'df_topic_stat.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    \n",
    "    my_file = 'df_topic_stat_cumulative.csv'\n",
    "    df_topics_cum = pd.read_csv(os.path.join(my_path, my_file),index_col=0) \n",
    "\n",
    "    sc_df = pd.DataFrame()\n",
    "    \n",
    "    for topic in topic_list: \n",
    "        print(topic)\n",
    "        sc_df_top = Exp1_stat_sc(df_topics=df_topics,df_topics_cum=df_topics_cum,topic=topic)   \n",
    "        sc_df = pd.concat([sc_df, sc_df_top], ignore_index = True, axis = 0)\n",
    "     \n",
    "    my_file = 'df_topic_stat_sc.csv'\n",
    "    sc_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa44fcc-5f43-49cc-968b-6e95e83bfaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
