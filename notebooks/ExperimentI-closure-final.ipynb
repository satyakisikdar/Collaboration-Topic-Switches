{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058c9df4-6c38-4d2e-ba71-fae65f9671d0",
   "metadata": {},
   "source": [
    "# EXPERIMENT I - CLOSURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46eb29f-edb5-403b-8d27-a1b3f393ae37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T22:05:07.072663Z",
     "iopub.status.busy": "2023-04-03T22:05:07.072150Z",
     "iopub.status.idle": "2023-04-03T22:05:07.588210Z",
     "shell.execute_reply": "2023-04-03T22:05:07.587229Z",
     "shell.execute_reply.started": "2023-04-03T22:05:07.072513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb736c-63ca-46b9-a186-74b26225f2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "import ujson as json\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import requests \n",
    "from scipy.stats import entropy\n",
    "\n",
    "tqdm.pandas()\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "sns.set(style=\"ticks\", context=\"talk\")\n",
    "# plt.style.use(\"dark_background\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "pio.templates.default = 'presentation'\n",
    "\n",
    "import rich\n",
    "from itertools import combinations\n",
    "import sys \n",
    "from statistics import mean, stdev\n",
    "import struct, io, string\n",
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from scipy.stats import chisquare,kstest\n",
    "from scipy import stats \n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec55852-dc2a-4a4c-942f-de68ddd80feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_parquet(name, **args):\n",
    "    path = basepath / f'{name}'\n",
    "    df = pd.read_parquet(path, engine='pyarrow')\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    if 'publication_year' in df.columns:\n",
    "        df.loc[:, 'publication_year'] = pd.to_numeric(df.publication_year)\n",
    "        df = df[df.publication_year != 0]  # discard works with missing years\n",
    "        \n",
    "    print(f'Read {len(df):,} rows from {path.stem!r}')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d46c2-e2b2-4b09-9174-161484699e82",
   "metadata": {},
   "source": [
    "## LOAD FIELDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721c318-55c2-4d75-8f15-b34ebd3914e8",
   "metadata": {},
   "source": [
    "### Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25ba38-2ace-432b-bdd1-4b8eee90dbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'Physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc23a0-c63d-4626-b8c9-697a5d7b56dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/Physics/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11c007-78d9-4293-af3a-f5ee3a5c3aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcc567-3d16-4cec-aa16-98c30302f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in def. impact 2 \n",
    "works_cit_counts_year = works_referenced_works.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "works_cit_counts_year.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "index = pd.MultiIndex.from_product(works_cit_counts_year.index.levels)\n",
    "works_cit_counts_year = works_cit_counts_year.reindex(index)\n",
    "works_cit_counts_year = works_cit_counts_year.reset_index(level=0).reset_index(level=0)\n",
    "works_cit_counts_year = works_cit_counts_year.fillna(0)\n",
    "works_cit_counts_year['cit_count_cum'] = works_cit_counts_year.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "works_cit_counts_year = works_cit_counts_year.rename(columns = {'referenced_work_id':'work_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb24e0-ccbe-4f97-ab86-eea93d3cd2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Gravitational wave',\n",
    "    'Dark matter',\n",
    "    'Fluid dynamics',\n",
    "    'Soliton',\n",
    "    'Supersymmetry',\n",
    "    'Statistical physics',          \n",
    "    'Superconductivity' \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e8872-ea88-4628-b203-29178e33a46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14e60a-3d08-4193-ae38-9e90dc2ba433",
   "metadata": {},
   "source": [
    "### Computer Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ed56c-354d-4a70-a923-064115067667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'CS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03009dac-215d-4092-b712-cdf4a5d9343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/CS/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0fdf2-3ea7-46d2-b208-092b118d703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc82ea-6486-452a-9518-65d45c579870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in def. impact 2 \n",
    "works_cit_counts_year = works_referenced_works.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "works_cit_counts_year.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "index = pd.MultiIndex.from_product(works_cit_counts_year.index.levels)\n",
    "works_cit_counts_year = works_cit_counts_year.reindex(index)\n",
    "works_cit_counts_year = works_cit_counts_year.reset_index(level=0).reset_index(level=0)\n",
    "works_cit_counts_year = works_cit_counts_year.fillna(0)\n",
    "works_cit_counts_year['cit_count_cum'] = works_cit_counts_year.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "works_cit_counts_year = works_cit_counts_year.rename(columns = {'referenced_work_id':'work_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffb944-7d01-4bc9-b09b-e49b5043b04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "    'Compiler',\n",
    "    'Mobile computing',\n",
    "    'Cryptography',\n",
    "    'Cluster analysis', \n",
    "    'Image processing',\n",
    "    'Parallel computing'         \n",
    "            ]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f0a30-86f0-4b79-993d-28e0d81cd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbebcda-1172-4d91-8caf-1359c6fcf2e1",
   "metadata": {},
   "source": [
    "### BioMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2ded9-1adf-4ca5-98e5-38032bfcdf30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discipline = 'BioMed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323dc9d-856a-4a9f-b322-54fb10e90699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basepath = Path('/N/project/openalex/slices/BioMed/feb-2023')\n",
    "\n",
    "works = read_parquet('works')\n",
    "works_authors = read_parquet('works_authorships')\n",
    "works_concepts = read_parquet('works_concepts')\n",
    "works_referenced_works = read_parquet('works_referenced_works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826fc69-1b96-404a-8100-c0d11b9ff4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "works['num_authors']=works['num_authors'].astype('int64')\n",
    "works['n_coauthors'] = works['num_authors'] - 1\n",
    "works_authors = pd.merge(works_authors, works['publication_date'], on=\"work_id\")\n",
    "works_authors.drop_duplicates(subset=['work_id','author_id'], inplace=True)\n",
    "works_concepts = pd.merge(works_concepts, works['publication_date'], on=\"work_id\")\n",
    "works_concepts = works_concepts.query('score > 0.3', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232b312-64c2-4e82-be00-63e89bec13d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#used in def. impact 2 \n",
    "works_cit_counts_year = works_referenced_works.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "works_cit_counts_year.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "index = pd.MultiIndex.from_product(works_cit_counts_year.index.levels)\n",
    "works_cit_counts_year = works_cit_counts_year.reindex(index)\n",
    "works_cit_counts_year = works_cit_counts_year.reset_index(level=0).reset_index(level=0)\n",
    "works_cit_counts_year = works_cit_counts_year.fillna(0)\n",
    "works_cit_counts_year['cit_count_cum'] = works_cit_counts_year.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "works_cit_counts_year = works_cit_counts_year.rename(columns = {'referenced_work_id':'work_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32d8fd-1801-475a-ae56-3dd6ed45e207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list =[\n",
    "            'Protein structure',\n",
    "            'Genome', \n",
    "            'Peptide sequence',\n",
    "            \"Alzheimer's disease\",\n",
    "            'Neurology',          \n",
    "            'Radiation therapy',\n",
    "            'Chemotherapy'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8d965-cdbb-4630-8081-c7b88171c980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "if not os.path.exists(discipline):\n",
    "    os.makedirs(discipline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc67e2f-1046-437f-847c-fa8d23f97c6c",
   "metadata": {},
   "source": [
    "## FUNCTIONS DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138406fe-bbd1-40fa-8986-1dfe1aa95dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d4e712-b795-4723-8d60-7e6bf976c88d",
   "metadata": {},
   "source": [
    "### Definition experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbcd91-f913-4ed5-9951-fc740999bc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact1 - papers:all, cits:topic\n",
    "def experts_impact_mean_1(works_authors,start_year_i,active_authors_start,works_cit_counts_year_concept):\n",
    "\n",
    "    #papers:all, citations:just tagged with concept \n",
    "    #all papers (with and without concept) written before start_date by active authors\n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('@start_year_i - 5 <= publication_year < @start_year_i', engine='python')\n",
    "                    .query('author_id.isin(@active_authors_start)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_concept_startyear = works_cit_counts_year_concept.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_concept_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf70ef6-5fbb-4ec8-909c-9a06a38f243c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact2 - papers:topic, cits:all\n",
    "def experts_impact_mean_2(works_authors,start_year_i,prior_works_ids_5yr,active_authors_start,works_cit_counts_year):\n",
    "\n",
    "    #papers:just tagged with concept, citations:all\n",
    "    #just papers tagged with concept written before start_date by active authors \n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('work_id.isin(@prior_works_ids_5yr)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_startyear = works_cit_counts_year.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0a68e-2914-4150-9b33-c0e508e5c1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mean_impact3 - papers:topic, cits:topic\n",
    "def experts_impact_mean_3(works_authors,start_year_i,prior_works_ids_5yr,active_authors_start,works_cit_counts_year_concept):\n",
    "\n",
    "    #papers:just tagged with concept, citations:just tagged with concept \n",
    "    #just papers tagged with concept written before start_date by active authors \n",
    "    prior_works_ids_tot_5yr = (works_authors\n",
    "                    .query('work_id.isin(@prior_works_ids_5yr)'))\n",
    "\n",
    "    #just citations from papers with concept\n",
    "    works_cit_counts_year_concept_startyear = works_cit_counts_year_concept.query('work_publication_year == @start_year_i - 1')\n",
    "\n",
    "    prior_works_ids_tot_5yr_cit = pd.merge(prior_works_ids_tot_5yr, works_cit_counts_year_concept_startyear, on=\"work_id\")\n",
    "    \n",
    "    #add authors zero citations\n",
    "    miss_list = list(active_authors_start.difference(set(prior_works_ids_tot_5yr_cit.author_id)))\n",
    "    miss_n = len(miss_list)\n",
    "    miss = {'work_id': [np.NaN]*miss_n, \n",
    "            'author_id': miss_list,\n",
    "            'author_name': [np.NaN]*miss_n, \n",
    "            'institution_id': [np.NaN]*miss_n, \n",
    "             'publication_year': [start_year_i-1]*miss_n,\n",
    "            'publication_date': [np.NaN]*miss_n,\n",
    "            'work_publication_year': [np.NaN]*miss_n,\n",
    "             'cit_count': [0]*miss_n,\n",
    "             'cit_count_cum': [0]*miss_n,\n",
    "    }\n",
    "    df_miss = pd.DataFrame(data=miss)\n",
    "    prior_works_ids_tot_5yr_cit = pd.concat([prior_works_ids_tot_5yr_cit, df_miss])\n",
    "    prior_works_ids_tot_5yr_cit = prior_works_ids_tot_5yr_cit[['author_id','cit_count_cum']].groupby(['author_id']).mean()\n",
    "\n",
    "    impact_df = prior_works_ids_tot_5yr_cit.sort_values(by=['cit_count_cum'],ascending=False)\n",
    "    impact_df = impact_df.reset_index()\n",
    "    impact_df.columns = ['author_id', 'val']\n",
    "    impact_df_len = len(impact_df)\n",
    "\n",
    "    return impact_df,impact_df_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049e452-a496-4bb5-b98e-c00a9bb9244b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def experts_productivity(works_authors,prior_work_ids_5yr,active_authors_start):\n",
    "    #count number of works written with topic during exposure window\n",
    "    sorted_author_works_count = (\n",
    "    works_authors\n",
    "    .query('work_id.isin(@prior_work_ids_5yr) & author_id.isin(@active_authors_start)') \n",
    "    .groupby('author_id')\n",
    "    .work_id\n",
    "    .count()\n",
    "    .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    sorted_author_works_count_len = len(sorted_author_works_count)\n",
    "    \n",
    "    sorted_author_works_count = sorted_author_works_count.to_frame().reset_index()\n",
    "    sorted_author_works_count.columns = ['author_id', 'val']\n",
    "    \n",
    "    return sorted_author_works_count,sorted_author_works_count_len "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2601b15-ea02-4060-8612-b0eaba005771",
   "metadata": {},
   "source": [
    "### Get author samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa85152-5ca8-45a3-ac7e-3254f26fcaf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_author_samples(author_stats_df, top_k, debug=False):\n",
    "    \"\"\"\n",
    "    author_stats_df: DataFrame where author_id has active author ids, and val has the productivity/impact values for that author\n",
    "    top_k: either 10 or 20 depending on top 10 or 20%\n",
    "    \n",
    "    Returns a dictionary where keys are class labels, and values are set of author IDs\n",
    "    \"\"\"\n",
    "    # Note highest scoring authors are ranked LAST \n",
    "    author_stats_df.loc[:, 'rank_pct'] = author_stats_df.val.rank(method='min', pct=True)  # rank rows based on val convert to percentiles\n",
    "    #author_stats_df.loc[:, 'rank_pct'] = author_stats_df.val.rank(pct=True)\n",
    "    \n",
    "    if top_k == 10:\n",
    "        bins = [0, 0.1, 0.3, 0.45, 0.55, 0.7, 0.9, 1]\n",
    "        labels=['bottom 10%', '10-30%', '30-45%', 'middle 10%', '55-70%', '70-90%', 'top 10%']\n",
    "    else:\n",
    "        bins = [0, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 1]\n",
    "        labels=['bottom 20%', '20-30%', '30-40%', 'middle 20%', '60-70%', '70-80%', 'top 20%']\n",
    "        \n",
    "    author_stats_df.loc[:, 'rank_cat'] = (  # assign category labels based on rank percentiles \n",
    "        pd.cut(\n",
    "            author_stats_df.rank_pct,\n",
    "            bins=bins,\n",
    "            labels=labels\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    samples_per_class = max(int((top_k / 100) * author_stats_df.author_id.nunique()), 1)\n",
    "    if debug:\n",
    "        print(f'{top_k=} taking {samples_per_class=:,}')\n",
    "        display(author_stats_df.head(2))\n",
    "    \n",
    "    buckets_size = list(author_stats_df.groupby('rank_cat').count()['rank_pct'])\n",
    "    #print(buckets_size)\n",
    "    \n",
    "    samples_dict = {}\n",
    "    \n",
    "    #keep = [f'bottom {top_k}%', f'middle {top_k}%', f'top {top_k}%']  # keep only these classes\n",
    "    keep = [f'bottom {top_k}%', f'top {top_k}%']\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in keep:\n",
    "            continue\n",
    "        \n",
    "        #initial bucket     \n",
    "        candidates = set(author_stats_df[author_stats_df.rank_cat==label].author_id)\n",
    "        candidates_size = buckets_size[i] #len(candidates)\n",
    "        if candidates_size >=  samples_per_class:\n",
    "            if debug:\n",
    "                print(f'{label}: Sampling {samples_per_class:,} from {len(candidates):,} candidates')\n",
    "            samples = set(random.sample(list(candidates), samples_per_class))  # sample here\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f'Insufficient items in {label}. Need {samples_per_class:,} have {len(candidates):,}')\n",
    "            samples = candidates  # pick everyone\n",
    "    \n",
    "        missing = samples_per_class - len(samples)\n",
    "        if missing > 0: \n",
    "            \n",
    "            #1 next bucket \n",
    "            if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i+1]\n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i+1]\n",
    "            else: # for the highest bucket, go one below\n",
    "                next_label = author_stats_df.rank_cat.cat.categories[i-1] \n",
    "                candidates = set(author_stats_df[author_stats_df.rank_cat==next_label].author_id)\n",
    "                candidate_size = buckets_size[i-1]\n",
    "\n",
    "            if candidate_size >= missing:    \n",
    "                new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                samples = samples | new_samples  # add these new samples\n",
    "                if debug:\n",
    "                    print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "            else: \n",
    "                new_samples = candidates  # pick everyone\n",
    "                samples = samples | new_samples\n",
    "            \n",
    "            missing = samples_per_class - len(samples)\n",
    "            if missing > 0: \n",
    "\n",
    "                #2 next bucket \n",
    "                if i != len(labels) - 1: #not last bucket # try the next bucket\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i+2]\n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i+2]\n",
    "                else: # for the highest bucket, go one below\n",
    "                    next_next_label = author_stats_df.rank_cat.cat.categories[i-2] \n",
    "                    candidates = set(author_stats_df[author_stats_df.rank_cat==next_next_label].author_id)\n",
    "                    candidate_size = buckets_size[i-2]\n",
    "                \n",
    "                if candidate_size >= missing:    \n",
    "                    new_samples = set(random.sample(list(candidates), missing))  # sample here\n",
    "                    samples = samples | new_samples  # add these new samples\n",
    "                    if debug:\n",
    "                        print(f'Missing {missing:,} samples for {label}. Expanding the range to {next_next_label}, Acquired {len(new_samples):,} new samples.')\n",
    "                else: \n",
    "                    new_samples = candidates  # pick everyone\n",
    "                    samples = samples | new_samples\n",
    "    \n",
    "        assert len(samples) == samples_per_class, f'Count mismatch {len(samples)=} {samples_per_class=} for samples {label}'\n",
    "        samples_dict[label] = samples\n",
    "        \n",
    "    return samples_dict,samples_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e6510-9823-40e0-82d0-c8def3edd997",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ba6e1-f45a-41ed-946b-ea410f50bed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_support_graph_ver1(bip_g, author_ids_supp):\n",
    "    support_graph_ = nx.bipartite.weighted_projected_graph(bip_g, nodes=author_ids_supp)\n",
    "    return support_graph_\n",
    "\n",
    "def get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final):\n",
    "    \n",
    "    neighbors_active = n_anas & active_authors_start #for each neighbors at the beginning that is active \n",
    "    if len(neighbors_active)!=0:      \n",
    "        #consider just active neighbors\n",
    "        neighbors_active.add(anas)\n",
    "        ego_active = support_graph_.subgraph(neighbors_active).copy()\n",
    "        #sum weights #number contacts with active authors in exposure window from activation date \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final.keys():\n",
    "            dict_final[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final[exposure_anas_start] = [anas]\n",
    "    else:\n",
    "        ego_active = nx.empty_graph() #empty\n",
    "        #add info to dictionary\n",
    "        if 0 in dict_final.keys():\n",
    "            dict_final[0].append(anas)\n",
    "        else:\n",
    "            dict_final[0] = [anas]\n",
    "                   \n",
    "    return dict_final,ego_active\n",
    "\n",
    "def get_scores_B_ver1(anas,n_anas, high_active_authors,low_active_authors,ego_active_total,dict_final_high,dict_final_low):\n",
    "    \n",
    "    neighbors_active_high = n_anas & high_active_authors   \n",
    "    neighbors_active_low = n_anas & low_active_authors  \n",
    "    \n",
    "    if len(neighbors_active_low)==0 and len(neighbors_active_high)!=0: #just contact with high (not low)\n",
    "        #consider just active neighbors\n",
    "        neighbors_active_high.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_high).copy()\n",
    "        #sum weights #number papers written with active authors in exposure window from activation date  \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final_high.keys():\n",
    "            dict_final_high[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_high[exposure_anas_start] = [anas]\n",
    "            \n",
    "    #low active 10%         \n",
    "    if len(neighbors_active_high)==0 and len(neighbors_active_low)!=0: \n",
    "        neighbors_active_low.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_low).copy() \n",
    "        exposure_anas_start = ego_active.degree(anas,weight='weight')\n",
    "        if exposure_anas_start in dict_final_low.keys():\n",
    "            dict_final_low[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_low[exposure_anas_start] = [anas] \n",
    "            \n",
    "    return dict_final_high,dict_final_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5135f82-4f0e-4d5f-bb4d-4b210318a923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_works(G, u, v):\n",
    "    w = set(G[u]) & set(G[v]) #works written together #G[u] neighbors of u in bipartite graph #weights are sets of works written by the two authors\n",
    "    return w\n",
    "\n",
    "def get_support_graph_ver2(bip_g, author_ids_supp,list_works):\n",
    "    #weighted graph number papers\n",
    "    support_graph_ = nx.bipartite.generic_weighted_projected_graph(bip_g, nodes=author_ids_supp, weight_function=list_works)\n",
    "    return support_graph_\n",
    "\n",
    "def get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final):\n",
    "    \n",
    "    neighbors_active = n_anas & active_authors_start #for each neighbors at the beginning that is active \n",
    "    #ego network anas\n",
    "    #ego = nx.ego_graph(support_graph_, anas)\n",
    "    if len(neighbors_active)!=0:      \n",
    "        #consider just active neighbors\n",
    "        neighbors_active.add(anas)\n",
    "        ego_active = support_graph_.subgraph(neighbors_active).copy()\n",
    "        #sum weights #number papers written with active authors in exposure window from activation date \n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final.keys():\n",
    "            dict_final[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final[exposure_anas_start] = [anas]\n",
    "    else:\n",
    "        ego_active = nx.empty_graph() #empty\n",
    "        #add info to dictionary\n",
    "        if 0 in dict_final.keys():\n",
    "            dict_final[0].append(anas)\n",
    "        else:\n",
    "            dict_final[0] = [anas]\n",
    "                   \n",
    "    #return dict_final,ego\n",
    "    return dict_final,ego_active\n",
    "\n",
    "def get_scores_B_ver2(anas,n_anas,high_active_authors,low_active_authors,ego_active_total,dict_final_high,dict_final_low):\n",
    "    \n",
    "    \n",
    "    neighbors_active_high = n_anas & high_active_authors   \n",
    "    neighbors_active_low = n_anas & low_active_authors  \n",
    "    \n",
    "    if len(neighbors_active_low)==0 and len(neighbors_active_high)!=0: #just contact with high (not low)\n",
    "        #consider just active neighbors\n",
    "        neighbors_active_high.add(anas)\n",
    "        #ego_active = ego.subgraph(neighbors_active_high).copy()\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_high).copy()\n",
    "        #sum weights #number papers written with infected authors in exposure window from activation date\n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final_high.keys():\n",
    "            dict_final_high[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_high[exposure_anas_start] = [anas]\n",
    "            \n",
    "    #low active 10%         \n",
    "    if len(neighbors_active_high)==0 and len(neighbors_active_low)!=0: \n",
    "        neighbors_active_low.add(anas)\n",
    "        ego_active = ego_active_total.subgraph(neighbors_active_low).copy() \n",
    "        #sum weights #number papers written with infected authors in exposure window from activation date\n",
    "        works_written = set()\n",
    "        for nn in list(ego_active.neighbors(anas)):\n",
    "            works_written = works_written | ego_active.edges[(anas,nn)]['weight']\n",
    "        exposure_anas_start = len(works_written)\n",
    "        if exposure_anas_start in dict_final_low.keys():\n",
    "            dict_final_low[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_low[exposure_anas_start] = [anas] \n",
    "            \n",
    "    return dict_final_high,dict_final_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee6cbf-9b39-442f-ab9d-40f0baa86696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_support_graph_ver3(bip_g, author_ids_supp):\n",
    "    #no weights\n",
    "    support_graph_ = nx.bipartite.weighted_projected_graph(bip_g, nodes=author_ids_supp) \n",
    "    return support_graph_\n",
    "\n",
    "def get_scores_A_ver3(anas,n_anas, active_authors_start,dict_final):\n",
    "    \n",
    "    neighbors_active = n_anas & active_authors_start #for each neighbors at the beginning that is active\n",
    "\n",
    "    if len(neighbors_active)!=0:\n",
    "        #number infected coauthors\n",
    "        exposure_anas_start = len(neighbors_active)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final.keys():\n",
    "            dict_final[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final[exposure_anas_start] = [anas]\n",
    "    else:\n",
    "        #add info to dictionary\n",
    "        if 0 in dict_final.keys():\n",
    "            dict_final[0].append(anas)\n",
    "        else:\n",
    "            dict_final[0] = [anas]\n",
    "            \n",
    "    return dict_final\n",
    "\n",
    "def get_scores_B_ver3(anas,n_anas,high_active_authors,low_active_authors,dict_final_high,dict_final_low):\n",
    "    \n",
    "    neighbors_active_high = n_anas & high_active_authors   \n",
    "    neighbors_active_low = n_anas & low_active_authors  \n",
    "    \n",
    "    if len(neighbors_active_low)==0 and len(neighbors_active_high)!=0: #just contact with high (not low)\n",
    "        #consider just active neighbors\n",
    "        #number infected coauthors\n",
    "        exposure_anas_start = len(neighbors_active_high)\n",
    "        #add info to dictionary\n",
    "        if exposure_anas_start in dict_final_high.keys():\n",
    "            dict_final_high[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_high[exposure_anas_start] = [anas]\n",
    "     \n",
    "    #low active 10% \n",
    "    if len(neighbors_active_high)==0 and len(neighbors_active_low)!=0: \n",
    "        #number infected coauthors\n",
    "        exposure_anas_start = len(neighbors_active_low)\n",
    "        if exposure_anas_start in dict_final_low.keys():\n",
    "            dict_final_low[exposure_anas_start].append(anas)\n",
    "        else:\n",
    "            dict_final_low[exposure_anas_start] = [anas] \n",
    "            \n",
    "    return dict_final_high,dict_final_low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57b155-d7da-4d9d-91df-4af921748d69",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e6d10-e33c-43d5-8e87-f4543acbf395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculation_A(i,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated):   \n",
    "    \n",
    "    dict_k_frac = {}\n",
    "    dict_k_num = {} #numerator\n",
    "    dict_k_den = {} #denumerator\n",
    "    \n",
    "    #key 0   #add authors not considered  \n",
    "    author_ids_ = set().union(*author_ids_tot_list[i:i+5]) #all authors windows restricted to eligible ones\n",
    "    author_ids_ = author_ids_ - prior_author_ids_list[i]\n",
    "    author_ids_ = author_ids_  - all_coauthors_list[i] #already considered\n",
    "    authors_k = author_ids_ | authors_isolated  \n",
    "    len_k = len(authors_k) #all authors zero contacts\n",
    "    new_auth_k = len(authors_k & first_time_authors_tot) #number of authors become infected first time during the observation window\n",
    "    dict_k_frac[0] = new_auth_k/len_k\n",
    "    dict_k_num[0] = new_auth_k\n",
    "    dict_k_den[0] = len_k\n",
    "\n",
    "    #key != 0      \n",
    "    dict_final_keys = list(dict_final.keys())\n",
    "    dict_final_keys.sort()\n",
    "    for k in dict_final_keys: #for each class k\n",
    "        authors_k = set(dict_final[k])\n",
    "        len_k = len(authors_k) #number of authors\n",
    "        new_auth_k = len(authors_k & first_time_authors) #number of authors become infected first time during the observation window\n",
    "        dict_k_frac[k] = new_auth_k/len_k\n",
    "        dict_k_num[k] = new_auth_k\n",
    "        dict_k_den[k] = len_k\n",
    "        \n",
    "    #order dictionary by key\n",
    "    dict_k_frac_ord = collections.OrderedDict(sorted(dict_k_frac.items()))\n",
    "    dict_k_frac_num = collections.OrderedDict(sorted(dict_k_num.items()))\n",
    "    dict_k_frac_den = collections.OrderedDict(sorted(dict_k_den.items()))\n",
    "    \n",
    "    dict_final_list.append(dict_k_frac_ord)\n",
    "    dict_final_num_list.append(dict_k_frac_num)\n",
    "    dict_final_den_list.append(dict_k_frac_den)\n",
    "          \n",
    "    return dict_final_list,dict_final_num_list,dict_final_den_list\n",
    "\n",
    "def calculation_B(first_time_authors,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list):\n",
    "         \n",
    "    dict_k_frac = {}\n",
    "    dict_k_num = {} #numerator\n",
    "    dict_k_den = {} #denumerator            \n",
    "    \n",
    "    dict_final_keys = list(dict_final.keys())\n",
    "    dict_final_keys.sort()\n",
    "    for k in dict_final_keys: #for each class k\n",
    "        authors_k = set(dict_final[k])\n",
    "        len_k = len(authors_k) #number of authors\n",
    "        new_auth_k = len(authors_k & first_time_authors) #number of authors become active first time during the period\n",
    "        dict_k_frac[k] = new_auth_k/len_k\n",
    "        dict_k_num[k] = new_auth_k\n",
    "        dict_k_den[k] = len_k \n",
    "        \n",
    "    #order dictionary by key\n",
    "    dict_k_frac_ord = collections.OrderedDict(sorted(dict_k_frac.items()))\n",
    "    dict_k_frac_num = collections.OrderedDict(sorted(dict_k_num.items()))\n",
    "    dict_k_frac_den = collections.OrderedDict(sorted(dict_k_den.items()))\n",
    "        \n",
    "    dict_final_list.append(dict_k_frac_ord)\n",
    "    dict_final_num_list.append(dict_k_frac_num)\n",
    "    dict_final_den_list.append(dict_k_frac_den)\n",
    "    \n",
    "        \n",
    "    return dict_final_list,dict_final_num_list,dict_final_den_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238a5a5-14fc-45ec-a6d9-68d6ecf89d34",
   "metadata": {},
   "source": [
    "### Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f44425-3a0b-4542-b4b3-a4cc2dbfdd8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create folder\n",
    "my_path = os.path.join(discipline, 'Impact_mean1')\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "    \n",
    "my_path = os.path.join(discipline, 'Productivity')\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22305cc-d087-4e76-bdd7-db6102776328",
   "metadata": {},
   "source": [
    "## INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d29d1-760d-43d6-845d-ef19e8e0db21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info(topic,my_path):\n",
    "\n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python') \n",
    "\n",
    "    #each year: work and authors topic\n",
    "    start_year = 1990 \n",
    "    work_ids_list =  []\n",
    "    author_ids_list =  []\n",
    "    for w in range(0,32): \n",
    "        start_year_w = start_year+w\n",
    "\n",
    "        work_ids = set(\n",
    "            works_concepts_conc\n",
    "            .query('publication_year == @start_year_w', engine='python')\n",
    "            .work_id\n",
    "        )\n",
    "        work_ids_list.append(work_ids)\n",
    "        # corrispondent authors\n",
    "        author_ids = set(\n",
    "            works_authors\n",
    "            .query('work_id.isin(@work_ids)', engine='python')\n",
    "            .author_id\n",
    "        )\n",
    "        author_ids_list.append(author_ids) \n",
    "        \n",
    "    #save\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(work_ids_list,fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(author_ids_list,fp)\n",
    "        \n",
    "    #each year: works and authors (with and without topic) \n",
    "    work_ids_tot_list =  []\n",
    "    author_ids_tot_list =  []\n",
    "    for w in range(0,28):\n",
    "        start_year_w = start_year+w\n",
    "\n",
    "        work_ids = (\n",
    "            works\n",
    "            .query('publication_year == @start_year_w', engine='python')\n",
    "            .index\n",
    "        )\n",
    "        work_ids_tot_list.append(work_ids)\n",
    "\n",
    "        author_ids = set(\n",
    "            works_authors\n",
    "            .query('work_id.isin(@work_ids)', engine='python')\n",
    "            .author_id\n",
    "        )\n",
    "        author_ids_tot_list.append(author_ids)  \n",
    "    #save\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(work_ids_tot_list,fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(author_ids_tot_list,fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    windows_cond = [] \n",
    "    for w in range(0,23):\n",
    "        start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "        # work and authors topic in EW\n",
    "        prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "        prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "        # work and authors topic in OW\n",
    "        work_ids = set().union(*work_ids_list[w+5:w+5+5]) \n",
    "        author_ids = set().union(*author_ids_list[w+5:w+5+5])\n",
    "\n",
    "        #active authors start observation window\n",
    "        active_authors_start = prior_author_ids_5yr\n",
    "        \n",
    "        info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'EW-papers topic': len(prior_work_ids_5yr),\n",
    "                'EW-authors topic - active authors': len(prior_author_ids_5yr),\n",
    "                'OW-papers topic': len(work_ids),\n",
    "                'OW-authors topic': len(author_ids),\n",
    "                  }\n",
    "        \n",
    "        #consider just windows with at least 3000 papers in EW and OW \n",
    "        windows_cond.append((len(prior_work_ids_5yr)>=3000) and (len(work_ids)>=3000))\n",
    "            \n",
    "        info_i = pd.DataFrame(data=[info_i_dict])\n",
    "        info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)\n",
    "        \n",
    "    my_file = 'info_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    #save\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(windows_cond,fp)\n",
    "\n",
    "    return info_df,windows_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede038f2-3760-4d7f-b43e-366cf2ba00b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "windows_cond = {}\n",
    "my_path = os.path.join(discipline, 'Info')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "for topic in topic_list:\n",
    "    info_df_top,windows_cond_top = info(topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    windows_cond[topic] = windows_cond_top\n",
    "my_file = 'info_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'windows_cond'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(windows_cond,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bc92c-6de7-4774-882a-7fe2906f4243",
   "metadata": {},
   "source": [
    "### Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ac839-1f00-406d-b4f6-9831441a5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_productivity(discipline,topic,my_path):\n",
    "       \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in range(0,23):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_productivity(works_authors,prior_work_ids_5yr,active_authors_start)\n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] \n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)  \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86b725-2634-4db5-804e-16d6ee523b00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(discipline, 'Info/Productivity')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "for topic in topic_list:\n",
    "    info_df_top,active_authors_classes_top = info_productivity(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c291336b-ef23-4e1b-b30b-704ae4c58f02",
   "metadata": {},
   "source": [
    "### Impact (mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05c040f-d09b-4bb4-ba22-f19d7baf1cdd",
   "metadata": {},
   "source": [
    "#### Def. impact 1 - papers:all, cits:topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f1244-f69b-45bd-b3c5-04506646dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    " def info_impact1(discipline,topic,my_path):\n",
    "    \n",
    "    works_concepts_conc_tot = works_concepts.query('concept_name==@topic', engine='python') \n",
    "    \n",
    "    work_ids_concept = set(works_concepts_conc_tot.work_id)\n",
    "    works_referenced_works_concept = works_referenced_works.query('work_id.isin(@work_ids_concept)', engine='python')\n",
    "    works_cit_counts_year_concept = works_referenced_works_concept.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "    works_cit_counts_year_concept.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "    index = pd.MultiIndex.from_product(works_cit_counts_year_concept.index.levels)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reindex(index)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reset_index(level=0).reset_index(level=0)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.fillna(0)\n",
    "    works_cit_counts_year_concept['cit_count_cum'] = works_cit_counts_year_concept.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.rename(columns = {'referenced_work_id':'work_id'})\n",
    "       \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in range(0,23):\n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_impact_mean_1(works_authors,start_year_w,active_authors_start,works_cit_counts_year_concept) \n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            # mid_active_authors1 = samples_dict_1['middle 10%']\n",
    "            # mid_active_authors1_val = sorted_author_works_count.query('author_id.isin(@mid_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] # [[samples_dict_1,n_1],[samples_dict_2,n_2]]\n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)   \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981741c-35cb-47f2-9244-da40eca4271c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(discipline, 'Info/Impact_mean1')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "for topic in topic_list:\n",
    "    info_df_top,active_authors_classes_top = info_impact1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee2aaf-beaf-465b-926a-e0ffa96e25b3",
   "metadata": {},
   "source": [
    "#### Def. impact 2 - papers:topic, cits:all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd46b0-1b6d-4ebd-97e1-da884871edfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def info_impact2(discipline,topic,my_path):\n",
    "       \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in range(0,23):\n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_impact_mean_2(works_authors,start_year_w,prior_work_ids_5yr,active_authors_start,works_cit_counts_year)\n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            # mid_active_authors1 = samples_dict_1['middle 10%']\n",
    "            # mid_active_authors1_val = sorted_author_works_count.query('author_id.isin(@mid_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] # [[samples_dict_1,n_1],[samples_dict_2,n_2]]\n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)   \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf6c4f-2e7e-452b-9e74-aacb870b1724",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(discipline, 'Info/Impact_mean2')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "for topic in topic_list:\n",
    "    info_df_top,active_authors_classes_top = info_impact2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdfe98-c07e-4a93-98bd-2432a11438e9",
   "metadata": {},
   "source": [
    "#### Def. impact 3 - papers:topic, cits:topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16da10-067a-4db1-8ab9-ba9dd80b486a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_impact3(discipline,topic,my_path):\n",
    "    \n",
    "    works_concepts_conc_tot = works_concepts.query('concept_name==@topic', engine='python')\n",
    "    \n",
    "    work_ids_concept = set(works_concepts_conc_tot.work_id)\n",
    "    works_referenced_works_concept = works_referenced_works.query('work_id.isin(@work_ids_concept)', engine='python')\n",
    "    works_cit_counts_year_concept = works_referenced_works_concept.groupby(['referenced_work_id','work_publication_year']).count()[\"work_id\"].reset_index(name=\"cit_count\")\n",
    "    works_cit_counts_year_concept.set_index(['referenced_work_id', 'work_publication_year'], inplace=True)\n",
    "    index = pd.MultiIndex.from_product(works_cit_counts_year_concept.index.levels)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reindex(index)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.reset_index(level=0).reset_index(level=0)\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.fillna(0)\n",
    "    works_cit_counts_year_concept['cit_count_cum'] = works_cit_counts_year_concept.groupby(['referenced_work_id'])['cit_count'].cumsum()\n",
    "    works_cit_counts_year_concept = works_cit_counts_year_concept.rename(columns = {'referenced_work_id':'work_id'})\n",
    "\n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    info_df  = pd.DataFrame()\n",
    "    active_authors_classes = []\n",
    "    for w in range(0,23):\n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            \n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "\n",
    "            #active authors start observation window\n",
    "            active_authors_start = prior_author_ids_5yr\n",
    "\n",
    "            #authors classes \n",
    "            sorted_author_works_count,sorted_author_works_count_len = experts_impact_mean_3(works_authors,start_year_w,prior_work_ids_5yr,active_authors_start,works_cit_counts_year_concept)\n",
    "\n",
    "            #10%\n",
    "            samples_dict_1,n_1 = get_author_samples(sorted_author_works_count, top_k=10, debug=True)   \n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            high_active_authors1_val = sorted_author_works_count.query('author_id.isin(@high_active_authors1)').val\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            low_active_authors1_val = sorted_author_works_count.query('author_id.isin(@low_active_authors1)').val\n",
    "\n",
    "            #save\n",
    "            active_authors_classes_w = [active_authors_start,samples_dict_1,n_1] # [[samples_dict_1,n_1],[samples_dict_2,n_2]]\n",
    "            active_authors_classes.append(active_authors_classes_w)\n",
    "\n",
    "            info_i_dict = {\n",
    "                'T_0':start_year_w, \n",
    "                'Size classes - 10%':n_1, \n",
    "                'HIGH 10% - MAX': max(high_active_authors1_val),\n",
    "                'HIGH 10% - MEAN': mean(high_active_authors1_val),\n",
    "                'HIGH 10% - MIN': min(high_active_authors1_val),\n",
    "                'LOW 10% - MAX': max(low_active_authors1_val),\n",
    "                'LOW 10% - MEAN': mean(low_active_authors1_val),\n",
    "                'LOW 10% - MIN': min(low_active_authors1_val),\n",
    "                  }\n",
    "            info_i = pd.DataFrame(data=[info_i_dict])\n",
    "            info_df = pd.concat([info_df, info_i], ignore_index = True, axis = 0)   \n",
    "        else:\n",
    "            active_authors_classes.append(np.nan)\n",
    "\n",
    "    my_file = 'info_classes_'+topic+'_windows.csv'\n",
    "    info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)\n",
    "    info_df.insert(0, 'topic', topic)\n",
    "    \n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_classes,fp)\n",
    "\n",
    "    return info_df,active_authors_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd22e5-ae02-4333-a452-0f6bb929c1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame()  \n",
    "active_authors_classes = {}\n",
    "my_path = os.path.join(discipline, 'Info/Impact_mean3')\n",
    "if not os.path.exists(my_path): #create folder\n",
    "    os.makedirs(my_path)\n",
    "for topic in topic_list:\n",
    "    info_df_top,active_authors_classes_top = info_impact3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    info_df = pd.concat([info_df, info_df_top], ignore_index = True, axis = 0)\n",
    "    active_authors_classes[topic] = active_authors_classes_top\n",
    "my_file = 'info_classes_windows.csv'\n",
    "info_df.to_csv(os.path.join(my_path, my_file), sep=';', index=False)  \n",
    "my_file = 'active_authors_classes'\n",
    "with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "    pickle.dump(active_authors_classes,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57ae07-2b94-438a-8e6c-aef40e0254f0",
   "metadata": {},
   "source": [
    "## EXP1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b6cb3-12a7-4596-af67-fca9de29a049",
   "metadata": {},
   "source": [
    "### Productivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7e13f-fd84-4e31-80bc-0d0d9613d920",
   "metadata": {},
   "source": [
    "#### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bb6c5-d928-4256-9c6e-96793a168303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')   \n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Productivity')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    all_coauthors_list = [] #coauthors collaboration graph\n",
    "    active_authors_start_union = set() #union active authors all windows\n",
    "    for w in range(0,23):\n",
    "        \n",
    "        #consider just windows with at least 2000 papers in EW and OW\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "            # work and authors topic in EW (5 years before)\n",
    "            prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "            prior_author_ids_5yr = set().union(*author_ids_list[w:w+5]) \n",
    "   \n",
    "            # all coauthors\n",
    "            all_coauthors_w = set(\n",
    "                works_authors\n",
    "                [\n",
    "                    works_authors.work_id.isin(\n",
    "                        works_authors\n",
    "                        .query('(author_id.isin(@prior_author_ids_5yr)) & (@start_year_w - 5 <= publication_year < @start_year_w)', engine='python')\n",
    "                        .work_id)\n",
    "                ]\n",
    "                .author_id\n",
    "            )\n",
    "            #save\n",
    "            all_coauthors_list.append(all_coauthors_w)\n",
    "\n",
    "            #union active authors\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            active_authors_start_union = active_authors_start_union.union(active_authors_start)\n",
    "        else:\n",
    "            all_coauthors_list.append(np.nan) \n",
    "            \n",
    "    active_authors_start_union_list = list(active_authors_start_union)     \n",
    "    \n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(all_coauthors_list,fp)\n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(active_authors_start_union,fp)\n",
    "            \n",
    "    prior_work_ids_list =  [] #paper written with topic before start year\n",
    "    prior_author_ids_list =  []\n",
    "    first_time_authors_list = [] #authors write first paper during observation window\n",
    "    first_time_authors_tot_list = [] \n",
    "    not_active_authors_start_list = [] #authors not already active at the beginning \n",
    "    first_time_authors_union = set() #first time authors all windows \n",
    "    for w in range(0,23):\n",
    "            start_year_w = start_year+w\n",
    "            #authors written at least one paper with concept before start_date --> already active nodes at the beginning\n",
    "            if w==0:\n",
    "                prior_work_ids_df = works_concepts_conc.query('publication_year < @start_year_w', engine='python')\n",
    "                prior_work_ids = set(\n",
    "                prior_work_ids_df\n",
    "                .work_id\n",
    "                )\n",
    "                prior_work_ids_list.append(prior_work_ids)\n",
    "                prior_author_ids = set(\n",
    "                    works_authors\n",
    "                    .query('work_id.isin(@prior_work_ids)')\n",
    "                    .author_id\n",
    "                )\n",
    "                prior_author_ids_list.append(prior_author_ids)\n",
    "\n",
    "            else: \n",
    "                prior_work_ids = (prior_work_ids_list[w-1]).union(work_ids_list[w+5-1])\n",
    "                prior_work_ids_list.append(prior_work_ids)\n",
    "                prior_author_ids = (prior_author_ids_list[w-1]).union(author_ids_list[w+5-1])\n",
    "                prior_author_ids_list.append(prior_author_ids) \n",
    "            \n",
    "            windows_cond_w = windows_cond[w]   \n",
    "            if windows_cond_w:\n",
    "            \n",
    "                all_coauthors = all_coauthors_list[w] # all coauthors         \n",
    "                author_ids = set().union(*author_ids_list[w+5:w+5+5]) # work and authors topic in OW\n",
    "                first_time_authors = (all_coauthors & author_ids) - prior_author_ids #authors write first paper during observation window\n",
    "                first_time_authors_list.append(first_time_authors)               \n",
    "                first_time_authors_tot = author_ids - prior_author_ids #authors write first paper during observation window\n",
    "                first_time_authors_tot_list.append(first_time_authors)\n",
    "                not_active_authors_start = all_coauthors - prior_author_ids\n",
    "                not_active_authors_start_list.append(not_active_authors_start)                \n",
    "                first_time_authors_union = first_time_authors_union.union(first_time_authors)  \n",
    "            else:\n",
    "                first_time_authors_list.append(np.nan) \n",
    "                not_active_authors_start_list.append(np.nan) \n",
    "   \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(prior_work_ids_list,fp)\n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(prior_author_ids_list,fp)\n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_list,fp)\n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_tot_list,fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(not_active_authors_start_list,fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(first_time_authors_union,fp)\n",
    "        \n",
    "    #for authors infected at the beginning: dictionary {author : date_infection/date first paper with concept} \n",
    "    date_activation_df = (\n",
    "                        works_authors\n",
    "                        [works_authors.work_id.isin(\n",
    "                         works_concepts_conc\n",
    "                        .work_id\n",
    "                        )]\n",
    "                        .query('author_id.isin(@active_authors_start_union)')\n",
    "                        .sort_values(by='publication_date')\n",
    "                        .drop_duplicates('author_id')     \n",
    "            )\n",
    "    dict_date_act_start = pd.Series(date_activation_df.publication_date.values,index=date_activation_df.author_id).to_dict()  \n",
    "    \n",
    "    #for each infected author keep just papers written after their infection date\n",
    "    works_authors_active = works_authors[works_authors.author_id.isin(active_authors_start_union)] #restrict to active authors\n",
    "    works_authors_aa_list = []\n",
    "    for aa in tqdm(active_authors_start_union):\n",
    "        works_authors_aa = works_authors_active[ (works_authors_active.author_id == aa) & (works_authors_active.publication_date >= dict_date_act_start[aa])] #select just works before activation year\n",
    "        works_authors_aa_list.append(works_authors_aa) \n",
    "    works_authors_activation_date = pd.concat(works_authors_aa_list)  \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(works_authors_activation_date,fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            \n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2d300-944b-46f7-b6b6-c41b71c51e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b25c2-d93e-4a29-95fb-9a983cb1cab4",
   "metadata": {},
   "source": [
    "#### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9366f-a7a7-4c7f-a3db-d72162299c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')  \n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Productivity')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning  \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be36ae7-390b-4e09-9889-44a213c8ae21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Productivity/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562122e9-9bee-4775-863a-4f4d1af21d20",
   "metadata": {},
   "source": [
    "#### Def. contact 3 - #coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435326-7860-4c43-a563-9c13ee7f1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_ver3(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Productivity')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver3(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_finall = get_scores_A_ver3(anas,n_anas,active_authors_start,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver3(anas,n_anas,high_active_authors1,low_active_authors1,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f73418-13a8-4b8c-9a8c-82e139c5bdcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Productivity/Exp1_ver3')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_ver3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d284b-31ac-4eb2-a675-0b365c3b563f",
   "metadata": {},
   "source": [
    "### Impact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc91f41-45f5-4594-a4b2-e6171b80f8e2",
   "metadata": {},
   "source": [
    "#### Def. impact 1 - papers:all, cits:topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6889e-cf5c-45be-9735-467ccdd90257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:15:53.540418Z",
     "iopub.status.busy": "2023-03-07T13:15:53.539935Z",
     "iopub.status.idle": "2023-03-07T13:15:53.546204Z",
     "shell.execute_reply": "2023-03-07T13:15:53.545261Z",
     "shell.execute_reply.started": "2023-03-07T13:15:53.540286Z"
    }
   },
   "source": [
    "##### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fd308-6525-4c2e-bd77-b1c95862292f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_1_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean1')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "            \n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)  \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0e7cf-6fac-4d96-9280-97455a69058b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean1/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list[:1]:\n",
    "    topic_df_top = Exp1_1_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d450b17-5868-4186-8245-e83bd7f7f985",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "##### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b8271-acc4-4aca-96a4-748db902ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_1_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean1')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning  \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc392d01-1c17-4ed7-973c-29dcf74b5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean1/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_1_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32de371-096e-4818-9bcf-c46e3c5a92f8",
   "metadata": {},
   "source": [
    "##### Def. contact 3 - #coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd90116-12cf-4300-9888-d9a4091dbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_1_ver3(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean1')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver3(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_finall = get_scores_A_ver3(anas,n_anas,active_authors_start,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver3(anas,n_anas,high_active_authors1,low_active_authors1,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f786b51-d90c-48b2-8111-3019002bae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean1/Exp1_ver3')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_1_ver3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d05e5-96b0-4d38-82b6-2d75f0ac7374",
   "metadata": {},
   "source": [
    "#### Def. impact 2 - papers:topic, cits:all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73e228-2752-4948-95d9-8c94f7ceb264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:15:53.540418Z",
     "iopub.status.busy": "2023-03-07T13:15:53.539935Z",
     "iopub.status.idle": "2023-03-07T13:15:53.546204Z",
     "shell.execute_reply": "2023-03-07T13:15:53.545261Z",
     "shell.execute_reply.started": "2023-03-07T13:15:53.540286Z"
    }
   },
   "source": [
    "##### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb13b8-97e9-4be3-9879-a4f4dc0b567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_2_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean2')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "            \n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f61ff-f62e-43bd-ae7a-efb543a01be7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean2/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_2_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c231da-45e4-4dd7-805c-8f3bc80f823b",
   "metadata": {},
   "source": [
    "##### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab0c00-b26a-46ff-8890-4da2804328a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_2_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean2')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebc8d9-c34f-4389-972c-7351d91b1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean2/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_2_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53480b28-9d11-44cd-890a-76e5aec19b4f",
   "metadata": {},
   "source": [
    "##### Def. contact 3 - #coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b03fc-53f5-4434-b00f-4925f47b4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Exp1_2_ver3(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean2')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver3(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_finall = get_scores_A_ver3(anas,n_anas,active_authors_start,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver3(anas,n_anas,high_active_authors1,low_active_authors1,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472f71b-9df7-4c9f-aeda-41a337954cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean2/Exp1_ver3')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_2_ver3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837a36b-2589-48cb-b7fd-a097e4d642b7",
   "metadata": {},
   "source": [
    "#### Def. impact 3 - papers:topic, cits:topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f30d26d-25a1-40c3-95cd-ba8c7c95954c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T13:15:53.540418Z",
     "iopub.status.busy": "2023-03-07T13:15:53.539935Z",
     "iopub.status.idle": "2023-03-07T13:15:53.546204Z",
     "shell.execute_reply": "2023-03-07T13:15:53.545261Z",
     "shell.execute_reply.started": "2023-03-07T13:15:53.540286Z"
    }
   },
   "source": [
    "##### Def. contact 1 - #contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edd45c-aa8f-4581-9630-5721c4d6c696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_3_ver1(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "  \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean3')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver1(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning  \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver1(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver1(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "            \n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696f00a-d8f4-4cbe-b43e-d0cdd615fe9a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean3/Exp1_ver1')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_3_ver1(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2be94c-99ab-4c1f-aaef-8e917706b339",
   "metadata": {},
   "source": [
    "##### Def. contact 2 - #papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b465e9-79ae-495b-a5af-8edafd22b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_3_ver2(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean3')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver2(bip_g, author_ids_supp,list_works)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_final,ego_active_total = get_scores_A_ver2(anas,n_anas, active_authors_start,support_graph_,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver2(anas,n_anas,high_active_authors1,low_active_authors1,ego_active_total,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef42c8-9b70-497c-b63e-c8dd3bffed0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean3/Exp1_ver2')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_3_ver2(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a15ca2-aab9-42a7-9f88-4ea94b62ce19",
   "metadata": {},
   "source": [
    "##### Def. contact 3 - #coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e0619-d646-40af-9143-6298117cc333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_3_ver3(discipline,topic,my_path):\n",
    "    \n",
    "    #restrict to topic\n",
    "    works_concepts_conc = works_concepts.query('concept_name==@topic', engine='python')\n",
    "      \n",
    "    #load\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_list = pickle.load(fp)\n",
    "    my_file = 'work_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        work_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'author_ids_tot_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        author_ids_tot_list = pickle.load(fp)\n",
    "    my_file = 'windows_cond_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_cond = pickle.load(fp)\n",
    "        \n",
    "    #load\n",
    "    my_path3 = os.path.join(my_path2, 'Impact_mean3')\n",
    "    my_file = 'active_authors_classes_'+topic\n",
    "    with open(os.path.join(my_path3, my_file),\"rb\") as fp:\n",
    "        active_authors_classes = pickle.load(fp)   \n",
    "    \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    my_path4 = os.path.join(discipline, 'Productivity/Exp1_ver1')\n",
    "    my_file = 'all_coauthors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        all_coauthors_list = pickle.load(fp) \n",
    "    my_file = 'active_authors_start_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        active_authors_start_union = pickle.load(fp) \n",
    "    active_authors_start_union_list = list(active_authors_start_union)    \n",
    "            \n",
    "    my_file = 'prior_work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_work_ids_list = pickle.load(fp) \n",
    "    my_file = 'prior_author_ids_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        prior_author_ids_list = pickle.load(fp)  \n",
    "    my_file = 'first_time_authors_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_list = pickle.load(fp) \n",
    "    my_file = 'first_time_authors_tot_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_tot_list = pickle.load(fp)\n",
    "    my_file = 'not_active_authors_start_list_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        not_active_authors_start_list = pickle.load(fp)\n",
    "    my_file = 'first_time_authors_union_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        first_time_authors_union = pickle.load(fp)\n",
    "        \n",
    "    my_file = 'works_authors_activation_date_'+topic\n",
    "    with open(os.path.join(my_path4, my_file),\"rb\") as fp:\n",
    "        works_authors_activation_date = pickle.load(fp)\n",
    "           \n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    for w in tqdm(range(0,23)): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "\n",
    "            start_year_w = start_year+w\n",
    "            all_coauthors = all_coauthors_list[w]\n",
    "            first_time_authors = first_time_authors_list[w]\n",
    "            first_time_authors_tot = first_time_authors_tot_list[w]\n",
    "            [active_authors_start,samples_dict_1,n_1] = active_authors_classes[w]\n",
    "            high_active_authors1 = samples_dict_1['top 10%']\n",
    "            low_active_authors1 = samples_dict_1['bottom 10%']\n",
    "            \n",
    "            #keep just works active_authors_start in this period and written in the period\n",
    "            work_id_active = works_authors_activation_date[works_authors_activation_date.author_id.isin(active_authors_start)]\n",
    "            work_id_active = work_id_active.query('@start_year_w-5 <= publication_year < @start_year_w', engine='python') \n",
    "            #add coauthors but not infected\n",
    "            work_id_active_collab = works_authors[works_authors.work_id.isin(work_id_active.work_id)].query('author_id not in @active_authors_start')\n",
    "            works_authors_collab = pd.concat([work_id_active,work_id_active_collab]).reset_index(drop=True)    \n",
    "\n",
    "            #bipartite graph work-authors union exposure window\n",
    "            bip_g = nx.from_pandas_edgelist(\n",
    "                    works_authors_collab[['work_id', 'author_id']],\n",
    "                    source='work_id', target='author_id'\n",
    "                )\n",
    "\n",
    "            #graph weight number papers written together\n",
    "            author_ids_supp =  set(works_authors_collab.author_id)\n",
    "            support_graph_ = get_support_graph_ver3(bip_g, author_ids_supp)\n",
    "            #dictionary {number exposure start year : list of authors that number}\n",
    "            not_active_authors_start = not_active_authors_start_list[w]\n",
    "            authors_isolated = not_active_authors_start - author_ids_supp\n",
    "                       \n",
    "            dict_final = {}\n",
    "            dict_final_high1 = {}\n",
    "            dict_final_low1 = {}\n",
    "            for anas in tqdm(author_ids_supp & not_active_authors_start): #for each author not active at the beginning \n",
    "                n_anas = set(support_graph_.neighbors(anas))\n",
    "\n",
    "                #A\n",
    "                dict_finall = get_scores_A_ver3(anas,n_anas,active_authors_start,dict_final)\n",
    "                #B \n",
    "                dict_final_high1,dict_final_low1 = get_scores_B_ver3(anas,n_anas,high_active_authors1,low_active_authors1,dict_final_high1,dict_final_low1)\n",
    "\n",
    "            #(iii) Define T(k) to be the fraction of these authors that have become active by the time of the second snapshot.\n",
    "            #dictionary {k : fraction}\n",
    "\n",
    "            #A \n",
    "            dict_final_list,dict_final_num_list,dict_final_den_list = calculation_A(w,author_ids_tot_list,all_coauthors_list,first_time_authors,first_time_authors_tot,dict_final,dict_final_list,dict_final_num_list,dict_final_den_list,prior_author_ids_list,authors_isolated)   \n",
    "            #B  \n",
    "            dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1 = calculation_B(first_time_authors,dict_final_high1,dict_final_list_high1,dict_final_num_list_high1,dict_final_den_list_high1)\n",
    "            dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1 = calculation_B(first_time_authors,dict_final_low1,dict_final_list_low1,dict_final_num_list_low1,dict_final_den_list_low1)\n",
    "\n",
    "        else:\n",
    "            dict_final_list.append(np.nan)\n",
    "            dict_final_den_list.append(np.nan)\n",
    "            dict_final_num_list.append(np.nan)\n",
    "            dict_final_list_high1.append(np.nan)\n",
    "            dict_final_den_list_high1.append(np.nan)\n",
    "            dict_final_num_list_high1.append(np.nan)\n",
    "            dict_final_list_low1.append(np.nan)\n",
    "            dict_final_den_list_low1.append(np.nan)\n",
    "            dict_final_num_list_low1.append(np.nan)\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    my_file = 'df_'+topic+'_windows.csv'\n",
    "    \n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,23): \n",
    "\n",
    "        windows_cond_w = windows_cond[w]   \n",
    "        if windows_cond_w:\n",
    "            start_year_w = start_year + w\n",
    "                \n",
    "            dict_final_df=pd.DataFrame(dict_final_list[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_w)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "              \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))\n",
    " \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24333b2f-51fc-4e42-a041-d43a8b18712e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path = os.path.join(discipline, 'Impact_mean3/Exp1_ver3')\n",
    "#create folder\n",
    "if not os.path.exists(my_path):\n",
    "    os.makedirs(my_path)\n",
    "topics_df = pd.DataFrame();\n",
    "for topic in topic_list:\n",
    "    topic_df_top = Exp1_3_ver3(discipline=discipline,topic=topic,my_path=my_path) \n",
    "    topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "my_file = 'df_topic_windows.csv'\n",
    "topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d548d35-4398-43b8-93bc-0bbf11ac5f10",
   "metadata": {},
   "source": [
    "## CALCULATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4d296-2662-4442-8f3b-4fbf050ec21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#select window according to condition minimum number of papers with concept in EW and OW \n",
    "from itertools import compress\n",
    "start_year = 1995\n",
    "end_year = 2017\n",
    "years_list = list(range(start_year,end_year+1)) #list T_0\n",
    "\n",
    "def windows_selection(topic,my_path,years_list,N):\n",
    "    \n",
    "    #load works with concepts each year\n",
    "    my_file = 'work_ids_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"rb\") as fp:\n",
    "        work_ids_list = pickle.load(fp)\n",
    "        \n",
    "    #consider consecutive EW and OW (5 years each)\n",
    "    start_year = 1995 \n",
    "    windows_cond = [] \n",
    "    for w in range(0,23):\n",
    "        start_year_w = start_year+w #T_0 #start OW\n",
    "\n",
    "        # work and authors topic in EW\n",
    "        prior_work_ids_5yr = set().union(*work_ids_list[w:w+5])\n",
    "\n",
    "        # work and authors topic in OW\n",
    "        work_ids = set().union(*work_ids_list[w+5:w+5+5]) \n",
    "\n",
    "        #consider just windows with at least N papers in EW and OW \n",
    "        windows_cond.append((len(prior_work_ids_5yr)>=N) and (len(work_ids)>=N))\n",
    "\n",
    "\n",
    "    #save\n",
    "    windows_list = list(compress(years_list, windows_cond)) \n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path, my_file),\"wb\") as fp:\n",
    "        pickle.dump(windows_list,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd8497-a5e0-4303-8b75-efdb5365a50b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 3000 #thereshold\n",
    "my_path = os.path.join(discipline, 'Info')\n",
    "#windows_selection\n",
    "for topic in topic_list:        \n",
    "    windows_selection(topic=topic,my_path=my_path,years_list=years_list,N=N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe6b7-a166-4231-a9e8-55b9bbaea34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574b7e9-b598-48b3-852a-eba7fe034671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_path_list = [\n",
    "        os.path.join(discipline, 'Productivity/Exp1_ver1'),\n",
    "        os.path.join(discipline, 'Productivity/Exp1_ver2'),\n",
    "        os.path.join(discipline, 'Productivity/Exp1_ver3'),\n",
    "        os.path.join(discipline, 'Impact_mean1/Exp1_ver1'),\n",
    "        os.path.join(discipline, 'Impact_mean1/Exp1_ver2'),\n",
    "        os.path.join(discipline, 'Impact_mean1/Exp1_ver3'),\n",
    "        os.path.join(discipline, 'Impact_mean2/Exp1_ver1'),\n",
    "        os.path.join(discipline, 'Impact_mean2/Exp1_ver2'),\n",
    "        os.path.join(discipline, 'Impact_mean2/Exp1_ver3'),\n",
    "        os.path.join(discipline, 'Impact_mean3/Exp1_ver1'),\n",
    "        os.path.join(discipline, 'Impact_mean3/Exp1_ver2'),\n",
    "        os.path.join(discipline, 'Impact_mean3/Exp1_ver3')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dea48-15f7-4fd2-bc64-a3a0fa134b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#union results\n",
    "for my_path in my_path_list:\n",
    "    topics_df = pd.DataFrame();\n",
    "    for topic in topic_list: \n",
    "        my_file = 'df_'+topic+'_windows.csv'\n",
    "        topic_df_top = pd.read_csv(os.path.join(my_path, my_file),index_col=0) \n",
    "        topic_df_top.insert(0, 'topic', topic)\n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)  \n",
    "    my_file = 'df_topic_windows.csv'    \n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da29d5-a8d1-4178-b9ec-3a2f7a9d92bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dcee8-764d-49e1-8143-0504059397d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_cumulative(dict_final_num_list,dict_final_den_list,i,dict_final_num_list_cum,dict_final_list_cum,dict_final_den_list_cum):\n",
    "    \n",
    "    dataframe_num_i = pd.DataFrame(dict_final_num_list[i].items(), columns=['k', 'num'])\n",
    "    dataframe_den_i = pd.DataFrame(dict_final_den_list[i].items(), columns=['k', 'den'])\n",
    "    dataframe_i = dataframe_num_i.merge(dataframe_den_i)\n",
    "    dataframe_i_rev = dataframe_i.loc[::-1] \n",
    "    dataframe_i['num_cum'] = dataframe_i_rev['num'].cumsum().loc[::-1]\n",
    "    dataframe_i['den_cum'] = dataframe_i_rev['den'].cumsum().loc[::-1]\n",
    "    dataframe_i['prob_cum'] = (dataframe_i.num_cum / dataframe_i.den_cum).loc[::-1]\n",
    "    dict_final_num_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.num_cum)))    \n",
    "    dict_final_den_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.den_cum))) \n",
    "    dict_final_list_cum.append(dict(zip(dataframe_i.k, dataframe_i.prob_cum)))\n",
    "\n",
    "    return dict_final_list_cum,dict_final_num_list_cum,dict_final_den_list_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb202d-e466-4e9a-ae14-94ee50f4e186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Exp1_calculation(dict_final_list_cum,dict_final_den_list_cum,dict_final_num_list_cum,num_windows_concept,dict_final_list_mean,dict_final_list_std,dict_final_den_list_mean,dict_final_den_list_std,dict_final_num_list_mean,dict_final_num_list_std,j):\n",
    "\n",
    "        values_j =  [d[j] for d in dict_final_list_cum]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_cum]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_cum]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_concept)\n",
    "        \n",
    "        return dict_final_list_mean,dict_final_list_std,dict_final_den_list_mean,dict_final_den_list_std,dict_final_num_list_mean,dict_final_num_list_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2297a9-4773-490e-9550-70e9fe43aa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#average windows periods \n",
    "def Exp1_stat(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "    \n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    \n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(0,num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(0,len(start_year_window_list)):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(0,len(start_year_window_list)):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                               \n",
    "    #average and std dictionaries\n",
    "    dict_final_list_mean = {}\n",
    "    dict_final_list_std = {}\n",
    "    dict_final_den_list_mean = {}\n",
    "    dict_final_den_list_std = {}\n",
    "    dict_final_num_list_mean = {}\n",
    "    dict_final_num_list_std = {}\n",
    "    dict_final_list_high1_mean = {}\n",
    "    dict_final_list_high1_std = {}\n",
    "    dict_final_den_list_high1_mean = {}\n",
    "    dict_final_den_list_high1_std = {}\n",
    "    dict_final_num_list_high1_mean = {}\n",
    "    dict_final_num_list_high1_std = {}\n",
    "    dict_final_list_low1_mean = {}\n",
    "    dict_final_list_low1_std = {}\n",
    "    dict_final_den_list_low1_mean = {}\n",
    "    dict_final_den_list_low1_std = {}\n",
    "    dict_final_num_list_low1_mean = {}\n",
    "    dict_final_num_list_low1_std = {}\n",
    "    for j in range(0,max_keys+1):\n",
    "        values_j =  [d[j] for d in dict_final_list]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_high1]\n",
    "        dict_final_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list_high1]\n",
    "        dict_final_den_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list_high1]\n",
    "        dict_final_num_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_low1]\n",
    "        dict_final_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)  \n",
    "        values_j =  [d[j] for d in dict_final_den_list_low1]\n",
    "        dict_final_den_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_num_list_low1]\n",
    "        dict_final_num_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "    \n",
    "    #save on file \n",
    "    my_file = 'df_'+topic+'_stat.csv'\n",
    "    Prob_mean_df=pd.DataFrame(dict_final_list_mean.items(), columns=['k', 'prob_mean'])\n",
    "    Prob_std_df=pd.DataFrame(dict_final_list_std.items(), columns=['k', 'prob_std'])\n",
    "    Den_mean_df=pd.DataFrame(dict_final_den_list_mean.items(), columns=['k', 'den_mean'])\n",
    "    Den_std_df=pd.DataFrame(dict_final_den_list_std.items(), columns=['k', 'den_std'])\n",
    "    Num_mean_df=pd.DataFrame(dict_final_num_list_mean.items(), columns=['k', 'num_mean'])\n",
    "    Num_std_df=pd.DataFrame(dict_final_num_list_std.items(), columns=['k', 'num_std'])\n",
    "    \n",
    "    Prob_mean_high1_df=pd.DataFrame(dict_final_list_high1_mean.items(), columns=['k', 'prob_mean_high1'])\n",
    "    Prob_std_high1_df=pd.DataFrame(dict_final_list_high1_std.items(), columns=['k', 'prob_std_high1'])\n",
    "    Den_mean_high1_df=pd.DataFrame(dict_final_den_list_high1_mean.items(), columns=['k', 'den_mean_high1'])\n",
    "    Den_std_high1_df=pd.DataFrame(dict_final_den_list_high1_std.items(), columns=['k', 'den_std_high1'])\n",
    "    Num_mean_high1_df=pd.DataFrame(dict_final_num_list_high1_mean.items(), columns=['k', 'num_mean_high1'])\n",
    "    Num_std_high1_df=pd.DataFrame(dict_final_num_list_high1_std.items(), columns=['k', 'num_std_high1'])\n",
    "    Prob_mean_low1_df=pd.DataFrame(dict_final_list_low1_mean.items(), columns=['k', 'prob_mean_low1'])\n",
    "    Prob_std_low1_df=pd.DataFrame(dict_final_list_low1_std.items(), columns=['k', 'prob_std_low1'])\n",
    "    Den_mean_low1_df=pd.DataFrame(dict_final_den_list_low1_mean.items(), columns=['k', 'den_mean_low1'])\n",
    "    Den_std_low1_df=pd.DataFrame(dict_final_den_list_low1_std.items(), columns=['k', 'den_std_low1'])\n",
    "    Num_mean_low1_df=pd.DataFrame(dict_final_num_list_low1_mean.items(), columns=['k', 'num_mean_low1'])\n",
    "    Num_std_low1_df=pd.DataFrame(dict_final_num_list_low1_std.items(), columns=['k', 'num_std_low1'])\n",
    "       \n",
    "    topic_df = ((Prob_mean_df.merge(Prob_std_df)).merge(Den_mean_df.merge(Den_std_df)).merge(Num_mean_df.merge(Num_std_df)))   \n",
    "    topic_high1_df = ((Prob_mean_high1_df.merge(Prob_std_high1_df)).merge(Den_mean_high1_df.merge(Den_std_high1_df)).merge(Num_mean_high1_df.merge(Num_std_high1_df))) \n",
    "    topic_low1_df = ((Prob_mean_low1_df.merge(Prob_std_low1_df)).merge(Den_mean_low1_df.merge(Den_std_low1_df)).merge(Num_mean_low1_df.merge(Num_std_low1_df))) \n",
    "    \n",
    "    topic_df_  = (topic_df.merge(topic_high1_df)).merge(topic_low1_df)\n",
    "    \n",
    "    #save all concept dataframes in one file \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc16e04-57b3-4a2c-9e65-a088991f694e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cumulative each window save - for p-values caluculation\n",
    "def Exp1_stat_cum_windows(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "\n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(0,num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(0,num_windows_topic):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(0,num_windows_topic):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                \n",
    "        #order dictionary by key\n",
    "        dict_final_num_list[w] = collections.OrderedDict(sorted(dict_final_num_list[w].items()))\n",
    "        dict_final_den_list[w] = collections.OrderedDict(sorted(dict_final_den_list[w].items()))\n",
    "        dict_final_list[w] = collections.OrderedDict(sorted(dict_final_list[w].items()))\n",
    "        dict_final_num_list_high1[w] = collections.OrderedDict(sorted(dict_final_num_list_high1[w].items()))\n",
    "        dict_final_den_list_high1[w] = collections.OrderedDict(sorted(dict_final_den_list_high1[w].items()))\n",
    "        dict_final_list_high1[w] = collections.OrderedDict(sorted(dict_final_list_high1[w].items()))\n",
    "        dict_final_num_list_low1[w] = collections.OrderedDict(sorted(dict_final_num_list_low1[w].items()))\n",
    "        dict_final_den_list_low1[w] = collections.OrderedDict(sorted(dict_final_den_list_low1[w].items()))\n",
    "        dict_final_list_low1[w] = collections.OrderedDict(sorted(dict_final_list_low1[w].items()))\n",
    "                \n",
    "                \n",
    "    #cumulative distributions (at least one)\n",
    "    dict_final_num_list_cum = []\n",
    "    dict_final_den_list_cum = []\n",
    "    dict_final_list_cum = []\n",
    "    dict_final_num_list_high1_cum = []\n",
    "    dict_final_den_list_high1_cum = []\n",
    "    dict_final_list_high1_cum = []\n",
    "    dict_final_num_list_low1_cum = []\n",
    "    dict_final_den_list_low1_cum = []\n",
    "    dict_final_list_low1_cum = []\n",
    "    for w in range(0,num_windows_topic):\n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum))) \n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "            \n",
    "    #save on file dictionary each window: concept - year_start \n",
    "    start_year = 1995\n",
    "    topic_df_  = pd.DataFrame()\n",
    "    for w in range(0,num_windows_topic): \n",
    "            start_year_window = start_year_window_list[w]    \n",
    "            dict_final_df=pd.DataFrame(dict_final_list_cum[w].items(), columns=['k', 'prob'])\n",
    "            dict_final_den_df=pd.DataFrame(dict_final_den_list_cum[w].items(), columns=['k', 'den'])\n",
    "            dict_final_num_df=pd.DataFrame(dict_final_num_list_cum[w].items(), columns=['k', 'num'])\n",
    "            dict_final_high1_df=pd.DataFrame(dict_final_list_high1_cum[w].items(), columns=['k', 'prob_high1'])\n",
    "            dict_final_den_high1_df=pd.DataFrame(dict_final_den_list_high1_cum[w].items(), columns=['k', 'den_high1'])\n",
    "            dict_final_num_high1_df=pd.DataFrame(dict_final_num_list_high1_cum[w].items(), columns=['k', 'num_high1'])\n",
    "            dict_final_low1_df=pd.DataFrame(dict_final_list_low1_cum[w].items(), columns=['k', 'prob_low1'])\n",
    "            dict_final_den_low1_df=pd.DataFrame(dict_final_den_list_low1_cum[w].items(), columns=['k', 'den_low1'])\n",
    "            dict_final_num_low1_df=pd.DataFrame(dict_final_num_list_low1_cum[w].items(), columns=['k', 'num_low1']) \n",
    "\n",
    "            topic_df = dict_final_df.merge(dict_final_den_df.merge(dict_final_num_df))\n",
    "            topic_high1_df = dict_final_high1_df.merge(dict_final_den_high1_df.merge(dict_final_num_high1_df))\n",
    "            topic_low1_df = dict_final_low1_df.merge(dict_final_den_low1_df.merge(dict_final_num_low1_df))\n",
    "\n",
    "            topic_df_w  = (topic_df.merge(topic_high1_df, how='outer')).merge(topic_low1_df, how='outer')\n",
    "            topic_df_w.insert(0, 'T_0', start_year_window)\n",
    "            topic_df_ = pd.concat([topic_df_, topic_df_w], ignore_index = True, axis = 0)\n",
    "            \n",
    "    my_file = 'df_'+topic+'_windows_cum.csv'          \n",
    "    topic_df_.to_csv(os.path.join(my_path, my_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d33c26-56e4-4945-9c56-c7ec499559db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for my_path in my_path_list:  \n",
    "    #Exp1_stat_cum\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)    \n",
    "    for topic in topic_list:  \n",
    "        Exp1_stat_cum_windows(df_topics=df_topics,topic=topic,my_path=my_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe5c96-cb32-483f-9629-b3fb81581cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cumulative \n",
    "def Exp1_stat_cum(df_topics,topic,my_path):\n",
    "    df_topic = df_topics.query('topic==@topic').reset_index() #concept\n",
    "    #df_topic = df_topic.drop_duplicates(subset=['T_0','k']) #drop duplicate windows\n",
    "    \n",
    "    #download all dictionaries\n",
    "    dict_final_list = []\n",
    "    dict_final_den_list = []\n",
    "    dict_final_num_list = []\n",
    "    dict_final_list_high1 = []\n",
    "    dict_final_den_list_high1 = []\n",
    "    dict_final_num_list_high1 = []\n",
    "    dict_final_list_low1 = []\n",
    "    dict_final_den_list_low1 = []\n",
    "    dict_final_num_list_low1 = []\n",
    "    my_path2 = os.path.join(discipline, 'Info')\n",
    "    my_file = 'windows_list_'+topic\n",
    "    with open(os.path.join(my_path2, my_file),\"rb\") as fp:\n",
    "        windows_list = pickle.load(fp)\n",
    "    start_year_window_list = windows_list #consider just windows selected by condition\n",
    "    #start_year_window_list = time_periods[topic]\n",
    "    #start_year_window_list = list(set(df_topic.T_0))\n",
    "    num_windows_topic = len(start_year_window_list) #number windows\n",
    "    for w in range(0,num_windows_topic): #each window\n",
    "        start_year_window = start_year_window_list[w]\n",
    "        df_topic_w = df_topic.query('T_0==@start_year_window').reset_index()\n",
    "        \n",
    "        dict_final_list.append(dict(zip(df_topic_w.k, df_topic_w.prob))) \n",
    "        dict_final_den_list.append(dict(zip(df_topic_w.k, df_topic_w.den)))\n",
    "        dict_final_num_list.append(dict(zip(df_topic_w.k, df_topic_w.num)))\n",
    "        dict_final_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.prob_high1)))\n",
    "        dict_final_den_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.den_high1)))\n",
    "        dict_final_num_list_high1.append(dict(zip(df_topic_w.k, df_topic_w.num_high1)))\n",
    "        dict_final_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.prob_low1)))\n",
    "        dict_final_den_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.den_low1)))\n",
    "        dict_final_num_list_low1.append(dict(zip(df_topic_w.k, df_topic_w.num_low1)))\n",
    "        \n",
    "    #add missing keys\n",
    "    max_keys=0\n",
    "    for w in range(0,num_windows_topic):\n",
    "        max_keys = max(max_keys,max(dict_final_list[w].keys()))\n",
    "    for w in range(0,num_windows_topic):\n",
    "        for j in range(0,max_keys+1):\n",
    "            if j not in dict_final_list[w].keys():\n",
    "                dict_final_list[w][j] = np.nan \n",
    "                dict_final_den_list[w][j] = np.nan\n",
    "                dict_final_num_list[w][j] = np.nan\n",
    "            if j not in dict_final_list_high1[w].keys():\n",
    "                dict_final_list_high1[w][j] = np.nan \n",
    "                dict_final_den_list_high1[w][j] = np.nan\n",
    "                dict_final_num_list_high1[w][j] = np.nan\n",
    "            if j not in dict_final_list_low1[w].keys():\n",
    "                dict_final_list_low1[w][j] = np.nan \n",
    "                dict_final_den_list_low1[w][j] = np.nan\n",
    "                dict_final_num_list_low1[w][j] = np.nan\n",
    "                \n",
    "        #order dictionary by key\n",
    "        dict_final_num_list[w] = collections.OrderedDict(sorted(dict_final_num_list[w].items()))\n",
    "        dict_final_den_list[w] = collections.OrderedDict(sorted(dict_final_den_list[w].items()))\n",
    "        dict_final_list[w] = collections.OrderedDict(sorted(dict_final_list[w].items()))\n",
    "        dict_final_num_list_high1[w] = collections.OrderedDict(sorted(dict_final_num_list_high1[w].items()))\n",
    "        dict_final_den_list_high1[w] = collections.OrderedDict(sorted(dict_final_den_list_high1[w].items()))\n",
    "        dict_final_list_high1[w] = collections.OrderedDict(sorted(dict_final_list_high1[w].items()))\n",
    "        dict_final_num_list_low1[w] = collections.OrderedDict(sorted(dict_final_num_list_low1[w].items()))\n",
    "        dict_final_den_list_low1[w] = collections.OrderedDict(sorted(dict_final_den_list_low1[w].items()))\n",
    "        dict_final_list_low1[w] = collections.OrderedDict(sorted(dict_final_list_low1[w].items()))\n",
    "                \n",
    "                \n",
    "    #cumulative distributions (at least one)\n",
    "    dict_final_num_list_cum = []\n",
    "    dict_final_den_list_cum = []\n",
    "    dict_final_list_cum = []\n",
    "    dict_final_num_list_high1_cum = []\n",
    "    dict_final_den_list_high1_cum = []\n",
    "    dict_final_list_high1_cum = []\n",
    "    dict_final_num_list_low1_cum = []\n",
    "    dict_final_den_list_low1_cum = []\n",
    "    dict_final_list_low1_cum = []\n",
    "    for w in range(0,num_windows_topic):\n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum))) \n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_high1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_high1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_high1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "        dataframe_num_w = pd.DataFrame(dict_final_num_list_low1[w].items(), columns=['k', 'num'])\n",
    "        dataframe_den_w = pd.DataFrame(dict_final_den_list_low1[w].items(), columns=['k', 'den'])\n",
    "        dataframe_w = dataframe_num_w.merge(dataframe_den_w)\n",
    "        dataframe_w_rev = dataframe_w.loc[::-1] \n",
    "        dataframe_w['num_cum'] = dataframe_w_rev['num'].cumsum().loc[::-1]\n",
    "        dataframe_w['den_cum'] = dataframe_w_rev['den'].cumsum().loc[::-1]\n",
    "        dataframe_w['prob_cum'] = (dataframe_w.num_cum / dataframe_w.den_cum).loc[::-1]\n",
    "        dict_final_num_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.num_cum)))    \n",
    "        dict_final_den_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.den_cum))) \n",
    "        dict_final_list_low1_cum.append(dict(zip(dataframe_w.k, dataframe_w.prob_cum)))\n",
    "        \n",
    "\n",
    "                                             \n",
    "    #average and std dictionaries\n",
    "    dict_final_list_mean = {}\n",
    "    dict_final_list_std = {}\n",
    "    dict_final_den_list_mean = {}\n",
    "    dict_final_den_list_std = {}\n",
    "    dict_final_num_list_mean = {}\n",
    "    dict_final_num_list_std = {}\n",
    "    dict_final_list_high1_mean = {}\n",
    "    dict_final_list_high1_std = {}\n",
    "    dict_final_den_list_high1_mean = {}\n",
    "    dict_final_den_list_high1_std = {}\n",
    "    dict_final_num_list_high1_mean = {}\n",
    "    dict_final_num_list_high1_std = {}\n",
    "    dict_final_list_low1_mean = {}\n",
    "    dict_final_list_low1_std = {}\n",
    "    dict_final_den_list_low1_mean = {}\n",
    "    dict_final_den_list_low1_std = {}\n",
    "    dict_final_num_list_low1_mean = {}\n",
    "    dict_final_num_list_low1_std = {}\n",
    "    for j in range(0,max_keys+1):\n",
    "        values_j =  [d[j] for d in dict_final_list_cum]\n",
    "        dict_final_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_cum]\n",
    "        dict_final_den_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_cum]\n",
    "        dict_final_num_list_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_high1_cum]\n",
    "        dict_final_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic) \n",
    "        values_j =  [d[j] for d in dict_final_den_list_high1_cum]\n",
    "        dict_final_den_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_high1_cum]\n",
    "        dict_final_num_list_high1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_high1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "        values_j =  [d[j] for d in dict_final_list_low1_cum]\n",
    "        dict_final_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_den_list_low1_cum]\n",
    "        dict_final_den_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_den_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        values_j =  [d[j] for d in dict_final_num_list_low1_cum]\n",
    "        dict_final_num_list_low1_mean[j] = np.nanmean(values_j)\n",
    "        dict_final_num_list_low1_std[j] = np.nanstd(values_j)/sqrt(num_windows_topic)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #save on file \n",
    "    Prob_mean_df=pd.DataFrame(dict_final_list_mean.items(), columns=['k', 'prob_mean'])\n",
    "    Prob_std_df=pd.DataFrame(dict_final_list_std.items(), columns=['k', 'prob_std'])\n",
    "    Den_mean_df=pd.DataFrame(dict_final_den_list_mean.items(), columns=['k', 'den_mean'])\n",
    "    Den_std_df=pd.DataFrame(dict_final_den_list_std.items(), columns=['k', 'den_std'])\n",
    "    Num_mean_df=pd.DataFrame(dict_final_num_list_mean.items(), columns=['k', 'num_mean'])\n",
    "    Num_std_df=pd.DataFrame(dict_final_num_list_std.items(), columns=['k', 'num_std'])\n",
    "    \n",
    "    Prob_mean_high1_df=pd.DataFrame(dict_final_list_high1_mean.items(), columns=['k', 'prob_mean_high1'])\n",
    "    Prob_std_high1_df=pd.DataFrame(dict_final_list_high1_std.items(), columns=['k', 'prob_std_high1'])\n",
    "    Den_mean_high1_df=pd.DataFrame(dict_final_den_list_high1_mean.items(), columns=['k', 'den_mean_high1'])\n",
    "    Den_std_high1_df=pd.DataFrame(dict_final_den_list_high1_std.items(), columns=['k', 'den_std_high1'])\n",
    "    Num_mean_high1_df=pd.DataFrame(dict_final_num_list_high1_mean.items(), columns=['k', 'num_mean_high1'])\n",
    "    Num_std_high1_df=pd.DataFrame(dict_final_num_list_high1_std.items(), columns=['k', 'num_std_high1'])\n",
    "    Prob_mean_low1_df=pd.DataFrame(dict_final_list_low1_mean.items(), columns=['k', 'prob_mean_low1'])\n",
    "    Prob_std_low1_df=pd.DataFrame(dict_final_list_low1_std.items(), columns=['k', 'prob_std_low1'])\n",
    "    Den_mean_low1_df=pd.DataFrame(dict_final_den_list_low1_mean.items(), columns=['k', 'den_mean_low1'])\n",
    "    Den_std_low1_df=pd.DataFrame(dict_final_den_list_low1_std.items(), columns=['k', 'den_std_low1'])\n",
    "    Num_mean_low1_df=pd.DataFrame(dict_final_num_list_low1_mean.items(), columns=['k', 'num_mean_low1'])\n",
    "    Num_std_low1_df=pd.DataFrame(dict_final_num_list_low1_std.items(), columns=['k', 'num_std_low1'])\n",
    "       \n",
    "    topic_df = ((Prob_mean_df.merge(Prob_std_df)).merge(Den_mean_df.merge(Den_std_df)).merge(Num_mean_df.merge(Num_std_df)))   \n",
    "    topic_high1_df = ((Prob_mean_high1_df.merge(Prob_std_high1_df)).merge(Den_mean_high1_df.merge(Den_std_high1_df)).merge(Num_mean_high1_df.merge(Num_std_high1_df))) \n",
    "    topic_low1_df = ((Prob_mean_low1_df.merge(Prob_std_low1_df)).merge(Den_mean_low1_df.merge(Den_std_low1_df)).merge(Num_mean_low1_df.merge(Num_std_low1_df))) \n",
    "\n",
    "    topic_df_  = (topic_df.merge(topic_high1_df)).merge(topic_low1_df)\n",
    "    \n",
    "    #save all concept dataframes in one file \n",
    "    topic_df_.insert(0, 'topic', topic)\n",
    "    return topic_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076e0d9-dfdf-4842-9b8c-8a7fd0a1beb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#simple contagion\n",
    "def Exp1_stat_sc(df_topics,df_topics_cum,topic):\n",
    "\n",
    "    df_topic = df_topics_cum.query('topic==@topic').reset_index()\n",
    "    a, b = df_topic.k[0:11],df_topic.prob_mean[0:11]\n",
    "    df_topic_baselines = df_topics.query('topic==@topic').reset_index()\n",
    "    p = df_topic_baselines.prob_mean[1]\n",
    "    prob_baselines = np.array([(1 - (1 - p)**k) for k in range(0, 11)])\n",
    "    den = np.array(df_topic_baselines.den_mean)[0:11]\n",
    "    num = prob_baselines * den \n",
    "    dataframe_num_baselines = pd.DataFrame(num, columns=['num'])\n",
    "    dataframe_den_baselines = pd.DataFrame(den, columns=['den'])\n",
    "    dataframe_baselines = dataframe_num_baselines.merge(dataframe_den_baselines, left_index=True,right_index=True)\n",
    "    dataframe_baselines_rev = dataframe_baselines.loc[::-1] \n",
    "    dataframe_baselines['num_cum'] = dataframe_baselines_rev['num'].cumsum().loc[::-1]\n",
    "    dataframe_baselines['den_cum'] = dataframe_baselines_rev['den'].cumsum().loc[::-1]\n",
    "    dataframe_baselines['prob_cum'] = (dataframe_baselines.num_cum / dataframe_baselines.den_cum).loc[::-1]\n",
    "    y_1 = list(dataframe_baselines['prob_cum'])     \n",
    "    baseline_df = pd.DataFrame(list(zip(range(0, 11), y_1)), columns =['k', 'val'])\n",
    "    \n",
    "    baseline_df.insert(0, 'topic', topic)\n",
    "    return baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac39c93-67d7-4b27-b581-905aa32dda66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for my_path in my_path_list:\n",
    "    \n",
    "    #Exp1_stat\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    topics_df = pd.DataFrame();\n",
    "    \n",
    "    for topic in topic_list:        \n",
    "        topic_df_top = Exp1_stat(df_topics=df_topics,topic=topic,my_path=my_path) \n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "    my_file = 'df_topic_stat.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))\n",
    "    \n",
    "    #Exp1_stat_cum\n",
    "    my_file = 'df_topic_windows.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    topics_df = pd.DataFrame()\n",
    "    \n",
    "    for topic in topic_list:  \n",
    "        topic_df_top = Exp1_stat_cum(df_topics=df_topics,topic=topic,my_path=my_path) \n",
    "        topics_df = pd.concat([topics_df, topic_df_top], ignore_index = True, axis = 0)\n",
    "    my_file = 'df_topic_stat_cumulative.csv'\n",
    "    topics_df.to_csv(os.path.join(my_path, my_file))\n",
    "    \n",
    "    #Exp1_stat_sc\n",
    "    my_file = 'df_topic_stat.csv'\n",
    "    df_topics = pd.read_csv(os.path.join(my_path, my_file),index_col=0)\n",
    "    \n",
    "    my_file = 'df_topic_stat_cumulative.csv'\n",
    "    df_topics_cum = pd.read_csv(os.path.join(my_path, my_file),index_col=0) \n",
    "\n",
    "    sc_df = pd.DataFrame()\n",
    "    \n",
    "    for topic in topic_list: \n",
    "        print(topic)\n",
    "        sc_df_top = Exp1_stat_sc(df_topics=df_topics,df_topics_cum=df_topics_cum,topic=topic)   \n",
    "        sc_df = pd.concat([sc_df, sc_df_top], ignore_index = True, axis = 0)\n",
    "     \n",
    "    my_file = 'df_topic_stat_sc.csv'\n",
    "    sc_df.to_csv(os.path.join(my_path, my_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
